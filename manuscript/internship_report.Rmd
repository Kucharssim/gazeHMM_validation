---
title             : "Keeping an Eye on Hidden Markov Models in Gaze Data Classification"
shorttitle        : "HMMs in Gaze Classification"

author: 
  - name          : "Malte Valentin LÃ¼ken"

authornote: |
  Affiliation: University of Amsterdam, Graduate School of Psychology, Department of Methodology and Statistics.
  
  This manuscript constitutes the internship report as part of the Research Master's Psychology.
  
  Student number: 12750166. Email address: malte.luken@student.uva.nl. 
  
  Specialization: Methodology and Statistics. Supervisor: dhr. dr. Ingmar Visser.
  
  I would like to thank Simon Kucharsky and Ingmar Visser for their helpful advice on developing and implementing this research project.

abstract: |
  Eye-tracking allows researchers to infer cognitive processes from eye movements that are classified into distinct events. Parsing the events is typically done by algorithms that transform, filter, classify, and merge raw data samples. Previous algorithms have successfully used hidden Markov models (HMMs) for classification but still inhere weaknesses. Therefore, I developed gazeHMM, an HMM algorithm that has no critical parameters to be set by users, does not require human coded data as input, and classifies fixations, saccades, PSOs, and smooth pursuits. The development was guided by the question of whether HMMs are useful at describing eye movements and whether they improve the event classification. I evaluated gazeHMM in a simulation study and on benchmark data. The simulation study showed that gazeHMM successfully recovered HMM parameters and hidden state sequences. Critical exceptions for good recovery were adding a smooth pursuit like state to gazeHMM and noisy data. For benchmark data, model comparisons with gazeHMM yielded preferred models with more states than expected. I assessed the classification performance of gazeHMM compared to other algorithms by the agreement to human event coding. Here, gazeHMM improved the event classification but did not outperform all other algorithms in most cases. Both the simulation study and benchmark application showed poor classification performance for smooth pursuits. Thus, I advice to classify smooth pursuits only for exploration and recommend gazeHMM with fixations, saccades, and PSOs for application. Future HMM algorithms could use covariates to better capture eye movement processes and explicitly model event durations to improve the classification of smooth pursuits.
  
keywords          : "eye movements; eye-tracking; parameter recovery; dependent mixture models"
wordcount         : "12808"

bibliography      : "report.bib"
appendix          : 
 - "report_appendix.Rmd"
 - "report_technical_appendix.Rmd"

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

# Introduction
For how long did humans seek to open a window into each other's minds, revealing their hidden cognitive processes? While many methods aiming to achieve this have been developed in the last decades, eye-tracking as one of them has become more and more reliable, accurate, and widely applied in cognitive psychology [@Duchowski2017]. Eye-tracking is typically used to study cognitive processes involving attention and information search based on recorded gaze position [@Schulte-Mecklenbeck2017]. Before these processes can be studied, the raw gaze data is classified into events that are distinct in their physiological patterns (e.g., duration), underlying neurological mechanisms, or cognitive functions [@Leigh2015]. Commonly distinguished events are fixations, saccades, smooth pursuit, and post-saccadic oscillations (PSOs). Classifying eye-tracking data reduces their complexity and is the first step towards cognitive interpretation [@Andersson2017; @Salvucci2000].  
The classification is normally done by algorithms, which is considered faster, more objective, and reproducible compared to human coding [@Andersson2017]. @Hein2017 give a comprehensive overview of different classification algorithms [for a structured review on classifying saccades, see also @Stuart2019]. 
As illustrated in Figure \@ref(fig:plot-algorithm-workflow), most classification algorithms consist of four steps: First, the raw gaze position is transformed into a metric of position and/or time. Second, a filtering or smoothing procedure is applied to the data to separate the gaze signal from noise and artifacts [@Spakov2012]. Third, depending on the method and settings of the algorithm, each sample is labeled as a candidate for one of the predefined events. Fourth, the algorithm decides which candidates to accept, relabel, or merge [@Hessels2017; @Komogortsev2010].  
A frequently implemented method uses probabilistic models to classify events. Instead of discrete classification, these models assign probabilities for belonging to an event to each sample. Probabilistic models have the advantage that they learn parameters from the data, can adapt to the task- and individual-specific gaze signals, and can be easily applied online [@Kasneci2014, p. 324].
One class of probabilistic models that are used in eye movement classification are hidden Markov models (HMMs). Figure \@ref(fig:plot-hmm-structure) illustrates the structure of HMMs: Parallel to the gaze signal evolves a sequence of distinct states that cannot be directly observed. Each gaze sample depends on its corresponding state. Each state depends on the previous but not on earlier states of the sequence [@Cappe2005; @Visser2019].  
In the context of eye movements, HMMs are suitable probabilistic models because the hidden states can be interpreted as events and gaze data are dependent time series. On this basis, several classification algorithms using HMMs have been developed.  
One instance is described in @Salvucci2000 and combines the HMM with a threshold approach (named "Identification by HMM" [I-HMM]). Samples are first labeled as fixations or saccades, depending on whether their velocity exceeds a threshold, and then reclassified by the HMM. Recently, @Pekkanen2017 developed an algorithm that filters the position of gaze samples through naive segmented linear regression (NSLR). The algorithm uses an HMM to parse the resulting segments into fixations, saccades, smooth pursuits, and PSOs based on their velocity and angle (named NSLR-HMM). Another version by @Mihali2017 uses a Bayesian HMM to separate microsaccades (short saccades during fixations) from motor noise based on sample velocity (named Bayesian microsaccade detection [BMD]). Moreover, @Houpt2018 developed a hierarchical approach that describes sample velocity and acceleration through an autoregression (AR) model, computes the regression weights through an HMM, and estimates the number of events with a beta-process (BP) from the data (named BP-AR-HMM).  
Several studies have tested the performance of HMM algorithms against other classification methods: I-HMM has been deemed as robust against noise, behaviorally accurate, and showing a high sample-to-sample agreement with a human coder [@Andersson2017; @Komogortsev2010; @Salvucci2000]. However, the agreement was lower when compared to an algorithm using a Bayesian mixture model [@Kasneci2014; @Tafaj2012]. NSLR-HMM showed even higher agreement to human coding than I-HMM but was outperformed by a recently developed algorithm using convolutional neural networks [@Bellet2019; @Pekkanen2017].
In sum, HMMs seem to be a promising method for classifying eye movements. Still, the existing HMM algorithms each have their weaknesses that could be improved upon (and that could explain their inferior performance in comparison to other methods).  
First, I-HMM relies on setting an appropriate threshold, which can distort the results [@Blignaut2009; @Komogortsev2010; @Shic2008]. Second, the current implementation of NSLR-HMM requires human-coded data, which limits its applicability. It also inherits fixed parameters that prevent the algorithm to adapt to the individual- or task-specific signals. Third, BMD limits the classification to microsaccades which are irrelevant in many applications and sometimes even considered as noise [@Duchowski2017]. The opposite problem was observed for BP-AR-HMM: It tends to estimate an unreasonable number of events from the data of which many are considered as noise events. Therefore, the authors suggest using it as an exploratory tool followed by further event classification.  
HMMs can also be used as a generative model to make inferences about the underlying processes of eye movements, a property that has been rarely used in the context of classification [cf. @Mihali2017]. For example, an HMM with four events could be compared to one with three events to see whether an event is present in the data. 
I propose to develop an algorithm that uses HMMs to classify eye movement data. It will address the weaknesses of previous HMM algorithms and utilize the generative property of HMMs.

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, results = "asis")

library(papaja)
library(tidyverse)
library(signal)
library(flexcircmix)
library(CircStats)
library(depmixS4)
library(psych)
library(here)
library(grid)
library(gridExtra)
library(DiagrammeR)
library(DiagrammeRsvg)
library(magrittr)
library(rsvg)
source(here("algorithm/preprocessing_helper_functions.R"))
source(here("algorithm/model_helper_functions.R"))

```

```{r set graphics theme, include=FALSE}

theme_set(theme_apa())

```

(ref:plot-algorithm-workflow) Example workflow for eye movement event classification algorithms: (a) the raw gaze signal is transformed into a velocity signal (in deg/s); (b) the velocity signal is smoothed and filtered; (c) samples are labeled as events (indicated by colors), and (d) relabeled. Sequences of samples belonging to the same event are merged (indicated by black segments). 

```{r plot-algorithm-workflow, fig.cap="(ref:plot-algorithm-workflow)", fig.height = 8}

set.seed(126)

velRaw <- sample(c(rgamma(25, 3, 1/6), rgamma(7, 25, 1/5), rgamma(8, 15, 1/5)), 40) 

sample <- 1:length(velRaw)

velSmooth <- as.numeric(smooth(velRaw, kind = "3R"))

eventPre <- ifelse(velSmooth > 40, "s", "f")

eventPost <- ifelse(velSmooth < 60, "f", eventPre)

df <- data.frame(sample, velRaw, velSmooth, eventPre, eventPost)


# Create plot

p1 <- ggplot(df, aes(sample, velRaw)) + geom_point()

p2 <- ggplot(df, aes(sample, velSmooth)) + geom_point()

p3 <- ggplot(df, aes(sample, velSmooth)) + geom_point(aes(color = eventPre))

p4 <- ggplot(df, aes(sample, velSmooth)) + geom_point(aes(color = eventPost))

plotList <- list(p1, p2, p3, p4)

for(p in 1:length(plotList)) {
  
  plotList[[p]] <- plotList[[p]] + scale_y_continuous(name = "Velocity\n(deg/s)", 
                                                      limits = c(0, 200), breaks = c(0, 100, 200)) + 
    xlab("") + geom_line(color = "grey") + ggtitle(letters[p]) + 
    theme(legend.position = "none")
  
}

plotList[[4]] <- plotList[[4]] + xlab("Sample") +
  geom_segment(aes(x = 1, y = 180, xend = 7, yend = 180), size = 1.2) +
  geom_segment(aes(x = 8, y = 180, xend = 16, yend = 180), size = 1.2) +
  geom_segment(aes(x = 17, y = 180, xend = 40, yend = 180), size = 1.2)

grid.arrange(plotList[[1]], plotList[[2]], plotList[[3]], plotList[[4]], ncol = 1)

```

(ref:plot-hmm-structure) Example structure of HMMs: The observed velocity signal *V* depends on hidden states *S* (displayed for six samples). Each state depends on the previous state. Colors indicate different states. Figure inspired by Visser and Speekenbrink [-@Visser2019, p. 133]. 

```{r plot-hmm-structure, fig.cap="(ref:plot-hmm-structure)", fig.height=2, fig.width=4}

knitr::include_graphics(here("manuscript/internship_report_files/external-files/hmm-structure.png"), auto_pdf = T, dpi = 200)

```

## Research Question
Can HMMs improve describing and classifying eye-tracking data? This is the general research question of my proposal and can be specified as following: Are HMMs useful generative models for recorded eye movements that improve classifying them into events? This research question will guide the development of the algorithm and leads to two hypotheses: First, I predict that HMMs are useful generative models for eye movements. This hypothesis will be supported if the algorithm yields events that match theoretical expectations for benchmark data. Other results will be treated as evidence against the hypothesis. Second, I hypothesize that HMMs will improve the classification performance of eye movements. I will treat it as support for this hypothesis if the algorithm outperforms algorithms using other methods when applied to benchmark data. I will treat other results as evidence against the hypothesis. 

# Operationalization
The proposed algorithm aims to improve the drawbacks of previous HMM algorithms. Therefore, its classification method is oriented to four major goals: (a) it does not require parameter settings through the user (e.g., thresholds) or (b) human-labeled data as input; (c) it covers the most relevant eye movement events, namely fixations, saccades, smooth pursuit, and PSOs; (d) it confirms rather than explores the presence of events in the data. Besides implementing the algorithm in the statistical programming language R @RCoreTeam2020, I will develop its concrete technical design exploratorily.  
The first step will be choosing an appropriate metric that allows separating the gaze data into events. @Zemblys2018 provide an overview of different gaze data metrics and compare how well they predict human-labeled events. The next step will choosing a filtering/smoothing method to reduce the noise in the gaze data [for a comparison of different gaze data filters, see @Spakov2012]. The criteria for choosing a metric and a filter will be its accuracy and not relying on user parameter settings.  
The generative part of the algorithm will allow users to build an HMM which matches their theoretical assumptions about the gaze data. The algorithm will take a preprocessed gaze data metric (e.g., filtered velocity) as input and optimize the model parameters. The output of the model will be probabilities for hidden states. The states can be interpreted as event labels for each sample. For instance, the model could assign to a sample a probability of 0.6 for belonging to a fixation and 0.4 for belonging to a saccade. These probabilities can be used to compute the hidden state sequence (e.g., by choosing events with the highest probability). Moreover, the model will output the parameters of the distribution of labeled samples for each event. It will allow generating samples from these distributions to simulate gaze data.  
Before testing the hypotheses, I will conduct a simulation study to estimate the parameter recovery of the generative model: Artificial eye movement data will be generated by the model. The parameters of the model will be varied across sensible ranges. Then, the model will be applied to the data. Finally, the estimated results of the model will be compared to the true parameters that have been used to generate the data.  
The first hypothesis will be tested by comparing different generative models on benchmark data. The models will differ in their assumed number of events in the data. The model comparison will result in a preferred model which scores best on the evaluation criterion. The preferred model is expected to match the theoretically assumed number of present events in the benchmark data.  
To test the second hypothesis, the proposed algorithm will be compared against other algorithms that have also been applied to benchmark data. The performance of the proposed algorithm will be assessed by metrics that are commonly used in classification (e.g., Cohen's Kappa).

## Sample Characteristics
The hypotheses will be tested on two benchmark data sets that have been published in previous studies. The first set was published in @Andersson2017 and contains eye-tracking data from 17 subjects. Three types of stimuli were presented on a screen ($1024 \times 768$ pixels) to each subject: A static image, a linearly moving dot, and a video. Subjects were instructed to look freely at the images and follow the movements of the dots and the objects in the videos. Their eye movements were recorded with a Hi-Speed 1250 tower-mounted eye-tracker (SensoMotoric Instruments) and a sampling rate of 500 Hz. Only data from the right eye were used. The data were labeled by two human experts in eye movement research as fixations, saccades, smooth pursuits, PSOs, or blinks [@Andersson2017, p. 621].
The second data set, published by @Ehinger2019, consists of eye-tracking data from 15 subjects who performed six blocks of 10 different experimental tasks. Here, I will only consider data from Tasks 4 and 5 because they are qualitatively different from the first data set. In Task 4, subjects were instructed to fixate a central target on the screen ($1920 \times 1080$ pixels) for 20 s. In Task 5, subjects also fixated a central target but were instructed to blink each time they heard a beep. Seven beeps with a duration of 100 ms and a 1.5 s interval in between were presented. For both tasks, eye-movements were recorded by two eye-trackers simultaneously (EyeLink 1000, SR Research; and Pupil Labs glasses, Pupil Labs) with a sampling rate of 250-500 Hz and 240 Hz (interpolated), respectively [@Ehinger2019, pp. 3-11].

## Data Analysis
I will treat the parameter recovery of the generative model as satisfactory when it yields a 95% CI covering the true parameters. The median rate of correct classifications should be a minimum of 90%. 
The first hypothesis will be tested using a model comparison approach: For both data sets, models with one, two, three, four, and five eye movement events plus a noise event will be compared. The noise event will represent blinks. For the first data set and static images, I expect the model with three events (excl. noise) to be the preferred model since smooth pursuit movements are not assumed to be present. For moving dots and video stimuli, I expect the model with four events (excl. noise) to be preferred, since smooth pursuits are expected in the data.
For the second data set, I expect a model with one event (excl. noise) to be preferred because only fixations should be present in the data from both tasks. For all comparisons, the preferred model will be chosen according to the highest Schwarz weight [@Wagenmakers2004].
I will test the second hypothesis using the metrics that have been used in the original study of the first data set: @Andersson2017 compared ten algorithms to human-labeled data by the root mean squared error (RMSE) of event distribution descriptives, the sample-to-sample agreement indicated by Cohen's Kappa, and the disagreement ratio across all samples indicated by the confusion matrix. I expect the algorithm with a generative model assuming four events (excl. noise) to achieve either a lower RMSE, higher Cohen's Kappa, or lower disagreement ratio than all the other algorithms which are evaluated in the study.

## Intended Results
Preferred generative models that match the theoretical assumptions on the number of events in the data would support the notion that HMMs describe gaze data appropriately. If a generative model with fewer events than expected is preferred, this would mean that the model is not able to distinguish the events well enough. If more events than expected are preferred, this would indicate that the model is describing noise or an unexpected event is present in the data.
Outperforming other algorithms in agreement to human-coded data would demonstrate that the proposed algorithm classifies events similar to human coders. However, @Hooge2018a observed considerable differences in how humans code gaze data. They concluded that human coders cannot be seen as a gold standard for event classification. 
The proposed algorithm would be the first allowing the user to conduct model comparisons as part of the classification. This comparison could be used to investigate the data more thoroughly on a qualitative (e.g., assumed present events) or quantitative (e.g., event sample distributions) level.
In contrast to many other classification algorithms, the proposed algorithm would not only classify gaze data but also explain why it is classified this way by using a generative model. This outstanding property of the algorithm would guarantee its competitiveness against other popular algorithms, especially the recently developed machine learning approaches which do not make generative assumptions [e.g., @Bellet2019]. It could spark a shift from pure classification output towards understanding eye movements on a more fundamental level.

```{r load data, include=FALSE}

load(here("validation/Andersson2017_raw.Rdata"))

res <- c(1024, 768)
dim <- c(380, 300)
dist <- 670
fr <- 500

```

# Algorithm Development
As part of answering my research questions, I developed an algorithm named gazeHMM to classify gaze data into discrete eye movement events. The following section describes the development of gazeHMM and its final version that has been used to obtain results. Technical details can be found in Appendix B.

## Eye Movement Metrics
Many different metrics can be used to describe gaze data and eye movement events [@Zemblys2018]. Here, my goal was to find those metrics that separate the gaze data samples belonging to different events the best. However, many metrics rely on thresholds or window ranges that have to be set by the user [e.g., the distance between the mean position in a 100 ms window before and after each sample, see @Olsson2007; @Zemblys2018]. This can be problematic because such parameters are often set without theoretical justification and they differ substantially between metrics. Because they belong to the most basic metrics which do not require parameter settings, I used velocity, acceleration, and sample-to-sample angle [synonymous to relative or change in angle; @Larsson2013] in gazeHMM.  
Theoretically, these three metrics should separate eye movement events clearly. Fixations typically inherit samples with low velocity and acceleration [@Larsson2013]. Due to tremor, the angle between samples should not follow any direction but a random walk [@Duchowski2017]. In contrast, saccade samples usually have a high velocity and acceleration and follow the same direction. PSO samples tend to have moderate velocity and high acceleration since they occur between saccades and low velocity events [@Larsson2013]. They can be specifically distinguished by their change in direction clustered around 180 degrees [@Pekkanen2017]. Importantly, the oscillations depend on the resolution of the gaze recording: Eye-trackers with higher sampling frequency yield more changes in direction and more samples in between those changes. Those samples in between typically follow the same direction. Thus, with high sampling frequencies, PSO samples might also cluster around a sample-to-sample angle of zero with outliers around 180 degrees. Lastly, smooth pursuit samples have a moderate velocity but low acceleration (due to the smoothness) and like saccades they follow the same direction [@Larsson2013; @Leigh2015]. Other algorithms focus exclusively on classifying microsaccades (e.g., @Mihali2017), but as proposed earlier, they are not in the scope of gazeHMM.  

## Filtering and Smoothing
As with metrics, many methods can be used to filter or smooth gaze data before, while, or after computing eye movement metrics. Their purpose is to remove noise or artifacts from the gaze data that could distort the classification [@Duchowski2017]. Filtering (smoothing) methods that are applied before computing the eye movement metrics target the recorded gaze position. While removing noise, filters might also erase differences in the metrics between samples. For instance, tremor movements during fixations could be filtered out and in consequence, the samples would follow the same direction. This could complicate separating fixations from smooth pursuits. The same problem applies to PSOs and saccades where filtering could erase oscillations (see Figure \@ref(fig:filter-angle)).  

(ref:filter-angle) Example data from @Andersson2017 displayed as sample-to-sample angle (in rad) over time (in s). Line colors indicate whether the positional data has been filtered before computing the sample-to-sample angle (red) or not (blue). For this example, I used a Butterworth filter of order three and a normalized cutoff frequency of 0.3. The full line segment marks where potential oscillations are filtered out. The dashed line segment illustrates where potentially random change in angle is filtered out.

```{r filter-angle, fig.cap="(ref:filter-angle)"}

filter.ex <- A2017$video[[3]] %>%
  mutate(x = px2va(x, dim[1], res[1], dist),
         y = px2va(y, dim[2], res[2], dist),
         angle = calc_theta(x, y),
         x.filt = as.numeric(signal::filter(signal::butter(3, 0.3), x)),
         y.filt = as.numeric(signal::filter(signal::butter(3, 0.3), y)),
         angle.filt = calc_theta(x.filt, y.filt),
         t = (t-t[1])/1e6) %>%
  dplyr::filter(t > 0.5, t < 0.75)

ggplot(filter.ex) + 
  geom_path(aes(x = t, y = force_neg_pi_pi(angle)), color = "blue") +
  geom_path(aes(x = t, y = force_neg_pi_pi(angle.filt)), color = "red") +
  annotate(geom = "segment", x = 0.525, xend = 0.54, y = -3, yend = -3, color = "black", size = 0.8) +
  annotate(geom = "segment", x = 0.635, xend = 0.67, y = -3, yend = -3, color = "black", size = 0.8, linetype = 6) +
  scale_y_circular(name = "Sample-to-sample angle (in rad)", units = "radians", limits = c(-pi, pi)) +
  scale_x_continuous(name = "Time (in s)")

```

Instead of filtering before computing the metrics, I decided to combine both in one step. Previous algorithms have used two methods that both filter and compute derivatives of the gaze position (i.e., velocity and acceleration). @Houpt2018 computed the first and second discrete derivative (similar to a Sobel and Laplace filter, respectively) of the gaze position to be used by their algorithm. Similarly, @Nystrom2010 used a Savitzky-Golay (SG) filter [@Savitzky1964] to estimate velocity and acceleration from a gaze signal. Figure \@ref(fig:filter-comparison) displays an example of velocity and acceleration signals obtained by both methods. They follow a similar trend, but the SG velocity signal was less noisy and the acceleration signal was consequently lower than the Laplace acceleration signal. Therefore, I chose to implement the SG filter to compute velocity and acceleration signals, because it filters out more noise than the discrete derivatives but still preserves the edges in the signal to distinguish between events. To preserve motor noise in the sample-to-sample angle signal, gazeHMM computes the absolute angle as the backward difference and the first discrete derivative as the forward difference (see Appendix B).

(ref:filter-comparison) Example data from @Andersson2017 displayed as velocity (deg/s) and acceleration (in deg/s$^2$) over time (in s). Line colors indicate which filter has been used to derive the signal. Savitzky-Golay filters were applied with order three and length five (corresponding to 10 ms).

```{r filter-comparison, fig.cap="(ref:filter-comparison)"}

filter.comp <- A2017$video[[3]] %>%
  mutate(x = px2va(x, dim[1], res[1], dist),
         y = px2va(y, dim[2], res[2], dist),
         vel.x.sg = signal::sgolayfilt(x, m = 1),
         vel.y.sg = signal::sgolayfilt(y, m = 1),
         vel.sg = sqrt(vel.x.sg^2 + vel.y.sg^2)*fr,
         acc.x.sg = signal::sgolayfilt(x, m = 2),
         acc.y.sg = signal::sgolayfilt(y, m = 2),
         acc.sg = sqrt(acc.x.sg^2 + acc.y.sg^2)*fr^2,
         vel.x.sb = lead(x) - lag(x),
         vel.y.sb = lead(y) - lag(y),
         vel.sb = sqrt(vel.x.sb^2 + vel.y.sb^2)*fr,
         acc.x.lp = lead(x) - 2*x + lag(x),
         acc.y.lp = lead(y) - 2*y + lag(y),
         acc.lp = sqrt(acc.x.lp^2 + acc.y.lp^2)*fr^2,
         t = (t-t[1])/1e6) %>%
  pivot_longer(c("vel.sg", "vel.sb", "acc.sg", "acc.lp"), names_to = c("metric", "filter"), names_sep = "[:punct:]", ) %>%
  mutate_at(c("metric", "filter"), as.factor) %>%
  dplyr::filter(t > 0.5, t < 1.5)

filter.comp$metric <- factor(filter.comp$metric, labels = c("Acceleration", "Velocity"))

ggplot(filter.comp, aes(x = t, y = value, color = filter)) + 
  geom_path() + facet_wrap(vars(metric), nrow = 2, scales = "free_y", strip.position = "left") + 
  scale_x_continuous(name = "Time (in s)") +
  scale_y_continuous(name = "") +
  scale_color_discrete(name = "Filter", labels = c("Laplace", "Sobel", "Savitzky-Golay")) +
  theme(strip.placement = "outside")
  

```

## The Generative Model
The generative model underlying gazeHMM is a multivariate hidden Markov model (HMM). It can contain between two and four states that correspond to different eye movement events: The first state always represents fixations, the second saccades, the third PSOs, and the fourth smooth pursuits. Thus, users can choose whether they would like to classify only fixations and saccades, or additionally PSOs and/or smooth pursuits.  
In general, HMMs consist of three submodels: An initial state model, a transition model, and a response model. The initial state model contains probabilites for the first state of the hidden sequence. The evolution of the sequence is in turn described by the trantision model, which comprises the probabilities for transitioning between different states in the HMM. The response model encompasses distributions describing the response variables for every state in the model.  
In gazeHMM, the response model has three response variables which are the velocity and acceleration signals obtained by the SG filter and the change in angle signal. The response variables are treated as conditionally independent on the states. Conditional independence might not accurately resemble the relationship between velocity and acceleration (which are naturally correlated). This step was merely taken to keep the HMM simple and identifiable.  
Previous algorithms using HMMs have used Gaussian distributions to describe velocity and acceleration signals (sometimes after log-transforming them). However, several reasons speak against choosing the Gaussian: First, both signals are usually positive (depending on the computation). Second, the distributions of both signals appear to be positively skewed conditionally on the states and third, to have variances increasing with their mean. Thus, instead of using the Gaussian, it could be more appropriate to describe velocity and acceleration with a distribution that follows these three properties. In gazeHMM, I use gamma distributions with a shape and scale parametrization for this purpose. It has to be noted that the gamma was chosen out of convenience and the best fitting distribution might be different between eye-trackers, subjects, and tasks.   
To model the sample-to-sample angle, I pursued a novel approach in gazeHMM: Using a mixture of von Mises distributions (with a mean and concentration parameter) and a uniform distribution. Both the distributions and the metric operate on the full unit circle (i.e., between 0 and $2\pi$), which should lead to rather symmetric distributions. Because the fixations change their direction similar to a random walk, their sample-to-sample angle can be modeled by a uniform distribution. Thus, the uniform distribution should distinguish fixations from the other events.  
The initial state model and the transition model use multinomial distributions (with a category for each state). For all three submodels of the HMM, no time-varying covariates are included. Since the implementation of gazeHMM allows for regression models for each submodel and state, this implies that only intercepts for each parameter are estimated.

## Missing Data and Blinks
HMMs can estimate parameters and hidden states despite missing data. They either treat data as missing at random or as missing depending on states [@Visser2019]. In gazeHMM, I assume data to be missing at random. However, most of the missing data in eye movement classification are due to blinks. Therefore, the user can indicate in gazeHMM which samples should be labeled as blinks (other missing samples are treated as noise). Often, eye-trackers record a few samples with unreasonably high velocity and acceleration before losing the pupil signal when a blink occurs. Since these samples could distort the classification of saccades in the HMM, I decided to remove them heuristically. Before classifying the samples, gazeHMM sets all samples within 50 ms before and after blink samples as missing. The window of 50 ms was rather motivated empirically than theoretically and can bet set to any value the user considers appropriate.  
When evaluating the likelihood of the HMM, given the parameters and joint density of response distributions, for missing data, the model integrates over all possible values. The resulting density for missing samples is therefore set to one [@Visser2019].

## Optimization and Classification
The parameters of the HMM are estimated through maximum likelihood using an expectation-maximization (EM) algorithm [@Dempster1977; @McLachlan1997]. The EM algorithm is generally suitable to estimate likelihoods with missing variables. For HMMs, it imputes missing with expected values and iteratively maximizes the joint likelihood of parameters conditional on the observed data (velocity, acceleration, and sample-to-sample angle) and the expected hidden states [eye movement events; @Visser2019]. The sequence of hidden states is estimated through the Viterbi algorithm [@Viterbi1967; @ForneyJr1973] by maximizing the posterior state probability. Parameters of the response distributions (except for the uniform distribution) are optimized on the log-scale (except for the mean parameter of the von Mises distribution) using a spectral projected gradient method [@Birgin2000] and Barzilai-Borwein step lengths [@Barzilai1988].

## Postprocessing
After classifying gaze samples into states, gazeHMM applies a postprocessing routine to the estimated state sequence. I implemented this routine because constraining the transition probabilities for PSOs to turn into non-saccade events to zero often caused PSOs not to appear in the state sequence at all. Moreover, gazeHMM does not explicitly control the duration of events in the HMM which occasionally led to unreasonably short events. Thus, the postprocessing routine heuristically compensates for such violations. This routine relabels one-sample fixations and smooth pursuits, saccades with a duration below a minimum threshold, and PSOs that follow non-saccade events. Samples are relabeled as the state of the previous event. Finally, samples initially indicated as missing are labeled as noise (including blinks) and event metrics are computed (e.g., fixation duration).

## Implementation
The algorithm is implemented in R [version: 3.6.3; @RCoreTeam2020] and uses the packages signal [@Ligges2015] to compute velocity and acceleration signals, depmixS4 [@Visser2010] for the HMM, and BB [@Varadhan2009] for Barzilai-Borwein spectral projected gradient optimization. The algorithm is available on GitHub (www.github.com/maltelueken/gazeHMM).

\newpage

# Simulation Study
To assess how well the HMM recovers parameters and state sequences, I conducted a simulation study. The design and analysis of the study were preregistered on the Open Science Framework (https://doi.org/10.17605/OSF.IO/VDJGP). The major part of this section is a direct copy of the preregistration. When appropriate, additional explanations were added and tenses adapted to make the design easier to read and understand.  
In the study, the HMM repeatedly generated data with a set of parameters (true parameter values). The same model was then applied to estimate the parameters from the generated data (estimated parameter values). I compared the true with the estimated parameter values to assess whether a parameter was recovered by the model. Additionally, I compared the true states of the HMM with the estimated states to judge how accurately the model recovered the states that generated the data.

## Starting Values
The HMM always started with a uniform distribution to estimate the initial state and state transition probabilities. To generate random starting values for the estimation of shape, scale, and concentration parameters, I used gamma distributions with a shape parameter of $\alpha_{start}=3$ and $\beta_{start}=\psi_{true}/2$ with $\psi_{true}$ being the true value of the parameter to be estimated. This setup ensured that the starting values were positive, their distributions were moderately skewed, and the modes of their distributions equaled the true parameter values. Mean parameters of the von Mises distribution always started at their true values.

## Design
### Parameter Variation
The simulation study was divided into four parts. In the first part, I varied the parameters of the HMM. For models with $k \in \{2, 3, 4\}$ states, $q \in \{10, 15, 20\}$ parameters were varied, respectively. For each parameter, the HMM generated 100 data sets with $N = 2500$ samples and the parameter varied in a specified interval in equidistant steps. This resulted in $100 \times (10+15+20) = 4500$ recoveries. Only one parameter was varied at once, the other parameters were set to their default values. All parameters of the HMM were estimated freely (i.e., there were no fixed parameters in the model). I did not manipulate the initial state probabilities because these are usually irrelevant in the context of eye movement classification. For the transition probabilities, I only simultaneously varied the probabilities for staying in the same state (diagonals of the transition matrix) to reduce the complexity of the simulation. The left over probability mass was split evenly between the probabilities for switching to a different state (per row of the transition matrix). Moreover, I did not modify the mean parameters of the von Mises distributions: As location parameters, they do not alter the shape of the distribution and they are necessary features for the HMM to distinguish between different states.  
I defined approximate ranges for each response variable and chose true parameter intervals and default values so that they produced samples that roughly corresponded to these ranges. Table \@ref(tab:sim-ranges) shows the assumed ranges for each event and Tables \@ref(tab:sim-parameters-trans) and \@ref(tab:sim-parameters-resp) show the intervals and default values for each parameter in the simulation. Parameters were scaled down by factor 10 (compared to the reported ranges) to improve fitting of the gamma distributions. I set the intervals for shape parameters of the gamma distributions for all events to [1,5] to examine how skewness influenced the recovery (shape values above five approach a symmetric distribution). The scale parameters were set so that the respective distribution approximately matched the assumed ranges. Since the concentration parameters of the von Mises distribution are the inverse of standard deviations, they were varied on the inverse scale. An example of simulated data from the HMM with default parameters is visualized in Figure \@ref(fig:plot-sim-ex).

### Sample Size and Noise Variation
In the second part, I varied the sample size of the generated data and the amount of noise added to it. The model parameters were set to their default values. For models with $k \in \{2, 3, 4\}$ states and sample sizes of $N \in \{500, 2500, 10000\}$, I generated 100 data sets ($100 \times 3 \times 3 = 900$ recoveries). These samples sizes roughly corresponded to small, medium, and large eye-tracking data sets for a single participant and trial. To simulate noise, I replaced velocity and acceleration values $y$ with draws from a gamma distribution with $\alpha_{noise} = 3$ and $\beta_{noise}=(y/2)\tau_{noise}$ with $\tau_{noise} \in [1,5]$ varying between data sets. This procedure ensured that velocity and acceleration values remained positive and were taken from moderately skewed distributions with modes equal to the original values. To angle, I added white noise from a von Mises distribution with $\mu_{noise} = 0$ and $\kappa_{noise} \in 1/[0.1,10]$ varying between data sets. $\tau_{noise}$ and $\kappa_{noise}$ were varied simultaneously in equidistant steps in their intervals.

### Variation of Starting Values
In the third part, I increased the variation in the starting values used for parameter estimation. The model parameters were set to their default values. For the shape, scale, and concentration parameters, I simultaneously increased the scale parameters of the starting value gamma distributions: For $k \in \{2, 3, 4\}$ states and $\beta_{start} = (\psi_{true}/2)\tau_{start}$ with $\tau_{start} \in \{1, 2, 3\}$, 100 data sets with $N = 2500$ samples were generated each ($100 \times 3 \times 3 = 900$ recoveries).

### Missing Data
In the last part, I set intervals of the generated data to be missing. The model parameters were set to their default values. For $k \in \{2, 3, 4\}$ states and $m \in \{1, 3, 5\}$ intervals, 100 data sets with $N = 2500$ samples were generated ($100 \times 3 \times 3 = 900$ recoveries). The length of the missing data interval $l \in [1,200]$ samples varied in equidistant steps between the data sets.

## Data Analysis [^6]
For each parameter, I calculated the root median square proportion deviation [RMdSPD; analogous to root median square percentage errors, see @Hyndman2006] between the true and estimated parameter values: $$\text{RMdSPD} = \sqrt{\text{Med}\left((\frac{\psi_{true}-\psi_{est}}{\psi_{true}})^2\right)}.$$
Even though it was not explicitly mentioned in the preregistration, this measure is only appropriate when $\psi_{true} \ne 0$. This was not the case for some mean parameters of the von Mises distributions. In those cases, I used $\psi_{true} = 2\pi$ instead. I treated $\text{RMdSPD} < 0.1$ as good, $0.1 \le \text{RMdSPD} < 0.5$ as moderate, and $\text{RMdSPD} \ge 0.5$ as bad recovery of a parameter. By taking the median, I reduced the influence of potential outliers in the estimation and using proportions enabled me to compare RMdSPD values across parameters and data sets.  
Additionally, I applied a bivariate linear regression with the estimated parameter values as the dependent and the true parameter values as the independent variable to each parameter that has been varied on an interval in part one. Regression slopes closer to one indicated that the model better captured parameter change. Regression intercepts different from zero reflected a bias in parameter estimation.  
To assess state recovery, I computed Cohen's kappa (for all events taken together, not for each event separately) as a measure of agreement between true and estimated states for each generated data set. Cohen's kappa estimates the agreement between two classifiers accounting for the agreement due to chance. Higher kappa values were interpreted as better model accuracy. I adopted the ranges proposed by @Landis1977 to interpret kappa values.
Models that could not be fitted were excluded from the recovery.

[^6]: Note that this section deviates from my proposed analysis plan. However, since I preregistered the changed analysis plan, I treated the results of the simulation as confirmatory.

(ref:sim-ranges) Approximate Ranges of Response Variables Used to Generate Parameter Values in the Simulation
(ref:sim-ranges-note) Units are deg/s (velocity), deg/s^2^ (acceleration), and radians (angle). ~ indicates that the distribution has a peak at this value. Velocity ranges are based on event velocities reported in @Larsson2013. Since I would also like to test the algorithm on extreme data distributions, I extended the ranges beyond those found in typical eye movement data.

```{r sim-ranges}

apa_table(data.frame("Event" = rep(c("Fixation", "Saccade", "PSO", "Smooth pursuit"), each = 3),
                     "Resp variable" = rep(c("Velocity", "Acceleration", "Angle"), 4),
                     "Range" = c("0-50", "0-50", "uniform", "50-1000", "50-500", "~0", "20-100", "10-90", paste("~", expression(pi), sep = ""), "20-100", "0-30", "~0")), 
          caption = "(ref:sim-ranges)",
          note = "(ref:sim-ranges-note)")

```

(ref:sim-parameters-trans) Intervals and Default Parameter Values for the Transition Model in the Simulation
(ref:sim-parameters-trans-note) The transition probability for staying in the same state is denoted by $a_{i=j}$ and the probability for switching to a different state by $a_{i\neq j}$. The number of states in the model is denoted by *k*.

```{r sim-parameters-trans}

sim.pars.trans <- as.data.frame(matrix(c("Interval", "-", "[.01,.99]", "1-a(i=j)/(k-1)",
                                           "Default", "1/k", "0.9", "0.1/(k-1)"), nrow = 2, byrow = T))

apa_table(sim.pars.trans,
          col.names = c("", "Initial\nstate\nprob.", "Trans. prob.\nsame state", "Trans. prob.\nother state"),
          caption = "(ref:sim-parameters-trans)",
          note = "(ref:sim-parameters-trans-note)")

```

(ref:sim-parameters-resp) Intervals and Default Parameter Values for the Response Model in the Simulation
(ref:sim-parameters-resp-note) Shape parameters are denoted by $\alpha$, scale parameters by $\beta$, mean parameters by $\mu$, and concentration parameters by $\kappa$. The default values for the uniform distribution in state one were min = 0 and max = $2\pi$.

```{r sim-parameters-resp}

sim.pars.resp <- as.data.frame(t(matrix(c("[1,5]", "[0.1,0.6]", "[1,5]", "[0.05,0.25]", "-", "-", 
                                  "3", "0.35", "3", "0.25", "-", "-",
                                  "[1,5]", "[5,15]", "[1,5]", "[1,5]", "-", "1/[0.1,10]",
                                  "3", "10", "3", "3", "0", "1",
                                  "[1,5]", "[0.5,1.5]", "[1,5]", "[1,5]", "-", "1/[0.1,10]",
                                  "3", "1", "3", "3", paste(expression(pi), sep = ""), "1",
                                  "[1,5]", "[0.5,1.5]", "[1,5]", "[0.05,0.25]", "-", "1/[0.1,10]",
                                  "3", "1", "3", "0.15", "0", "1"), nrow = 6, byrow = F)))

apa_table(cbind(rep(c("Interval", "Default"), 4), sim.pars.resp),
          col.names = c("", "Shape", "Scale", "Shape", "Scale", "Mean", "Concentration"),
          col_spanners = list(Velocity = c(2, 3), Acceleration = c(4, 5), "Rel. angle" = c(6, 7)),
          stub_indents = list("State 1" = c(1, 2), "State 2" = c(3, 4), "State 3" = c(5, 6), "State 4" = c(7, 8)),
          caption = "(ref:sim-parameters-resp)",
          note = "(ref:sim-parameters-resp-note)")

```

```{r load sim results and helper functions, include=FALSE}

# Load simulation results

for (part in 1:4) {
  
  load(file = paste(here("simulation/part"), part, ".Rdata", sep = ""))
  
}


# Create functions to apply and invert mlogit link function (from depmixS4, Visser & Speekenbrink, 2019)

linkfun <- function(p, base) {
  lfun <- function(p, base) {
    p <- p/sum(p)
    beta <- numeric(length(p))
    if (any(p == 1)) 
      beta[which(p == 1)] = Inf
    else beta[-base] <- log(p[-base]/p[base])
    return(beta)
  }
  if (is.matrix(p)) {
    beta <- t(apply(p, 1, lfun, base = base))
  }
  else {
    beta <- lfun(p, base)
  }
  return(beta)
}

linkinv <- function(eta,base) {
  linv <- function(eta,base) {
    pp <- numeric(length(eta))
    if(any(is.infinite(eta)) || any(eta > log(.Machine$double.xmax)) || any(eta < log(.Machine$double.xmin))) {
      pp[which(is.infinite(eta))] <- 1
      pp[which(eta > log(.Machine$double.xmax))] <- 1 # change this to something better!
    } else {
      expb <- exp(eta)
      sumb <- sum(expb)
      pp[base] <- 1/sumb
      pp[-base] <- expb[-base]/sumb
    }
    return(pp)
  }
  if(is.matrix(eta)) {
    if(ncol(eta)==1) {
      pp <- as.matrix(apply(eta,1,linv,base=base)) # fixes problem with column matrix eta
    } else pp <- t(apply(eta,1,linv,base=base)) 	
  } else {
    pp <- linv(eta,base)
  }
  return(pp)
}


# Create function to transform parameters to normal scale

backtrans <- function(x) {
  
  out <- x
  
  nms <- names(x)
  
  out[str_detect(nms, "(Intercept)")] <- as.vector(apply(matrix(x[str_detect(nms, "(Intercept)")],
                                                        ncol = sqrt(length(x[str_detect(nms, "(Intercept)")])),
                                                        byrow = T), 1, linkinv, base = 1))
  
  out[nms %in% c("shape", "scale", "kappa")] <- exp(x[nms %in% c("shape", "scale", "kappa")])
  
  return(out)
}

```

```{r calculate RMdSPD, include=FALSE}

rmsd <- list()

for (part in 1:4) {
  
  rmsd[[part]] <- lapply(get(paste("estimates.", part, sep = "")), function(x) {
    lapply(x, function(y) {
      sqerr <- lapply(y, function(z) {
        
        err <- try(((backtrans(z$pars.est) - backtrans(z$pars.true))/
                      ifelse(backtrans(z$pars.true) == 0, 2*pi, backtrans(z$pars.true)))^2)
        
        if(is.numeric(err)) {
          
          out <- err
          
        } else {
          
          out <- rep(NA, length(z$pars.true))
          
        }
        
        names(out) <- names(z$pars.true)
        
        return(out)
      })
      
      rows <- length(sqerr)

      nms <- names(sqerr[[1]])

      pars <- matrix(unlist(sqerr), nrow = rows, byrow = T)
      
      msqerr <- apply(pars, 2, median, na.rm = T)
      
      names(msqerr) <- nms
      
      rmsqerr <- sqrt(msqerr)
      
      return(rmsqerr)
    })
  })
}


# Display RMdSPD in data frame

rmsd.data <- lapply(rmsd, function(x) lapply(x, as.data.frame))
rmsd.data <- lapply(rmsd.data, function(x) lapply(x, function(y) {as.data.frame(t(as.matrix(y)))}))
rmsd.data <- lapply(rmsd.data, function(x) lapply(1:length(x), function(y, data) {
  names(data[[y]]) <- names(rmsd[[1]][[y]][[1]])
  return(data[[y]])}, data = x))


# Create plots for part 1

plots.rmsd.1 <- lapply(rmsd.data[[1]], function(x) {
  
  names.pars.varied <- list(bquote(a["i=j"]), bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                            bquote(beta["acc"]), bquote(kappa))
  
  if(ncol(x) == 18) {
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]))
    
  } else if(ncol(x) == 30) {
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]))
    
  } else {
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]), bquote(a["i4"]))
    
  }
  
  names.pars.est <- append(trnames, list(bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                         bquote(beta["acc"]), bquote(mu), bquote(kappa)))
  
  x <- as_tibble(x, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(par.varied = c(0, 1, 2, 3, 4, rep(c(1, 2, 3, 4, 5), (nrow(x) %/% 5)-1)),
           state.varied = c(1, rep(1, 4), rep(2:((nrow(x) %/% 5)), each = 5))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "RMdSPD", names_repair = "unique") %>%
    mutate(state.est = rep(c(rep(1:max(state.varied), max(state.varied)+1),
                         rep(1:max(state.varied), each = 6)), nrow(x)),
           par.est = rep(c(rep(1, max(state.varied)), 
                           rep(2:(max(state.varied)+1), each = max(state.varied)), 
                           rep((max(state.varied)+2):(max(state.varied)+7), max(state.varied))), nrow(x))) %>%
    mutate_at(vars(par.varied, state.varied, par.est, state.est), as.factor) %>%
    mutate(state.varied = fct_recode(state.varied, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"),
           state.est = fct_recode(state.est, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"))

  p <- ggplot(data = data.long, aes(x = par.varied, y = par.est, fill = RMdSPD)) +
    geom_tile() + facet_grid(cols = vars(state.varied), rows = vars(state.est)) +
    scale_x_discrete(name = "Varied parameter", labels = names.pars.varied) +
    scale_y_discrete(name = "Estimated parameter", labels = names.pars.est) +
    scale_fill_distiller(breaks = c(0, 0.1, 0.5, 1), palette = "Spectral")
  
  return(p)
})


# Create plots for parts 2,3, and 4

plots.rmsd.234 <- lapply(2:4, function(y, data) lapply(data[[y]], function(x) {
  
  if(ncol(x) == 18) {
    
    k <- 2
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]))
    
  } else if(ncol(x) == 30) {
    
    k <- 3
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]))
    
  } else {
    
    k <-4
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]), bquote(a["i4"]))
    
  }
  
  names.pars.est <- append(trnames, list(bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                                         bquote(beta["acc"]), bquote(mu), bquote(kappa)))
  
  x <- as_tibble(x, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(cond = 1:3) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "RMdSPD", names_repair = "unique") %>%
    mutate(state.est = rep(c(rep(1:k, k+1),
                             rep(1:k, each = 6)), nrow(x)),
           par.est = rep(c(rep(1, k), 
                           rep(2:(k+1), each = k), 
                           rep((k+2):(k+7), k)), nrow(x))) %>%
    mutate_at(vars(cond, par.est, state.est), as.factor) %>%
    mutate(state.est = fct_recode(state.est, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"))
  
  p <- ggplot(data = data.long, aes(x = par.est, y = RMdSPD, color = cond)) +
    geom_point(position = position_dodge(0.25)) + facet_grid(cols = vars(state.est)) +
    scale_x_discrete(name = "Estimated parameter", labels = names.pars.est) +
    scale_y_continuous(breaks = c(0.1, 0.5, 1, 1.5, 2, 3, 4, 5)) +
    geom_hline(yintercept = 0.1, linetype = "dashed") +
    geom_hline(yintercept = 0.5, linetype = "dashed")
  
  if(y == 2) {
    names.cond <- c("500", "2500", "10000")
    label.cond <- "N"
  } else if (y == 3) {
    names.cond <- c("1", "2", "3")
    label.cond <- bquote(tau["start"])
  } else {
    names.cond <- c("1", "3", "5")
    label.cond <- "Number of\nintervals"
  }
  
  p <- p + scale_color_discrete(name = label.cond, labels = names.cond)
  
  return(p)
}), data = rmsd.data)

```

```{r calculate linear regressions, include=FALSE}

# Calculate regression weights for transition probabilities

regw.tr <- list()

regw.tr <- lapply(get("estimates.1"), function(x) {
  
  varpar <- lapply(x[1], function(y) {
    
    lapply(y, function(z) {
      
      nms <- names(z$pars.true)
      
      pars.tr <- logical(length(z$pars.true))
      
      pars.tr[str_detect(nms, "(Intercept)")] <- T
      
      intpar <- try(cbind(backtrans(z$pars.true[pars.tr]), backtrans(z$pars.est[pars.tr])))
      
      if(is.numeric(intpar)) {
        out <- intpar
      } else {
        out <- matrix(NA, nrow = length(z$pars.true[pars.tr]), ncol = 2)
      }
      
      out <- apply(out, 1, list)
      
      return(out)
    })
  })
  
  df <- list()
  
  for (i in 1:length(varpar[[1]][[1]])) {
    
    df[[i]] <- lapply(varpar, function(y, index) {
      
      lapply(y, function(z) {z[[index]]})
      
    }, index = i)
  }
  
  df <- lapply(df, as.data.frame)

  df <- lapply(df, function(z) {
    
    out <- as.data.frame(t(as.matrix(z)))
    
    names(out) <- c("true", "est")
    
    return(out)
  })
})


# Calculate linear regression weights for response parameters

regw.resp <- list()
  
regw.resp <- lapply(get("estimates.1"), function(x) {
  
  index <- 1:length(x[-1])
  
  varpar <- lapply(index, function(i, y) {
    
    lapply(y[[i]], function(z) {
      
      nms <- names(z$pars.true)
      
      #pars.tr <- logical(length(z$pars.true))
      pars.resp <- logical(length(z$pars.true))
      
      #pars.tr[str_detect(nms, "(Intercept)")] <- T
      pars.resp[nms %in% c("shape", "scale", "kappa")] <- T
      
      intpar <- try(c(backtrans(z$pars.true[pars.resp][i]), backtrans(z$pars.est[pars.resp][i])))
      
      if(is.numeric(intpar)) {
        return(intpar)
      } else {
        return(rep(NA, 2))
      }
    })
  }, y = x[-1])
  
  df <- lapply(varpar, as.data.frame)
  df <- lapply(df, function(z) {

    out <- as.data.frame(t(as.matrix(z)))

    names(out) <- c("true", "est")

    return(out)
  })
})


# Plot linear regressions for transition probabilities

D <- 100

regw.tr.data <- lapply(regw.tr, function(x) lapply(x, function(y) rbind(y)))
regw.tr.data <- lapply(regw.tr.data, function(x) reduce(x, rbind))

plots.lm.tr <- lapply(1:length(regw.tr.data), function(x, y) {
  
  data <- y[[x]] %>% 
    mutate(par = rep(1:(x+1)^2, each = D),
           From = rep(rep(1:(x+1), each = D), x+1),
           To = rep(1:(x+1), each = D*(x+1)))
  
  p <- ggplot(data, aes(x = true, y = est)) + 
    facet_grid(rows = vars(From), cols = vars(To), labeller = label_both) +
    geom_point() + geom_smooth(method = "lm") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    scale_x_continuous(name = "True transition probability") +
    scale_y_continuous(name = "Estimated transition probability")
  
  return(p)
}, y = regw.tr.data)


# Plot linear regressions for response parameters

regw.resp.data <- lapply(regw.resp, function(x) lapply(x, function(y) rbind(y)))
regw.resp.data <- lapply(regw.resp.data, function(x) reduce(x, rbind))

parnames <- list(bquote(alpha["vel;1"]), bquote(beta["vel;1"]), bquote(alpha["acc;1"]), bquote(beta["acc;1"]),
                 bquote(alpha["vel;2"]), bquote(beta["vel;2"]), bquote(alpha["acc;2"]), bquote(beta["acc;2"]), bquote(kappa["2"]),
                 bquote(alpha["vel;3"]), bquote(beta["vel;3"]), bquote(alpha["acc;3"]), bquote(beta["acc;3"]), bquote(kappa["3"]),
                 bquote(alpha["vel;4"]), bquote(beta["vel;4"]), bquote(alpha["acc;4"]), bquote(beta["acc;4"]), bquote(kappa["4"]))

plots.lm.resp <- lapply(regw.resp.data, function(x) {
  
  npar <- nrow(x) %/% D
  
  data <- x %>% 
    mutate(par = rep(1:npar, each = D),
           type = rep(1, 2, 1, 2))
  
  p <- ggplot(data, aes(x = true, y = est)) + 
    facet_wrap(vars(par), scales = "free", labeller = label_bquote(.(parnames[[par]]))) +
    geom_point() + geom_smooth(method = "lm") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    scale_x_continuous(name = "True response parameter") +
    scale_y_continuous(name = "Estimated response parameter")
  
  return(p)
})

```

```{r calculate accuracy, include=FALSE}

# Summarise accuracy

acc.data <- list()

for (part in 1:4) {
  
  acc.data[[part]] <- lapply(get(paste("estimates.", part, sep = "")), function(x) {
    out <- lapply(x, function(y) { 
      out <- lapply(y, function(z) {
        
        if(is.numeric(z$accuracy)) {
          acc <- z$accuracy
        } else {
          acc <- NA
        }
        
        return(acc)
      })
      
      return(as.vector(reduce(out, cbind)))
    })
    
    return(as.data.frame(t(reduce(out, cbind))))
  })
}

# Plot accuracy part 1

plots.acc.1 <- lapply(acc.data[[1]], function(x) {
  
  names.pars.varied <- list(bquote(a["i=j"]), bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                            bquote(beta["acc"]), bquote(kappa))
  
  x <- as_tibble(x, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(par.varied = c(0, 1, 2, 3, 4, rep(c(1, 2, 3, 4, 5), (nrow(x) %/% 5)-1)),
           state.varied = c(1, rep(1, 4), rep(2:((nrow(x) %/% 5)), each = 5))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy") %>%
    mutate_at(vars(par.varied, state.varied), as.factor) %>%
    mutate(state.varied = fct_recode(state.varied, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"))
  
  p <- ggplot(data = data.long, aes(x = par.varied, y = accuracy)) +
    geom_boxplot(outlier.shape = 4) + facet_grid(cols = vars(state.varied)) +
    scale_x_discrete(name = "Varied parameter", labels = names.pars.varied) + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  return(p)
})


# Plot accuracy parts 2, 3, and 4

acc.data.234 <- lapply(acc.data[2:4], function(x) reduce(x, rbind))

plots.acc.234 <- lapply(1:3, function(y, data) {
  
  x <- as_tibble(data[[y]], .name_repair = "unique")
  
  data.long <- x %>%
    mutate(cond = as.factor(rep(1:3, 3)),
           k = as.factor(rep(2:4, each = 3))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy")
  
  p <- ggplot(data = data.long, aes(x = k, y = accuracy, color = cond)) +
    geom_boxplot(outlier.shape = 4) + 
    scale_x_discrete(name = "k (number of states)")  + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  if(y == 1) {
    names.cond <- c("500", "2500", "10000")
    label.cond <- "N"
  } else if (y == 2) {
    names.cond <- c("1", "2", "3")
    label.cond <- bquote(tau["start"])
  } else {
    names.cond <- c("1", "3", "5")
    label.cond <- "Number of\nintervals"
  }
  
  p <- p + scale_color_discrete(name = label.cond, labels = names.cond)
  
  return(p)
}, data = acc.data.234)


# Plot accuracy over interval parts 2 and 4

plots.acc.int.24 <- lapply(c(1, 3), function(y, data) {
  
  x <- as_tibble(data[[y]], .name_repair = "unique")
  
  if(y == 1) {
  
    int <- rep(seq(1, 5, length.out = D), 9) 
  
  } else {
    
    int <- rep(floor(seq(1, 200, length.out = D)), 9)
    
  }
  
  data.long <- x %>%
    mutate(cond = as.factor(rep(1:3, 3)),
           k = as.factor(rep(2:4, each = 3))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy") %>%
    mutate(int = int)
  
  p <- ggplot(data = data.long, aes(x = int, y = accuracy, color = cond)) +
    facet_grid(cols = vars(k), labeller = partial(label_both, sep = " = ")) + 
    geom_point()
  
  if(y == 1) {
    names.cond <- c("500", "2500", "10000")
    label.cond <- "N"
    x.name <- bquote(tau["noise"])
  } else {
    names.cond <- c("1", "3", "5")
    label.cond <- "Number of\nintervals"
    x.name <- "Interval length (in samples)"
  }
  
  p <- p + scale_color_discrete(name = label.cond, labels = names.cond) +
    scale_x_continuous(name = x.name) + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  return(p)
}, data = acc.data.234)


# Exploratory analysis for label switching

load(here("simulation/part3_expl.Rdata"))

labsw <- lapply(1:length(estimates.3), function(x) {
  out <- lapply(estimates.3[[x]], function(y) {
    out <- lapply(y, function(z) {
      
      kappa <- try(cohen.kappa(z$states)$kappa)
      
      states <- z$states
      
      if(x == 2 & is.numeric(kappa) & kappa < 0.95) {
          
        states$y <- ifelse(z$states$y == 3, 2, ifelse(z$states$y == 2, 3, z$states$y))
          
        kappa <- cohen.kappa(states)$kappa
        
        if(is.numeric(kappa) & kappa < 0.95) {
          
          states$y <- ifelse(z$states$y == 3, 1, ifelse(z$states$y == 1, 3, z$states$y))
          
          kappa <- cohen.kappa(states)$kappa
        }
        
        if(is.numeric(kappa) & kappa < 0.95) {
          
          states$y <- ifelse(z$states$y == 3, 2, ifelse(z$states$y == 1, 3, 1))
          
          kappa <- cohen.kappa(states)$kappa
        }
      }
      
      if(x == 3 & is.numeric(kappa) & kappa < 0.5) {
        
        states$y <- ifelse(z$states$y == 3, 2, ifelse(z$states$y == 2, 3, z$states$y))
        
        kappa <- cohen.kappa(states)$kappa
        
        if(is.numeric(kappa) & kappa < 0.5) {

          states$y <- ifelse(z$states$y == 3, 4, ifelse(z$states$y == 4, 3, z$states$y))

          kappa <- cohen.kappa(states)$kappa
        }

        if(is.numeric(kappa) & kappa < 0.5) {

          states$y <- ifelse(z$states$y == 3, 1, ifelse(z$states$y == 1, 3, z$states$y))

          kappa <- cohen.kappa(states)$kappa
        }
        
        if(is.numeric(kappa) & kappa < 0.5) {
          
          kappa <- cohen.kappa(z$states)$kappa
        }
      }
      
      if(is.numeric(kappa)) {return(kappa)
        } else {return(NA)}
    })
    
    return(as.vector(reduce(out, cbind)))
  })
  
  return(t(reduce(out, cbind)))
})

labsw.df <- as.data.frame(reduce(labsw, rbind))


# Plot exploratory analysis results

plots.acc.expl <- lapply(1, function(y, data) {
  
  x <- as_tibble(data, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(cond = as.factor(rep(1:3, 3)),
           k = as.factor(rep(2:4, each = 3))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy")
  
  p <- ggplot(data = data.long, aes(x = k, y = accuracy, color = cond)) +
    geom_boxplot(outlier.shape = 4) + 
    scale_x_discrete(name = "k (number of states)")  + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  names.cond <- c("1", "2", "3")
  label.cond <- bquote(tau["start"])
  
  p <- p + scale_color_discrete(name = label.cond, labels = names.cond)
  
  return(p)
}, data = labsw.df)


```

\newpage

# Results [^5]
## Simulation Study
### Parameter Variation
#### Two States
In the first part of the simulation, I examined how varying the parameters in the HMM affects the deviation of estimated parameters and accuracy of estimated state sequences. Figure \@ref(fig:plot-rmdspd-1-2) displays the RMdSPD between true and estimated parameters depending on which parameter has been manipulated in the HMM. The RMdSPD was below 0.1 for all estimated and manipulated parameters, indicating good recovery [^2]. The regressions between manipulated true and estimated parameters are shown in Figures \@ref(fig:plot-regtr-1-2) and \@ref(fig:plot-regresp-1-2). With one outlier at parameter $\alpha_{acc;1}$, the estimated parameters matched the true parameters very closely. The deviation seemed to increase slightly with parameter magnitude. Thus, parameter change was capture well and the estimation almost unbiased. Considering accuracy, Figure \@ref(fig:plot-acc-1-2) displays Cohen's kappa between true and estimated hidden state sequences. With two exceptions, kappa values were almost one, suggesting nearly perfect agreement. In sum, the HMM with two states recovered parameters and hidden states very well and outliers only occurred rarely.  

[^5]: The design and analysis plan of the simulation study have not been included in my proposal. However, since I preregistered both the design and analysis, I treated the results as confirmatory. In general, the evaluation of an algorithm can hardly be proposed in detail before the algorithm has been developed. Therefore, I chose to report a general results section. Nevertheless, I indicated whenever results were obtained through exploratory analyses that were not planned beforehand.

[^2]: Note that the initial state probability $\rho_i$ has $\textrm{RMdSPD} = 1$. Since the HMM only simulated one state sequence, this parameter is always either zero or one (leading to $\textrm{RMdSPD} = 1$). Therefore, I decided not to include it in the analysis.

(ref:plot-rmdspd-1-2) RMdSPD between true and estimated parameters of the two-state HMM in part one of the simulation. Labels on the x-axis indicate which true parameters have been manipulated and labels on the y-axis show for which estimated parameter the RMdSPD is displayed. Top facet labels specify in which state the parameters have been varied and right facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-1-2, fig.cap="(ref:plot-rmdspd-1-2)"}

print(plots.rmsd.1[[1]])

```

(ref:plot-regtr-1-2) Regression lines between true and estimated transition probabilities for the two-state HMM in part one. Top facet labels show to and right facet labels show from which state the HMM is moving. Dashed lines refer to perfect recovery.

```{r plot-regtr-1-2, fig.cap="(ref:plot-regtr-1-2)"}

print(plots.lm.tr[[1]])

```

(ref:plot-regresp-1-2) Regression lines between true and estimated response parameters of the two-state HMM in part one. Top facet labels indicate response parameters. Dashed lines refer to perfect recovery.

```{r plot-regresp-1-2, fig.cap="(ref:plot-regresp-1-2)"}

print(plots.lm.resp[[1]] + theme(axis.text = element_text(size = 10)))

```

(ref:plot-acc-1-2) Boxplots displaying Cohen's kappa depending on which parameter of the two-state HMM has been manipulated in part one. Top facet labels indicate for which state parameters have been manipulated. Black solid lines symbolize medians. Crosses represent outliers (distance to first/third quartile higher than 1.5 times the inter-quartile range [IQR]).

```{r plot-acc-1-2, fig.cap="(ref:plot-acc-1-2)"}

print(plots.acc.1[[1]])

```

#### Three States
For the simulation with three states, the RMdSPD is shown in Figure \@ref(fig:plot-rmdspd-1-3). When response parameters (other than $a_{i=j}$) were manipulated, the RMdSPDs for $a_{12}$ and $a_{31}$ were consistently below 0.5. Varying $\kappa$ in states two and three led to RMdSPDs below 0.5 in the respective states, which can be interpreted as moderate recovery. Otherwise, RMdSPDs were consistently lower than 0.1, indicating good recovery. Inspecting the regressions between manipulated true and estimated parameters (see Figures \@ref(fig:plot-regtr-1-3) and \@ref(fig:plot-regresp-1-3)) revealed strong and unbiased linear relationships (intercepts close to zero and slopes close to one). Again, the deviation seemed to increase with true parameter magnitude (reverse for kappas). In contrast to the two-state HMM, larger deviations and more outliers were observed. Cohen's kappa values are presented in Figure \@ref(fig:plot-acc-1-3). For most estimated models, the kappas between true and estimated state sequences were above 0.95, meaning almost perfect agreement. However, for some models, I observed kappas clustered around zero or -0.33, which suggests that state labels were switched. To summarize, the three-state HMM had a slightly worse parameter recovery and more outliers than the two-state model but shows a similar good accuracy.  

(ref:plot-rmdspd-1-3) RMdSPD between true and estimated parameters of the three-state HMM in part one of the simulation. Labels on the x-axis indicate which true parameters have been manipulated and labels on the y-axis show for which estimated parameter the RMdSPD is displayed. Top facet labels specify in which state the parameters have been varied and right facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-1-3, fig.cap="(ref:plot-rmdspd-1-3)"}

print(plots.rmsd.1[[2]]+ theme(axis.text = element_text(size = 8)))

```

(ref:plot-regtr-1-3) Regression lines between true and estimated transition probabilities for the three-state HMM in part one. Top facet labels show to and right facet labels show from which state the HMM is moving. Dashed lines refer to perfect recovery.

```{r plot-regtr-1-3, fig.cap="(ref:plot-regtr-1-3)"}

print(plots.lm.tr[[2]])

```

(ref:plot-regresp-1-3) Regression lines between true and estimated response parameters of the three-state HMM in part one. Top facet labels indicate response parameters. Dashed lines refer to perfect recovery.

```{r plot-regresp-1-3, fig.cap="(ref:plot-regresp-1-3)", fig.height=7}

print(plots.lm.resp[[2]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-acc-1-3) Boxplots displaying Cohen's kappa depending on which parameter of the three-state HMM has been manipulated in part one. Top facet labels indicate for which state parameters have been manipulated. Black solid lines symbolize medians and hinges the first and third quartile. Whiskers range from hinges to lowest/highest value within 1.5 times the IQR. Crosses represent outliers.

```{r plot-acc-1-3, fig.cap="(ref:plot-acc-1-3)"}

print(plots.acc.1[[2]])

```

#### Four States
The RMdSPDs for the four-state HMM is shown in Figure \@ref(fig:plot-rmdspd-1-4). For estimated transition probabilities and $\alpha_{vel}$ and $\beta_{vel}$ parameters in states one and four, RMdSPDs were below 0.5, suggesting moderate recovery. Estimated kappa parameters in state four were also often below 0.5 when parameters in states two, three, and four were varied. Otherwise, RMdSPDs were below 0.1, indicating good recovery. Looking at the regressions between true and estimated parameters, Figures \@ref(fig:plot-regtr-1-4) and \@ref(fig:plot-regresp-1-4) illustrate strong and unbiased relationships. However, there were larger deviations and more outliers than in the previous models, especially for states one and four. Accuracy measured by Cohen's kappa ranged between 0.6 and 0.9 for the majority of models, meaning moderate to almost perfect agreement between true and estimated state sequences (see Figure \@ref(fig:plot-acc-1-4)). Here, some kappa values clustered around 0.25 and zero, which, again, can be interpreted as the result of label switching. Summarizing, the parameter recovery for the four-state HMM was slightly worse with even more outliers compared to the three-state model. Especially the recovery of transition parameters in states corresponding to fixations and smooth pursuits decreased.  
Overall, simulations in part one demonstrated that the HMM recovered parameters very well when parameters were manipulated. Deviations from true parameters were mostly small. In the four-state model, estimated transition probabilities for state one and four deviated moderately. Moreover, the HMM estimated state sequences very accurately with decreasing accuracy for the four-state model.

(ref:plot-rmdspd-1-4) RMdSPD between true and estimated parameters of the four-state HMM in part one of the simulation. Labels on the x-axis indicate which true parameters have been manipulated and labels on the y-axis show for which estimated parameter the RMdSPD is displayed. Top facet labels specify in which state the parameters have been varied and right facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-1-4, fig.cap="(ref:plot-rmdspd-1-4)"}

print(plots.rmsd.1[[3]] + theme(axis.text = element_text(size = 6)))

```

(ref:plot-regtr-1-4) Regression lines between true and estimated transition probabilities for the four-state HMM in part one. Top facet labels show to and right facet labels show from which state the HMM is moving. Dashed lines refer to perfect recovery.

```{r plot-regtr-1-4, fig.cap="(ref:plot-regtr-1-4)", fig.height=8}

print(plots.lm.tr[[3]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-regresp-1-4) Regression lines between true and estimated response parameters of the four-state HMM in part one. Top facet labels indicate response parameters. Dashed lines refer to perfect recovery.

```{r plot-regresp-1-4, fig.cap="(ref:plot-regresp-1-4)", fig.height=8}

print(plots.lm.resp[[3]] + theme(axis.text = element_text(size = 6)))

```

(ref:plot-acc-1-4) Boxplots displaying Cohen's kappa depending on which parameter of the four-state HMM has been manipulated in part one. Top facet labels indicate for which state parameters have been manipulated. Black solid lines symbolize medians and hinges the first and third quartile. Whiskers range from hinges to lowest/highest value within 1.5 times the IQR. Crosses represent outliers.

```{r plot-acc-1-4, fig.cap="(ref:plot-acc-1-4)"}

print(plots.acc.1[[3]] + theme(axis.text = element_text(size = 8)))

```

### Sample Size and Noise Variation
#### Two States
In the second part, I varied the sample size of the HMM and added noise to the generated data. For the two-state HMM, the RMdSPDs were above 0.5 for $\beta_{vel}$ and $\beta_{acc}$ in both states (see Figure \@ref(fig:plot-rmdspd-2-2)), suggesting bad recovery. The other estimated parameters showed RMdSPDs close to or below 0.1, which means they were recovered well. Increasing the sample size seemed to improve RMdSPDs for most parameters slightly. For $\beta_{vel}$ and $\beta_{acc}$ in both states, models with 2500 samples had the lowest RMdSPDs. Accuracy measured by Cohen's kappa was almost perfect with kappa values very close to one (see Figure \@ref(fig:plot-acc-2), left plot). To conclude, adding noise only affected the recovery of scale parameters but not the accuracy of the two-state HMM. Increasing the sample size improved the recovery slightly.  

(ref:plot-rmdspd-2-2) RMdSPD between true and estimated parameters of the two-state HMM in part two of the simulation. Colours indicate different sizes of generated data. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-2-2, fig.cap="(ref:plot-rmdspd-2-2)"}

print(plots.rmsd.234[[1]][[1]] + theme(axis.text = element_text(size = 10)))

```

(ref:plot-acc-2) Cohen's kappa depending on the variation of noise added to the data generated by the HMM. Colours indicate different sizes of generated data. Top facet labels indicate the number of states in the HMM.

```{r plot-acc-2, fig.cap="(ref:plot-acc-2)"}

print(plots.acc.int.24[[1]])

```

#### Three States
The RMdSPDs for the $\beta_{vel}$ and $\beta_{acc}$ were above 0.5 in all three states (see Figure \@ref(fig:plot-rmdspd-2-3)), indicating bad recovery. Again, the other estimated parameters were below or close to 0.1, only $a_{12}$ and $a_{31}$ with 500 samples were closer to 0.5. For most parameters across all three states, higher sample sizes had lower RMdSPDs. The accuracy of the estimated models was almost perfect with most kappa values above 0.95 (see Figure \@ref(fig:plot-acc-2), middle plot). Several outliers clustered around kappas of zero and -0.33, signaling label switching. For the three-state HMM, adding noise led to a slightly worse recovery and accuracy overall. Scale parameters of gamma distributions in all three states were only badly recovered.  

(ref:plot-rmdspd-2-3) RMdSPD between true and estimated parameters of the three-state HMM in part two of the simulation. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-2-3, fig.cap="(ref:plot-rmdspd-2-3)"}

print(plots.rmsd.234[[1]][[2]] + theme(axis.text = element_text(size = 8)))

```

#### Four States
RMdSPDs regarding the four-state HMM are displayed in Figure \@ref(fig:plot-rmdspd-2-4). For states one and four, values for most parameters (including all transition probabilities) were above 0.5, suggesting bad recovery. Similarly, RMdSPDs for $\beta_{vel}$ and $\beta_{acc}$ in states two and three were above 0.5. For states two and three, higher samples sizes showed slightly lower RMdSPDs. As in the previous part, most Cohen's kappa values ranged between 0.6 and 0.9, meaning substantial to almost perfect agreement between true and estimated states (Figure \@ref(fig:plot-acc-2), right plot). Multiple kappa values clustered around 0.25 or zero, which can be explained by label switching. In summary, adding noise to data generated by the four-state HMM heavily decreased the recovery for most parameters in states corresponding to fixations and smooth pursuits. For saccade and PSO states, only scale parameters of gamma distributions were badly recovered, but increasing the sample size slightly improved the recovery.  
In general, the HMM recovered parameters well despite noise being added to the data. However, in the four-state model, the parameter recovery for states one and four substantially decreased. In the three- and four-state models, scale parameters of gamma distributions were badly recovered. Increasing the sample size in the HMM slightly improved the recovery of most parameters. The accuracy of the model was slightly lowered when more states were included, but it was neither affected by the noise variability $\tau_{noise}$ nor the sample size.

(ref:plot-rmdspd-2-4) RMdSPD between true and estimated parameters of the three-state HMM in part two of the simulation. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-2-4, fig.cap="(ref:plot-rmdspd-2-4)"}

print(plots.rmsd.234[[1]][[3]] + theme(axis.text = element_text(size = 6)))

```


### Variation of Starting Values
The third part of the simulation investigated how increasing the variation in generating starting values affected parameter recover and accuracy of the HMM. For the two-state HMM, all parameters displayed RMdSPDs lower than 0.1 (see Figure \@ref(fig:plot-rmdspd-3-2)), suggesting good recovery. Cohen's kappa values were slightly lower than 1, indicating almost perfect accuracy (see Figure \@ref(fig:plot-acc-3)). For the three-state HMM, RMdSPDs were lower than 0.1 except for $a_{12}$ and $a_{31}$, which were below 0.5 (see Figure \@ref(fig:plot-rmdspd-3-3)), which means they were moderately well recovered. As in previous parts, Cohen's kappa values were mostly above 0.95 (almost perfect accuracy) with exceptions clustering around zero and -0.33 (see Figure \@ref(fig:plot-acc-3)). Regarding the four state-HMM, RMdSPS of transition probabilities for states one and four were clustered above 0.1, other estimated parameters in the model showed values below 0.1 (see Figure \@ref(fig:plot-rmdspd-3-4)). The HMM showed substantial to almost perfect accuracy, as Cohen's kappa values were mostly above 0.8 with a few values clustering around 0.6, 0.25, and zero (see Figure \@ref(fig:plot-acc-3)). Taken together, increasing the variation in starting values for parameter estimation did neither affect the parameter recovery nor the accuracy.  

(ref:plot-rmdspd-3-2) RMdSPD between true and estimated parameters of the two-state HMM in part three of the simulation. Colours indicate the variation in starting values used to estimate parameters. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-3-2, fig.cap="(ref:plot-rmdspd-3-2)"}

print(plots.rmsd.234[[2]][[1]] + theme(axis.text = element_text(size = 10)))

```

(ref:plot-acc-3) Boxplots displaying Cohen's kappa depending on the number of states in the HMM in part three. Colours indicate the variation in starting values used to estimate parameters. Solid vertical lines symbolize medians and hinges the first and third quartile. Whiskers range from hinges to lowest/highest value within 1.5 times the IQR. Crosses represent outliers.

```{r plot-acc-3, fig.cap="(ref:plot-acc-3)"}

print(plots.acc.234[[2]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-rmdspd-3-3) RMdSPD between true and estimated parameters of the three-state HMM in part three of the simulation. Colours indicate the variation in starting values used to estimate parameters. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-3-3, fig.cap="(ref:plot-rmdspd-3-3)"}

print(plots.rmsd.234[[2]][[2]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-rmdspd-3-4) RMdSPD between true and estimated parameters of the four-state HMM in part three of the simulation. Colours indicate the variation in starting values used to estimate parameters. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-3-4, fig.cap="(ref:plot-rmdspd-3-4)"}

print(plots.rmsd.234[[2]][[3]] + theme(axis.text = element_text(size = 6)))

```

In this part, I explored post-hoc whether clusters of low Cohen's kappa values were due to label switching. According to @Visser2019, label switching occurs when exchanging the order of the states leads to equally likely models. Thus, the model is accurate but has oppositely estimated states compared to the true states. In this case, an HMM with three states that is perfectly accurate but has two labels switched would have a Cohen's kappa of 0. To test if label switching occurred, I manually switched one or two labels post-hoc (see Figure \@ref(fig:plot-acc-3-expl)). It can be seen that this approach resolved low accuracy clusters for three and four states.

(ref:plot-acc-3-expl) Boxplots displaying Cohen's kappa depending on the number of states in the HMM in part three after state labels were switched post-hoc (exploratory analysis). Colours indicate the variation in starting values used to estimate parameters. Solid vertical lines symbolize medians and hinges the first and third quartile. Whiskers range from hinges to lowest/highest value within 1.5 times the IQR. Crosses represent outliers.

```{r plot-acc-3-expl, fig.cap="(ref:plot-acc-3-expl)"}

print(plots.acc.expl[[1]] + theme(axis.text = element_text(size = 8)))

```

### Missing Data
In the last part, data intervals of varying length were set to be missing. Regarding parameter recovery, RMdSPDs were almost exactly mirroring those in the previous part for two, three, and four states (see Figures \@ref(fig:plot-rmdspd-4-2), \@ref(fig:plot-rmdspd-4-3), and \@ref(fig:plot-rmdspd-4-4), respectively). Looking at accuracies, Figure \@ref(fig:plot-acc-4) illustrates an interaction between the length of and amount of missing data intervals. Cohen's kappa values linearly decreased with the length of missing data intervals and the decrease became linearly steeper when more intervals were included. For two and three state models, kappas started at almost one (almost perfect) and decreased to 0.6 (substantial accuracy) for five missing intervals. For the four-state model, kappas started around 0.85 (almost perfect) and decreased to 0.5 (substantial accuracy) for five missing intervals. As in previous parts, several kappa values clustered around zero and -0.33 for the three-state model and around 0.6, 0.25, and zero for the four-state model. In total, missing data did not affect the parameter recovery but linearly decreased the accuracy of the model from a nearly perfect to a moderate extent.  
With a few exceptions, the simulation study revealed that the HMM recovered parameters well and accurately estimated the true state sequences. The most critical decrease in recovery occurred when noise was added to the data generated by models including four states. In the noise condition, scale parameters of gamma distributions were often badly recovered. Higher sample sizes slightly improved parameter recovery, but neither variation in starting values nor missing data affected it. In contrast, the accuracy of the model was linearly decreasing with more data missing but not influenced by manipulating parameters, noise, or starting value variation. Adding more states to the HMM generally decreased the parameter recovery and the accuracy.

(ref:plot-rmdspd-4-2) RMdSPD between true and estimated parameters of the two-state HMM in part four of the simulation. Colours indicate the number of missing data intervals. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-4-2, fig.cap="(ref:plot-rmdspd-4-2)"}

print(plots.rmsd.234[[3]][[1]] + theme(axis.text = element_text(size = 10)))

```

(ref:plot-rmdspd-4-3) RMdSPD between true and estimated parameters of the three-state HMM in part four of the simulation. Colours indicate the number of missing data intervals. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-4-3, fig.cap="(ref:plot-rmdspd-4-3)"}

print(plots.rmsd.234[[3]][[2]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-rmdspd-4-4) RMdSPD between true and estimated parameters of the four-state HMM in part four of the simulation. Colours indicate the number of missing data intervals. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-4-4, fig.cap="(ref:plot-rmdspd-4-4)"}

print(plots.rmsd.234[[3]][[3]] + theme(axis.text = element_text(size = 6)))

```

(ref:plot-acc-4) Cohen's kappa depending on the length of missing data intervals. Colours indicate the number of missing data intervals. Top facet labels indicate the number of states in the HMM.

```{r plot-acc-4, fig.cap="(ref:plot-acc-4)"}

print(plots.acc.int.24[[2]])

```

## Validation
### Starting Values and Model Fitting
To test my hypotheses, I applied gazeHMM on two benchmark data sets. Note that, for the @Ehinger2019 data set, I only fitted models to the Eyelink data to reduce the complexity of the analysis. For both data sets, I used the same set of starting values (initial state model: $\rho=1/k$; transition model: $a_{i=j}=0.9$ and $a_{i\neq j}=0.1/k$; response model: see Table \@ref(tab:tab-starting-values)) to estimate the parameters of the HMM. In contrast to the simulation study, generating random starting values often led to bad model fits and label switching between states. To improve the fitting of the gamma distributions, velocity and acceleration signals were scaled down by factor 100 [^7] (so were the starting values for their gamma distributions). The algorithm was applied separately for every participant and every condition, task, or block. For the data set by @Andersson2017, the algorithm was successfully fitted for every participant and condition. Examples of event classification and parameter estimates from gazeHMM are presented in Appendix A.

[^7]: Scaling down by factor 100 differs from the simulation study (scaling down by 10). The algorithm allows the user to manually specify this factor and in this case, factor 100 led to better model fits than factor 10. Thus, I chose it exploratorily.

(ref:tab-starting-values) Starting Values for the Response Model for Fitting gazeHMM to Benchmark Data
(ref:tab-starting-values-note) Starting values for velocity and acceleration signals are shown before scaling down by factor 100. Values for event 5 were chosen so that they approximately match plausible distributions for microsaccades.

```{r tab-starting-values}

tab.start <- as.data.frame(matrix(c(10, 10, 10, 10, NA, NA,
                      50, 50, 50, 50, 0, 10,
                      50, 50, 50, 50, pi, 10,
                      20, 20, 20, 20, 0, 10,
                      20, 20, 50, 50, 0, 10), nrow = 5, byrow = T))

colnames(tab.start) <- c("Shape", "Scale", "Shape", "Scale", "Mean", "Concentration")
rownames(tab.start) <- c("Fixation", "Saccade", "PSO", "Pursuit", "Event 5")

apa_table(as.data.frame(tab.start),
          col_spanners = list(Velocity = c(2, 3), Acceleration = c(4, 5), "Rel. angle" = c(6, 7)),
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-starting-values)",
          note = "(ref:tab-starting-values-note)")

```


### Model Comparison

```{r load validation results, include=FALSE}

load(here("validation/Andersson2017_fitted.Rdata"))
load(here("validation/Andersson2017_raw.Rdata"))

```

```{r calculate Schwarz weights, include=FALSE}

# Compute Schwarz weights

schwarz.weights <- function(bic, na.rm = T) {
  
  d.bic <- bic - min(bic, na.rm = na.rm) # eq 2
  
  exp(-0.5 * d.bic)/sum(exp(-0.5 * d.bic), na.rm = na.rm) # eq 4
  
}

A2017.bic <- lapply(A2017.fit, function(stim) {
  out <- lapply(stim, function(subj) {
    out <- lapply(1:length(subj), function(mod) {
      
      if(mod == 1) {
        
        bic <- try(-2*subj[[mod]][["LL"]] + 6*log(subj[[mod]][["N"]]))
        
      } else {
        
        bic <- try(BIC(subj[[mod]]$model))
        
      }
      
      return(ifelse(is.numeric(bic), bic, NA))
    })
    
    return(schwarz.weights(unlist(out)))
  })
  
  df <- as.data.frame(reduce(out, rbind))
  
  names(df) <- paste("model_", 1:length(stim[[1]]), sep = "")
  
  return(df)
})


# Create Schwarz weight plots

bic.plot <- lapply(A2017.bic, function(x) {
  
  data.long <- x %>% 
    mutate(subject = 1:nrow(x)) %>%
    pivot_longer(names(x), names_to = "model", values_to = "weight") %>%
    mutate_at(c("subject", "model"), as.factor)
  
  p <- ggplot(data.long, aes(x = model, y = subject)) + geom_tile(aes(fill = weight)) +
    scale_x_discrete(name = "Number of states", labels = as.character(1:5)) +
    scale_y_discrete(name = "Subject") +
    scale_fill_distiller(name = "Schwarz\nweight", breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), palette = "Spectral")
  
  return(p)
})

```

To examine whether gazeHMM accurately describes eye movements, I applied gazeHMM with one, two, three, four, and five states on two benchmark data sets and compared the Schwarz weights of the HMMs. The starting values for the fifth state were chosen to represent distributions for microsaccade samples (velocity and sample-to-sample angle as for smooth pursuits, acceleration as for saccades and PSOs). Schwarz weights are the result of transforming a set of BIC values [@Wagenmakers2004]. They can be interpreted as the probability for a model to have generated the data it was fitted to. For the @Andersson2017 data set, I expected the highest weights for the three-state models in the image condition. For the moving dots and video conditions, I predicted the highest weights for the four-state model. Figure \@ref(fig:plot-schwarz-weights-img) shows the Schwarz weights for different subjects and models in the image condition. For all subjects, the five-state model displayed the highest weight, suggesting that it most likely generated the data. In the moving dots condition, the five-state model had the highest weights for most subjects, but the one-, three, and four-state models exhibited the highest weights for two subjects each (see Figure \@ref(fig:plot-schwarz-weights-dots)). The video condition had the same pattern as the image condition, as the five-state model consistently had the highest weights (see Figure \@ref(fig:plot-schwarz-weights-vid)). In all three conditions, response parameters of the fifth state were mostly similar to those of the smooth pursuit state.  
In contrast to my hypothesis, applying different HMMs to the @Andersson2017 data suggests that the five-state model has most likely generated the data. Except for the moving dots condition, there was little variation in the Schwarz weights, indicating large differences in the model likelihoods. For the @Ehinger2019 data, I predicted a one-state model to be preferred. Here, the model comparison for the yielded similar results (see Appendix A), contradicting my hypothesis.  
A recent model recovery study showed that the BIC tended to prefer overly complex HMMs when they were misspecified [e.g., the conditional independence assumption was violated; @Pohle2017]. Instead, the integrated completed likelihood (ICL) criterion [@Biernacki2000] performed better in choosing the correct data generating model. Therefore, I computed the weighted ICL criterion for the HMMs fitted to the @Andersson2017 data set as an exploratory analysis. Using the ICL as the model selection criterion yielded very similar results to the BIC (see Appendix A). The preference for the five-state model was even more consistent across conditions and subjects.

(ref:plot-schwarz-weights-img) Schwarz weights displayed for each subject and HMMs with different numbers of states. Models were applied to the image condition of the @Andersson2017 data set. Higher weights indicate better model fit.

```{r plot-schwarz-weights-img, fig.cap="(ref:plot-schwarz-weights-img)"}

print(bic.plot[[2]])

```

(ref:plot-schwarz-weights-dots) Schwarz weights displayed for each subject and HMMs with different numbers of states. Models were applied to the moving dots condition of the @Andersson2017 data set. Higher weights indicate better model fit.

```{r plot-schwarz-weights-dots, fig.cap="(ref:plot-schwarz-weights-dots)"}

print(bic.plot[[1]])

```

(ref:plot-schwarz-weights-vid) Schwarz weights displayed for each subject and HMMs with different numbers of states. Models were applied to the video condition of the @Andersson2017 data set. Higher weights indicate better model fit.

```{r plot-schwarz-weights-vid, fig.cap="(ref:plot-schwarz-weights-vid)"}

print(bic.plot[[3]])

```

### Comparison to Other Algorithms

```{r calculate duration RMSD, include=FALSE}

# Calculate event descriptives for human coders

fr <- 500

A2017.events <- lapply(A2017, function(stim) {
  lapply(1:4, function(e) {
    out <- lapply(stim, function(df) {
      
      if(all(is.nan(df$t) || df$t == 0)) {
        
        df$t <- seq(0, length(df$t)-1)/fr
        
      } else {
        
        df$t <- (df$t - df$t[1])/1e6
        
        df <- df[df$t >= 0,]
        
      }
      
      counter_MN <- 1
      counter_RA <- 1
      
      number_MN <- numeric(nrow(df))
      number_RA <- numeric(nrow(df))
      
      number_MN[1] <- 1
      number_RA[1] <- 1
      
      for (i in 2:nrow(df)) {
        if(df$label_MN[i] != df$label_MN[i-1]) counter_MN <- counter_MN + 1
        if(df$label_RA[i] != df$label_RA[i-1]) counter_RA <- counter_RA + 1
        
        number_MN[i] <- counter_MN
        number_RA[i] <- counter_RA
    
      }
      
      dur_MN <- numeric(max(number_MN))
      event_MN <- numeric(max(number_MN))
      
      for (n in unique(number_MN)) {
        if(n > 1) {
          dur_MN[n] <- max(df$t[number_MN == n], na.rm = T) - max(df$t[number_MN == (n-1)], na.rm = T)
        } else {
          dur_MN[n] <- max(df$t[number_MN == n], na.rm = T) - min(df$t[number_MN == n], na.rm = T)
        }
        
        event_MN[n] <- max(df$label_MN[number_MN == n], na.rm = T)
      }
      
      dur_RA <- numeric(max(number_RA))
      event_RA <- numeric(max(number_RA))
      
      for (n in unique(number_RA)) {
        if(n > 1) {
          dur_RA[n] <- max(df$t[number_RA == n], na.rm = T) - max(df$t[number_RA == (n-1)], na.rm = T)
        } else {
          dur_RA[n] <- max(df$t[number_RA == n], na.rm = T) - min(df$t[number_RA == n], na.rm = T)
        }
        
        event_RA[n] <- max(df$label_RA[number_RA == n], na.rm = T)
      }
      
      return(list(dur_MN[event_MN == e], dur_RA[event_RA == e]))
    })
    
    df <- reduce(out, cbind)
    
    dur_MN <- unlist(df[1,])
    dur_RA <- unlist(df[2,])
    
    return(data.frame(MN = c(mean(dur_MN), sd(dur_MN), length(dur_MN)),
                      RA = c(mean(dur_RA), sd(dur_RA), length(dur_RA))))
  })
})


# Calculate RMSD between event distribution descriptives

ref <- list(dot = list(fix = list(CDT = c(60, 127, 165),
                                  EM = c(NA, NA, NA),
                                  IDT = c(323, 146, 8),
                                  IKF = c(217, 184, 72),
                                  IMST = c(268, 140, 12),
                                  IHMM = c(214, 286, 67),
                                  IVT = c(203, 282, 71),
                                  NH = c(380, 333, 30),
                                  BIT = c(189, 113, 67),
                                  LNS = c(NA, NA, NA)),
                       sac = list(CDT = c(NA, NA, NA),
                                  EM = c(17, 14, 93),
                                  IDT = c(32, 14, 10),
                                  IKF = c(60, 26, 29),
                                  IMST = c(13, 5, 18),
                                  IHMM = c(41, 17, 27),
                                  IVT = c(36, 14, 28),
                                  NH = c(43, 16, 42),
                                  BIT = c(NA, NA, NA),
                                  LNS = c(26, 11, 53)),
                       pso = list(NH = c(24, 12, 17),
                                  LNS = c(20, 9, 31))),
            img = list(fix = list(CDT = c(397, 559, 251),
                                  EM = c(NA, NA, NA),
                                  IDT = c(399, 328, 242),
                                  IKF = c(174, 239, 513),
                                  IMST = c(304, 293, 333),
                                  IHMM = c(133, 216, 701),
                                  IVT = c(114, 204, 827),
                                  NH = c(258, 299, 292),
                                  BIT = c(209, 136, 423),
                                  LNS = c(NA, NA, NA)),
                       sac = list(CDT = c(NA, NA, NA),
                                  EM = c(25, 22, 787),
                                  IDT = c(25, 15, 258),
                                  IKF = c(62, 37, 353),
                                  IMST = c(17, 10, 335),
                                  IHMM = c(48, 26, 368),
                                  IVT = c(41, 22, 373),
                                  NH = c(50, 20, 344),
                                  BIT = c(NA, NA, NA),
                                  LNS = c(29, 12, 390)),
                       pso = list(NH = c(28, 13, 237),
                                  LNS = c(25, 9, 319))),
            vid = list(fix = list(CDT = c(213, 297, 211),
                                  EM = c(NA, NA, NA),
                                  IDT = c(554, 454, 48),
                                  IKF = c(258, 296, 169),
                                  IMST = c(526, 825, 71),
                                  IHMM = c(234, 319, 194),
                                  IVT = c(202, 306, 227),
                                  NH = c(429, 336, 83),
                                  BIT = c(248, 215, 170),
                                  LNS = c(NA, NA, NA)),
                       sac = list(CDT = c(NA, NA, NA),
                                  EM = c(20, 16, 252),
                                  IDT = c(24, 53, 41),
                                  IKF = c(55, 20, 107),
                                  IMST = c(18, 10, 76),
                                  IHMM = c(42, 18, 109),
                                  IVT = c(36, 16, 112),
                                  NH = c(44, 18, 104),
                                  BIT = c(NA, NA, NA),
                                  LNS = c(28, 12, 122)),
                       pso = list(NH = c(28, 13, 78),
                                  LNS = c(24, 10, 87))))


A2017.rmsd <- lapply(2:4, function(k) {
  lapply(1:length(A2017.fit), function(x) { 
    out <- lapply(1:k, function(y) {
      out <- lapply(A2017.fit[[x]], function(z) {
        
        if(class(z[[k]]) == "gazeHMM") z[[k]]$events[[y]]
        
      })
      
      df <- try(reduce(out, rbind))
      
      alg <- try(c(mean(df$dur, na.rm = T), sd(df$dur, na.rm = T), nrow(df)))
      
      if(y == 4) {
        M <- try(cbind(A2017.events[[x]][[y]], alg))
      } else {
        M <- try(cbind(A2017.events[[x]][[y]], alg, as.data.frame(ref[[x]][[y]])))
        M[1:2,4:ncol(M)] <- M[1:2,4:ncol(M)]/1e3
      }
      
      M.norm <- try(apply(M, 1, function(x) {(x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T))}))
      
      if(y == 4) {
        rmsd <- try(sum(sqrt((M.norm[3,] - colSums(M.norm[1:2,]/2))^2)))
      } else {
        rmsd <- try(apply(M.norm[3:nrow(M.norm),], 1, function(r) {sum(sqrt((r - colSums(M.norm[1:2,]/2))^2))}))
      }
      
      coder <- try(sum(sqrt((M.norm[1,] - M.norm[2,])^2)))
      
      rmsd <- c(coder, coder, rmsd)
      
      return(rbind(M, rmsd))
    })
  })
})


# Create tables for RMSDs

rmsd.table <- lapply(1:length(A2017.rmsd), function(x) {
  out <- lapply(1:length(A2017.rmsd[[x]]), function(y) {
    out <- lapply(1:length(A2017.rmsd[[x]][[y]]), function(z) {
      
      tabmat <- t(apply(A2017.rmsd[[x]][[y]][[z]], 2, function(d) {as.character(round(d, 3))}))
      
      # tabmat[3,] <- paste("**", tabmat[3,], "**", sep = "")
      
      colnames(tabmat) <- c("Mean", "SD", "Events", "RMSD")
      rownames(tabmat)[1:3] <- c("coderMN", "coderRA", "gazeHMM")
      
      return(cbind(z, rownames(tabmat), tabmat))
    })
    
    cbind(y, reduce(out, rbind))
  })
  
  df <- as.data.frame(reduce(out, rbind), stringsAsFactors = F) %>%
    mutate(Condition = factor(y, labels = c("moving dots", "image", "video"))) 
  
  lapply(1:(x+1), function(x) {
    
    df <- df %>% dplyr::filter(z == x) %>%
      dplyr::select(-c("z", "y")) %>%
      rename(Algorithm = V3) %>%
      pivot_wider(names_from = "Condition", names_glue = "{Condition}_{.value}", names_sort = T, values_from = c("Mean", "SD", "Events", "RMSD")) 
    
    df %>% dplyr::select(c("Algorithm", "image_Mean", "image_SD", "image_Events", "image_RMSD",
                           "moving dots_Mean", "moving dots_SD", "moving dots_Events", "moving dots_RMSD",
                           "video_Mean", "video_SD", "video_Events", "video_RMSD"))
  })
})

```

```{r calculate Cohen s kappa and confusion matrices, include= FALSE}

# Compute Cohen's kappa and confusion matrices

ref.acc <- matrix(c(0.92, 0.81, 0.83, 0.95, 0.91, 0.94, 0.88, 0.82, 0.83, NA, NA, NA,
                    0.92, 0.84, 0.82, 0.95, 0.91, 0.94, 0.88, 0.80, 0.81, NA, NA, NA,
                    0.38, 0.06, 0.11, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.00, 0.00, 0.00, 0.64, 0.66, 0.67, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.36, 0.00, 0.03, 0.45, 0.26, 0.38, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.63, 0.03, 0.14, 0.58, 0.46, 0.59, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.38, 0.00, 0.03, 0.54, 0.30, 0.52, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.67, 0.03, 0.13, 0.69, 0.60, 0.71, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.67, 0.03, 0.13, 0.75, 0.63, 0.76, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.52, 0.00, 0.01, 0.67, 0.60, 0.68, 0.24, 0.20, 0.25, NA, NA, NA,
                    0.67, 0.03, 0.14, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.00, 0.00, 0.00, 0.81, 0.75, 0.81, 0.64, 0.59, 0.63, NA, NA, NA), ncol = 12, byrow = T)

A2017.acc <- lapply(2:4, function(k) {
  lapply(1:length(A2017.fit), function(x) { 
    out <- lapply(1:length(A2017.fit[[x]]), function(y) {
      
      if(class(A2017.fit[[x]][[y]][[as.character(k)]]) == "gazeHMM") {
        cod.MN <- as.data.frame(cbind(A2017.fit[[x]][[y]][[as.character(k)]]$samples$label, A2017[[x]][[y]]$label_MN))
        cod.RA <- as.data.frame(cbind(A2017.fit[[x]][[y]][[as.character(k)]]$samples$label, A2017[[x]][[y]]$label_RA))
        
        return(rbind(cod.MN, cod.RA))
      }
    })
    
    df <- reduce(out, rbind)
    
    df[df[,1] == 0,1] <- 5
    
    events <- lapply(1:k, function(e) {
      
      alg <- df[,1] == e
      cod <- df[,2] == e
      
      mat <- matrix(c(sum(alg & cod, na.rm = T), sum(alg & !cod, na.rm = T), 
                      sum(!alg & cod, na.rm = T), sum(!alg & !cod, na.rm = T)), 2, 2)
      
      over <- mat[2,1]/sum(mat[,1])
      under <- mat[1,2]/sum(mat[,2])
      
      return(list(kappa = cohen.kappa(mat)$kappa, 
                  over = over,
                  under = under))
    })
    
    ratio <- mean(df[,1] != df[,2], na.rm = T)
    
    conmat <- table(df[,1], df[,2])
    
    return(list(events = events, ratio =  ratio, conmat = conmat))
  })
})


# Create tables for Cohen's kappa and confusion matrices

kappa.table <- lapply(A2017.acc, function(x) {
  out <- lapply(x, function(y){
    out <- lapply(y$events, function(z){
      return(round(z$kappa, 3))
    })
    
    return(reduce(out, cbind))
  })
  
  tabmat <- reduce(out, rbind)
  
  tabmat <- rbind(tabmat[2,], tabmat[1,], tabmat[3,])
  
  return(as.vector(tabmat))
})

conf.table <- lapply(A2017.acc, function(x) {
  out <- lapply(x, function(y){
    
    tabmat <- apply(y$conmat, 2, as.character)
    
    # diag(tabmat)[1:(nrow(tabmat)-1)] <- paste("**", diag(tabmat)[1:(nrow(tabmat)-1)], "**", sep = "")
    
    # tabmat[nrow(tabmat), 5] <- paste("**", tabmat[nrow(tabmat), 5], "**", sep = "")

    colnames(tabmat) <- c("Fixations", "Saccades", "PSOs", "Pursuits", "Blinks", "Other")
    rownames(tabmat) <- c("Fixations", "Saccades", "PSOs", "Pursuits", "Blinks", "Other")[1:nrow(tabmat)]
    rownames(tabmat)[nrow(tabmat)] <- "Blinks"

    return(as.data.frame(tabmat))
  })
})


# Create figure for disagreement ratio

ref.disag <- list(c(7, 7, A2017.acc[[3]][[2]]$ratio*100, A2017.acc[[2]][[2]]$ratio*100, 23, 92, 20, 24, 20, 20, 19, 32, 31, 84),
                  c(11, 11, A2017.acc[[3]][[1]]$ratio*100, A2017.acc[[2]][[1]]$ratio*100, 89, 96, 86, 85, 86, 84, 84, 93, 89, 93),
                  c(19, 19, A2017.acc[[3]][[3]]$ratio*100, A2017.acc[[2]][[3]]$ratio*100, 64, 95, 61, 62, 61, 59, 59, 70, 67, 92))

disag.df <- data.frame(Image = ref.disag[[1]], Dots = ref.disag[[2]], Video = ref.disag[[3]],
                       Algorithm = c("coderMN", "coderRA", "gazeHMM-4", "gazeHMM-3", "CDT", "EM", "IDT", "IKF", "IMST", "IHMM", "IVT", "NH", "BIT", "LNS"),
                       stringsAsFactors = F)

disag.df$Algorithm <- factor(disag.df$Algorithm, levels = unique(disag.df$Algorithm))

disag.df <- disag.df %>% pivot_longer(c("Image", "Dots", "Video"), names_to = "Condition", values_to = "Ratio")

disag.plot <- ggplot(disag.df, aes(x = Algorithm, y = Ratio, fill = Condition)) + 
  geom_col(position = "dodge") + 
  scale_y_continuous(name = "Disagreement (in %)", limits = c(0, 100), breaks = c(0, 0.25, 0.5, 0.75, 1)*100) +
  scale_x_discrete(name = "") + coord_flip()

```

#### Event Durations
The hypothesis that gazeHMM improves the classification performance of eye movement events was investigated by comparing it against other algorithms. Therefore, I applied gazeHMM with four events to the @Andersson2017 data set. I calculated the RMSD [^3] of event durations as described in the original article by Andersson et al. for all algorithms included in the study plus gazeHMM: First, the mean and standard deviation of event durations as well as the number of events were normalized to [0,1] for each coder and algorithm. Second, the sum of the root square deviation between each algorithm and the average of the two human coders is computed. This metric can only be relatively interpreted [@Andersson2017].  
Table \@ref(tab:tab-rmsd-4-fix) shows that, in all three conditions, gazeHMM has a higher RMSD than all the the other algorithms for classified fixations. The high RMSD values were mostly due to a large amount of very short fixations.  
For saccades, gazeHMM has a relatively low RMSD in the image and video condition and a relatively high RMSD in the moving dots condition (see Table \@ref(tab:tab-rmsd-4-sac)). Here, gazeHMM overestimated the duration of saccades, as indicated by high mean and SD durations.  
For PSOs, gazeHMM had the highest RMSD in the image and video condition (see Table \@ref(tab:tab-rmsd-4-pso)). In the moving dots condition, its RMSD was lower than for NH [@Nystrom2010] but higher than for LNS [@Larsson2013]. Here, gazeHMM underestimated the number of PSOs and their duration compared to the human coders.  
No other algorithm in the study was designed to parse smooth pursuits, but gazeHMM classified a large number of short smooth pursuits, causing a substantially higher RMSD than the human coders (see Figure \@ref(tab:tab-rmsd-4-sp)).  
In sum, gazeHMM did not outperform all other algorithms regarding the RMSD of event durations compared to human coders. The duration of fixations and smooth pursuits was largely underestimated and the number of these two events largely overestimated. For saccades and PSOs, gazeHMM estimated relatively accurate durations and event numbers, but did not reach the best algorithms.

(ref:tab-rmsd-4-fix) Fixation Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-fix-note) Durations are displayed in seconds. gazeHMM classified four events. RMSD = root mean square deviation. Table adapted from @Andersson2017.

```{r tab-rmsd-4-fix}

apa_table(rmsd.table[[3]][[1]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-rmsd-4-fix)",
          note = "(ref:tab-rmsd-4-fix-note)")

```

(ref:tab-rmsd-4-sac) Saccade Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-sac-note) Durations are displayed in seconds. gazeHMM classified four events. RMSD = root mean square deviation. Table adapted from @Andersson2017.

```{r tab-rmsd-4-sac}

apa_table(rmsd.table[[3]][[2]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-rmsd-4-sac)",
          note = "(ref:tab-rmsd-4-sac-note)")

```

(ref:tab-rmsd-4-pso) PSO Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-pso-note) Durations are displayed in seconds. gazeHMM classified four events. RMSD = root mean square deviation. Table adapted from @Andersson2017.

```{r tab-rmsd-4-pso}

apa_table(rmsd.table[[3]][[3]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-rmsd-4-pso)",
          note = "(ref:tab-rmsd-4-pso-note)")

```

(ref:tab-rmsd-4-sp) Smooth Pursuit Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-sp-note) Durations are displayed in seconds. gazeHMM classified four events. RMSD = root mean square deviation. Table adapted from @Andersson2017.

```{r tab-rmsd-4-sp}

apa_table(rmsd.table[[3]][[4]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-rmsd-4-sp)",
          note = "(ref:tab-rmsd-4-sp-note)")

```

[^3]: In my proposal, I declared to compute the RMSE instead of RMSD. However, since I conducted an in-sample comparison and it was used by @Andersson2017, the term *deviation* is more appropriate (i.e., RMSD).

#### Sample-to-sample Agreement
Additionally, I compared gazeHMM to the other algorithms by using Cohen's kappa as a measure of sample-to-sample agreement between algorithms and human coders (see Table \@ref(tab:tab-kappa-4)). Absolute kappa values were interpreted according to @Landis1977.  
For fixations, absolute kappa values in all three conditions indicated a slight to fair agreement between gazeHMM and human coders. Compared to the other algorithms, the agreement of gazeHMM was the lowest in the image condition but the highest in the moving dots and video condition.  
For saccades, gazeHMM showed kappa values that correspond to a moderate to substantial agreement and were relatively high compared to the other algorithms. However, gazeHMM never reached the highest agreement.  
For PSOs, absolute kappa values showed slight to fair agreement agreement to human coders. It was relatively low in the moving dots condition and second-rate in the image and video conditions compared to NH and LNS.  
The agreement for smooth pursuits was moderate in the moving dots condition, fair in the video condition, and slight in the image condition. No other algorithm in the study was designed to detect smooth pursuits.  
In sum, gazeHMM revealed a relatively high sample-to-sample agreement to human coders for saccades. For PSOs and smooth pursuits, I found fair to moderate agreement. The performance for fixations was absolutely and relatively low.

(ref:tab-kappa-4) Cohen's Kappa Between Human Coders and Algorithms for Different Conditions and Events
(ref:tab-kappa-4-note) Negative values were set to zero. gazeHMM classified four events. Table adapted from @Andersson2017.

```{r tab-kappa-4}

tab.kappa.4 <- cbind(c("coderMN", "coderRA", "gazeHMM", "CDT", "EM", "IDT", "IKF", "IMST", "IHMM", "IVT", "NH", "BIT", "LNS"),
                     as.data.frame(rbind(ref.acc[1:2,], kappa.table[[3]], ref.acc[3:nrow(ref.acc),]))) 

apa_table(tab.kappa.4,
          col.names = c("Algorithm", rep(c("Image", "Dots", "Video"), 4)),
          col_spanners = list("Fixations" = c(2, 4), "Saccades" = c(5, 7), "PSOs" = c(8, 10), "Smooth pursuit" = c(11, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-kappa-4)",
          note = "(ref:tab-kappa-4-note)")

```

#### Disagreement and Confusion
At last, I computed the overall disagreement [^4] as the percentage of all samples taken together that were classified differently by gazeHMM than the human coders. Figure \@ref(fig:plot-disag) shows that in the image condition, gazeHMM with four events had a higher overall disagreement by between 10 and 20% than all but two of the other algorithms. In the moving dots and video conditions, the disagreement was between 20 and 30% lower than for all other algorithms. Notably, gazeHMM's disagreement was similar across conditions (around 45%). In summary, gazeHMM had a lower overall disagreement than all other algorithms in the moving dots and video conditions but higher disagreement in the image condition.

[^4]: Even though I proposed to compute the *disagreement ratio* and @Andersson2017 also used that term, the comparison technically involves percentages and not ratios. Thus, I decided to use only the term *disagreement* instead.

(ref:plot-disag) Disagreement between algorithms and human coders for different conditions (in %). gazeHMM-3 classified three and gazeHMM-4 classified four events. Figure adapted from @Andersson2017.

```{r plot-disag, fig.cap="(ref:plot-disag)"}

print(disag.plot)

```

(ref:tab-conf-4) Confusion Matrix Between gazeHMM (Rows) and Human Coders (Columns) for Different Conditions
(ref:tab-conf-4-note) gazeHMM classified four events and blinks.

```{r tab-conf-4}

tab.conf.4 <- cbind(Event = rep(c("Fixations", "Saccades", "PSOs", "Pursuits", "Blinks"), 3), reduce(conf.table[[3]], rbind)) %>%
  mutate(Condition = rep(c("Moving dots", "Image", "Video"), each = 5)) %>%
  arrange(Condition) %>%
  dplyr::select(-Condition)

apa_table(tab.conf.4,
          stub_indents = list("Image" = c(1:5), "Moving dots" = c(6:10), "Video" = c(11:15)),
          midrules = c(6, 12),
          caption = "(ref:tab-conf-4)",
          note = "(ref:tab-conf-4-note)")

```

Contrary to my second hypothesis, applying gazeHMM to a benchmark data set revealed that it did not outperform all other algorithms in terms of RMSD and sample-to-sample agreement. The two criteria rather indicated a moderate performance of gazeHMM. However, the overall disagreement was lower for gazeHMM than for all other algorithms in the dynamic conditions but the highest in the static image condition.  
Looking at the confusion matrix in Table \@ref(tab:tab-conf-4), it can be seen that gazeHMM confused fixations and smooth pursuit to a large extend. In the static image condition, it overclassified smooth pursuit events, while they were underclassified in the dynamic conditions.

### gazeHMM With Three States
Even though it was not confirmed by the model comparison, applying gazeHMM with three states to static data seems theoretically more appropriate. Including only three states could prevent gazeHMM from overclassifying smooth pursuits in the image data and yield better validation results. Therefore, I decided to apply gazeHMM with three states to the image data and explore the comparison to other algorithms.  
Table \@ref(tab:tab-rmsd-3) shows the RMSD between gazeHMM and human coders only for the image condition. For fixations, gazeHMM shows a comparably low RMSD, but it was relatively high for saccades and PSOs. 
The sample-to-sample agreement between gazeHMM with three states and the human coders measured by Cohen's kappa is displayed in Table \@ref(tab:tab-kappa-3) (only for the image condition). For fixations, the absolute kappa indicated substantial agreement and was relatively high compared to the other algorithms. The absolute kappa for saccades corresponded to moderate agreement. Compared to the other algorithms, it was rather low. For PSOs, the absolute agreement was fair, but relatively low compared to the other algorithms.  
Figure \@ref(fig:plot-disag) illustrates the overall disagreement between gazeHMM with three events and the human coders compared to the other algorithms. In all three conditions, the disagreement was slightly lower for gazeHMM than for all the other algorithms (5-10% for images and moving dots, around 20% for videos). Compared to gazeHMM with four states, the RMSD for fixations decreased while the sample-to-sample agreement increased. For saccades and PSOs, the opposite pattern was found. The overall disagreement substantially decreased. Overall, applying gazeHMM with three states to static data seems more appropriate, since the increase in fixation classification outweighs the decrease in classification of saccades and PSOs. There was support for my second hypothesis since gazeHMM had lower overall disagreement to human coders, ranging between 5 and 20% across conditions, than all other algorithms.

(ref:tab-rmsd-3) Event Duration Descriptives and RMSD between Algorithms and Human Coders for the Image Condition
(ref:tab-rmsd-3-note) Durations are displayed in seconds. gazeHMM classified three events. RMSD = root mean square deviation. Table adapted from @Andersson2017.

```{r tab-rmsd-3}

tab.rmsd.3 <- lapply(A2017.rmsd[[2]][[2]], function(x) {
  
  df <- as.data.frame(t(as.matrix(x)))
  
  rownames(df)[3] <- "gazeHMM"
  
  out <- cbind(algorithm = rownames(df), df)
  
  return(out)
}) %>% reduce(left_join, by = "algorithm")

apa_table(tab.rmsd.3,
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Fixations" = c(2, 5), "Saccades" = c(6, 9), "PSOs" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-rmsd-3)",
          note = "(ref:tab-rmsd-3-note)")

```

(ref:tab-kappa-3) Cohen's Kappa Between Human Coders and Algorithms for Different Events in the Image Condition
(ref:tab-kappa-3-note) Negative values were set to zero. gazeHMM-3 classified three and gazeHMM-4 classified four events. Table adapted from @Andersson2017.

```{r tab-kappa-3}

tab.kappa.3 <- cbind(c("coderMN", "coderRA", "gazeHMM-4", "gazeHMM-3", "CDT", "EM", "IDT", "IKF", "IMST", "IHMM", "IVT", "NH", "BIT", "LNS"),
                     as.data.frame(rbind(ref.acc[1:2, 1:9], kappa.table[[3]][1:9], kappa.table[[2]], ref.acc[3:nrow(ref.acc), 1:9]))) %>%
  dplyr::select(c(1, seq(2, 10, 3)))

apa_table(tab.kappa.3,
          col.names = c("Algorithm", "Fixations", "Saccades", "PSOs"),
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-kappa-3)",
          note = "(ref:tab-kappa-3-note)")

```

\newpage

# Discussion
In this report, I presented gazeHMM, a novel algorithm for classifying gaze data into eye movement events. The algorithm models velocity, acceleration, and sample-to-sample angle signals with gamma distributions and a mixture of von Mises and a uniform distribution. An HMM classifies the gaze samples into fixations, saccades, and optionally PSOs, and/or smooth pursuits. A simulation study revealed that the generative model of gazeHMM recovered parameters and hidden state sequences well with a few exceptions. Adding smooth pursuits to the model and noise to the generated data were the most critical factors for decreasing recovery. In contrast, higher sample sizes yielded a better parameter recovery. The variation in starting values did not affect the recovery but the amount of missing data decreased the recovery of hidden states.  
I applied gazeHMM with different numbers of states to benchmark data and compared the model fit, showing that a five-state HMM had consistently most likely generated the data. Thus, the model comparison provided evidence against my first hypothesis that the number of events in gazeHMM matches theoretical expectations.  
When comparing gazeHMM with four events against other algorithms and human coders, it did not show the best performance for the duration and number of classified events nor the sample-to-sample agreement across all conditions. However, it had a lower overall disagreement for dynamic data but higher disagreement for static data. I argued that four states might not be appropriate for static data and additionally applied gazeHMM with three states to them. That led to substantially better classifications for fixations, but slightly worse performance for saccades and PSOs. With three states, gazeHMM had a lower overall disagreement than all other algorithms. I treat this pattern as moderate evidence for my second hypothesis that gazeHMM improves the classification of eye movements. Taken together, I only found some evidence for one part of my research question, that gazeHMM improved classifying gaze data but not describing them.  
Considering the results of the simulation study, it seems reasonable that adding the smooth pursuit state to the HMM decreased parameter and state recovery: It is the event that is overlapping most closely with another event (fixations) in terms of velocity, acceleration, and sample-to-sample angle. The overlap can cause the HMM to confuse parameters and hidden states. The decrease in parameter recovery (especially for scale parameters) due to noise shows that the overlap is enhanced by more dispersion in the data. The scale parameters might be particularly vulnerable to extreme data points. The decrease in state recovery for long and many intervals of missing data can be explained by the tendency of HMMs to stay in the same state when no information is available from the data. However, the decrease was rather small, leading to the conclusion that the HMM can deal well with missing data. Despite these drawbacks, the recovery of the generative model in gazeHMM seems very promising. The simulation study gives also an approximate reference for the maximum recovery of hidden states that can be achieved by the HMM (Cohen's kappa values of ~1 for two, ~0.95 for three, and ~0.8 for four events).  
The model comparison on the benchmark data suggested that gazeHMM is not the best generative model for eye movement data. Even using a model selection criterion that identified correct models successfully in previous studies [@Pohle2017], did not yield the expected pattern. There are several explanations for this result:  
*There were more eye movement events present in the data than I expected*. Eye movement events can be divided into subevents. For example, fixations consist of drift and tremor movements [@Duchowski2017] and PSOs encompass dynamic, static, and glissadic over- and undershoots [@Larsson2013]. A study on a recently developed HMM algorithm supports this explanation: @Houpt2018 applied the unsupervised BP-AR-HMM algorithm to the @Andersson2017 data set and classified more distinct states than the human coders. Some of the states classified by BP-AR-HMM matched the same event coded by humans. Since the subevents are usually not interesting for users of classification algorithms, the ability of HMMs to classify might limit their ability to describe eye movements here.  
*Model selection criteria are generally not appropriate for comparing HMMs with different numbers of states*. This argument has been discussed in the field of ecology [see @Li2017], where studies found that selection criteria preferred models with more states than expected [similar to the result of this study; e.g., @Langrock2015]. @Li2017 explain this bias with the simplicity of the submodels in HMMs: Initial state, transition, and response models for each state are usually relatively simple. When they do not describe the processes in the respective states accurately, the selection criteria compensate for that by preferring a model with more states. Thus, there are not more latent states present in the data, but the submodels of the HMM are misspecified or too simple. Correcting for model misspecifications, led to better model recovery in studies on animal movements [@Langrock2015; @Li2017]. However, @Pohle2017 showed in simulations that the ICL identified the correct model despite several misspecifications. It has to be noted that the study by @Pohle2017 only used data generating models with two states.  
*The submodels of gazeHMM were misspecified*. @Pohle2017 identified two scenarios in which model recovery using the ICL did not give optimal results: Outliers in the data and inadequate distributions in the response models. Both situations could apply to gazeHMM and eye movement data. Outliers occur frequently in eye-tracking data due to measurement error. Choosing adequate response distributions in HMMs is usually difficult and can depend on the individual and task the data is obtained from [@Langrock2015]. Moreover, gazeHMM only estimated intercepts for all parameters and thus, no time-varying covariates were included [cf. @Li2017]. This aspect could indeed oversimplify the complex nature of eye movement data.  
Comparing gazeHMM to other algorithms on benchmark data showed that gazeHMM moderately improved the event classification in agreement with human coders. However, the evaluation criteria (RMSD of event durations, sample-to-sample agreement, and overall disagreement) yielded different results. The fact that gazeHMM outperformed all other algorithms regarding the overall disagreement can be because it is the only algorithm classifying five events (incl. blinks). Nevertheless, Cohen's kappa values of 0.72 (saccades - video - four states) or 0.68 (fixations - image - three states) indicate substantial agreement to human coders, especially in light of the maximum reference of ~0.8 from the simulation study. At this point it is important to mention that human coding should not be considered a gold standard in event classification: @Hooge2018a observed substantial differences between coders and within coders over time. Despite these differences, they recommend comparisons to human coding to demonstrate the performance of new algorithms and to find errors in their design.

## Advantages of gazeHMM
In view of the four proposed goals that gazeHMM should fulfill, I can draw the following conclusions: Even though gazeHMM does require some parameter settings (in the pre- and postprocessing), they only have minor influence on the classification and are merely included to compensate for some drawbacks of the generative model. Their default values should be appropriate for most applications. A major advantage of gazeHMM is that it does not require human labeled data as input. Instead, it estimates all parameters and hidden states from the data. Since human coding is quite laborious, difficult to reproduce, and by times inconsistent [as noted earlier, @Hooge2018a], this property makes gazeHMM a good alternative to other recently developed algorithms that require human coded input [@Pekkanen2017; @Zemblys2018; @Bellet2019]. This could also explain why the agreement to human coding is lower for gazeHMM than for algorithms that learn from human labeled data. Another advantage of gazeHMM is its ability to classify four eye movement events, namely fixations, saccades, PSOs, and smooth pursuit. Whereas most algorithms only parse fixations and saccades [@Andersson2017], few are able to classify PSOs [e.g., @Zemblys2018], and even less categorize smooth pursuits [e.g., @Pekkanen2017]. However, including smooth pursuits in gazeHMM led to some undesirable classifications on benchmark data, resulting in rapid switching between fixation and smooth pursuit events. Therefore, I recommend using gazeHMM with four events only for exploratory purposes. Without smooth pursuits, I consider gazeHMM's classification as appropriate for application. Lastly, its implementation in R using depmixS4 [@Visser2010] should make gazeHMM a tool that is easy to use and customize for individual needs.

## Future Directions
Despite its advantages, there are several aspects in which gazeHMM can be improved: First, a multivariate distribution could be used to account for the correlation between velocity and acceleration signals [for examples, see @Balakrishnan2009]. Potential problem of this approach might be choosing the right distribution and convergence issues (due to a large number of parameters). Another option to model the correlation could be to include one of the response variables as a covariate of the other.  
Second, instead of the gamma being the generic (and potentially inappropriate) response distribution, a non-parametric approach could be used: @Langrock2015 use a linear combination of standardized B-splines to approximate response densities, which led to HMMs with less states being preferred. This approach could potentially combat the problem of unexpectedly high-state HMMs being preferred for eye movement data.  
Third, one solution to diverging results when comparing gazeHMM with different events could be model averaging: Instead of using the maximum posterior state probability of each sample from the preferred model, the probabilities could be weighted according to a model selection criterion (e.g., Schwarz weight) and averaged. Then, the maximum averaged probability could be used to classify the samples into events. This approach could lead to a more robust classification because it reduces the overconfidence of each competing model and easily adapts to new data [analogous to Bayesian model averaging; @Hinne2020]. However, the model comparison for gazeHMM often showed extreme weights for a five-state model, which would lead to a very limited influence of the other models in the averaged probabilities.  
Fourth, including covariates of the transition probabilities and response parameters could improve the fit of gazeHMM on eye movement data. As pointed out earlier, just estimating intercepts of parameters could be too simple to model the complexity of eye movements. As candidate covariates I suggest periodic functions of time [@Li2017] which could, for instance, capture the specific pattern of saccades. Whether covariates are improving the fit of submodels to eye movement data could in turn be assessed by inspecting pseudo-residuals and auto-correlation functions [see @Zucchini2016].  
Fifth, to avoid rapid switching between fixations and smooth pursuits as well as unreasonably short saccades, gazeHMM could explicitly model the duration of events. This can be achieved by setting the diagonal transition probabilities to zero and assign a distribution of state durations to each state [@Bishop2006]. Consequently, the duration distributions of fixations and smooth pursuits could differ from saccades and PSOs. This extension of the HMM is also called hidden semi-Markov model and has been successfully used by @Mihali2017 to classify microsaccades. Drawback of this extension are higher computational costs and difficulties with including covariates [@Zucchini2016].  
Lastly, allowing constrained parameters in the HMM could replace some of the postprocessing steps in gazeHMM. This could potentially be achieved by using different response distributions or parameter optimization methods. Moreover, switching from the maximum likelihood to the Markov chain Monte Carlo (Bayesian) framework could help avoiding convergence problems with constrained parameters, but would also open new research questions about suitable priors for HMM parameters in the eye movement domain.

## Conclusion
Both the simulation and validation studies showed that gazeHMM is a suitable algorithm for classifying eye movement events. For smooth pursuits, the classification is not optimal and thus not recommended, yet. On one hand, the algorithm has some advantages over concurrent event classification algorithms, not relying on human-labeled input being the most important one. On the other hand, it is not able to perform model comparisons as expected and still poses difficulties regarding good model specifications for different eye movement events. These difficulties highlight challenges for HMMs in general but also specifically for eye movements. Revisiting my research question, I conclude that gazeHMM is suitable for classifying eye movement events but still lacks the ability to accurately describe them.  

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
