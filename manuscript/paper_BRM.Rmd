---
title             : "Classifying Eye Movement Events With an Unsupervised Generative Hidden Markov Model"
shorttitle        : "Classifying Eye Movement Events"

author: 
  - name          : "Malte Lüken"
    affiliation   : "1"
    corresponding : yes
    address       : "Postbus 15906, 1001 NK Amsterdam, The Netherlands"
    email         : "malte_lueken@arcor.de"
  - name          : "Šimon Kucharský"
    affiliation   : "1"
  - name          : "Ingmar Visser"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, University of Amsterdam. Amsterdam, The Netherlands"

authornote: |
 We would like to thank Daan van Renswoude, Maartje Raijmakers, and Maximilian Maier for their helpful comments on earlier versions of this paper. Furthermore, we would like to acknowledge feedback by Karel Veldkamp and Phil Norberts in the early stage of this project. Šimon Kucharský was supported by the NWO (Nederlandse Organisatie voor Wetenschappelijk Onderzoek) grant no. 406.10.559.

abstract: |
 Eye-tracking allows researchers to infer cognitive processes from eye movements that are classified into distinct events. Parsing the events is typically done by algorithms. Previous algorithms have successfully used hidden Markov models (HMMs) for classification but can still be improved in several aspects. To address these aspects, we developed gazeHMM, an algorithm that uses an HMM as a generative model, has no critical parameters to be set by users, and does not require human coded data as input. The algorithm classifies gaze data into fixations, saccades, and optionally postsaccadic oscillations and smooth pursuits. We evaluated gazeHMM’s performance in a simulation study, showing that it successfully recovered HMM parameters and hidden states. Parameters were less well recovered when we included a smooth pursuit state and/or added even small noise to simulated data. We applied generative models with different numbers of events to benchmark data. Comparing them indicated that HMMs with more events than expected had most likely generated the data. We also applied the full algorithm to benchmark data and assessed its similarity to human coding. For static stimuli, gazeHMM showed high similarity and outperformed other algorithms in this regard. For dynamic stimuli, gazeHMM tended to rapidly switch between fixations and smooth pursuits but still displayed higher similarity than other algorithms. Concluding that gazeHMM can be used in practice, we recommend parsing smooth pursuits only for exploratory purposes. Future HMM algorithms could use covariates to better capture eye movement processes and explicitly model event durations to classify smooth pursuits more accurately.
  
keywords          : "eye-tracking; event detection; parameter recovery; dependent mixture models"
wordcount         : "10328"

bibliography      : "references.bib"

header-includes   :
  - \usepackage{placeins}
floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man, noextraspace"
output            : papaja::apa6_pdf
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, results = "asis")

library(papaja)
library(tidyverse)
library(signal)
library(CircStats)
library(depmixS4)
library(psych)
library(here)
library(grid)
library(gridExtra)
library(viridis)
source(here("algorithm/preprocessing_helper_functions.R"))
source(here("algorithm/model_helper_functions.R"))

```

```{r set graphics theme, include=FALSE}

theme_set(theme_apa())

```

```{r load data, include=FALSE}

load(here("validation/Andersson2017_raw.Rdata"))

res <- c(1024, 768)
dim <- c(380, 300)
dist <- 670
fr <- 500

```

# Introduction
Eye-tracking is typically used to study cognitive processes involving attention and information search based on recorded gaze position [@Schulte-Mecklenbeck2017]. Before these processes can be studied, the raw gaze data is classified into events that are distinct in their physiological patterns (e.g., duration), underlying neurological mechanisms, or cognitive functions [@Leigh2015]. Basic events are fixations, saccades, smooth pursuits, and post-saccadic oscillations (PSOs). Classifying raw eye-tracking data into these events reduces their complexity and is usually the first step towards cognitive interpretation [@Salvucci2000]. The classification is typically done by algorithms, which is considered faster, more objective, and reproducible compared to human coding [@Andersson2017]. @Hein2017 give a comprehensive overview of different classification algorithms [for a structured review on classifying saccades, see also @Stuart2019].  
To motivate our decision to add another algorithm to this array of classification tools, we first need to distinguish between the different classes of methods they use. On one hand, many classification algorithms use non-parametric methods to differentiate between eye movement events. A classic example is the "Velocity-threshold" algorithm [@Stampe1993], which classifies samples with a velocity above a fixed threshold as saccades [see also @Nystrom2010; @Larsson2013; @Larsson2015]. On the other hand, many parametric methods have been developed recently. Some of them require human-labeled training data as input and can therefore be termed as *supervised* [@Hastie2017]. For example, @Bellet2019 trained a convolutional neural network (CNN) on eye-tracking data from humans and macaques and achieved classification results that were similarly accurate to those of human coders [see also @Zemblys2018; @Zemblys2019; @Startsev2019]. Despite their accuracy, the requirement of labeled training data is a disadvantage of supervised methods because the labeling process can easily become costly and time-consuming [@Zemblys2019]. Supervised methods also (implicitly) treat human-labeled training data as a gold standard, an assumption that may be unwarranted [see discussion in @Hooge2018]. In contrast, *unsupervised* classification algorithms do not require labeled training input. Instead, they learn parameters from the characteristics of the data themselves [@Hastie2017]. In consequence, they are also more flexible in classifying data from different individuals, tasks, or eye-trackers [e.g., @Houpt2018; @Hessels2017].  
Besides discriminating between supervised and unsupervised methods, algorithms can vary in whether they are explicitly modeling the data generating process and are thus able to simulate new data. To our knowledge, these *generative models* have been rarely used to classify eye movement data [cf. @Mihali2017; @Wadehn2020]. Classifiers with generative assumptions have the advantage that their parameters can be easily interpreted in terms of the underlying theory. In the context of eye movements, they can also help to explain or confirm observed phenomena: For instance, their parameters can indicate that oscillations only occur after but not before saccades. When the goal is to understand eye movement events and improve their classification based on this understanding, this aspect is an advantage over non-parametric or supervised methods. Moreover, generative models can challenge common theoretical assumptions and bring up new research questions [@Epstein2008]. For example, they might suggest that oscillations also occur before saccadic eye movements [as mentioned in @Nystrom2010] or that the assumption that eye movements are discrete events (e.g., saccades and PSOs cannot overlap) does not hold [as discussed in @Andersson2017].  
One class of generative models that are used in eye movement classification are HMMs. They estimate a sequence of hidden states (i.e., a discrete variable that cannot be directly observed) that evolves parallel to the gaze signal. Each gaze sample depends on its corresponding state. Each state depends on the previous but not on earlier states of the sequence [@Zucchini2016]. Further, HMMs can be viewed as unsupervised models that can learn the hidden states and parameters of the emission process from the observed data alone, and as such do not in principle need labeled training data. They are suitable models for eye movement classification because the hidden states can be interpreted as eye movement events and gaze data are dependent time series (i.e., one gaze sample depends on the previous).  
On this basis, several classification algorithms using HMMs have been developed: One instance is described in @Salvucci2000 and combines the HMM with a fixed threshold approach (named "Identification by HMM" [I-HMM]). Samples are first labeled as fixations or saccades, depending on whether their velocity exceeds a threshold, and then reclassified by the HMM. @Pekkanen2017 developed an algorithm that filters the position of gaze samples through naive segmented linear regression (NSLR). The algorithm uses an HMM to parse the resulting segments into fixations, saccades, smooth pursuits, and PSOs based on their velocity and change in angle (named NSLR-HMM). Another version by @Mihali2017 uses a Bayesian HMM to separate microsaccades (short saccades during fixations) from motor noise based on sample velocity (named "Bayesian Microsaccade Detection" [BMD]). Moreover, @Houpt2018 developed a hierarchical approach that describes sample velocity and acceleration through an autoregression (AR) model, computes the regression weights through an HMM, and estimates the number of events with a beta-process (BP) from the data (named BP-AR-HMM).  
Several studies have tested the performance of HMM algorithms against other classification methods: I-HMM has been deemed as robust against noise, behaviorally accurate, and showing a high sample-to-sample agreement to human coders [@Andersson2017; @Komogortsev2010; @Salvucci2000]. However, the agreement was lower when compared to an algorithm using a Bayesian mixture model [@Kasneci2014; @Tafaj2012]. NSLR-HMM showed even higher agreement to human coding than I-HMM but was outperformed by the CNN algorithm in @Bellet2019 [@Pekkanen2017].  
In sum, HMMs seem to be a promising method for classifying eye movements. Nevertheless, the existing HMM algorithms each have at least one aspect in which they could be improved.  
First, I-HMM relies on setting an appropriate threshold to determine the initial classification, which can distort the results [@Blignaut2009; @Komogortsev2010; @Shic2008]. Second, the current implementation of NSLR-HMM requires human-coded data, which narrows its applicability. It also inheres fixed parameters that prevent the algorithm to adapt to individual- or task-specific signals. Third, BMD limits the classification to microsaccades which are irrelevant in many applications and sometimes even considered as noise [@Duchowski2017]. The opposite problem was observed for BP-AR-HMM: It tends to estimate an unreasonable number of events from the data of which many are considered as noise events. Therefore, the authors suggest using it as an exploratory tool followed by further event classification [@Houpt2018].  
In this article, we developed a novel algorithm for classifying gaze data, named gazeHMM, that relies on an HMM as a generative model and addresses the aforementioned aspects of previous algorithms. The development was guided by four goals: (a) the algorithm should not require parameter settings through the user (e.g., thresholds) or (b) human-labeled data as input; (c) it should cover the most relevant eye movement events, namely fixations, saccades, smooth pursuits, and PSOs; (d) it should confirm rather than explore the presence of events in the data.  
The following section describes gazeHMM and the underlying generative model in detail. Then, we present the parameter recovery of the HMM and show how the algorithm performs compared to other eye movement event classification algorithms concerning the agreement to human coding. Finally, we discuss these results and propose directions in which gazeHMM and other HMM algorithms could be improved.

# Developing gazeHMM
As illustrated in Figure \@ref(fig:plot-algorithm-workflow), most eye movement event classification algorithms consist of three steps [cf. @Hessels2017]: During *preprocessing*, the raw gaze position is transformed into a metric of position and/or time. Often, a filtering or smoothing procedure is applied to the data, before or after the transformation, to separate the gaze signal from noise and artifacts [@Spakov2012]. Then follows the *classification*, depending on the method and settings of the algorithm, each sample is labeled as a candidate for one of the predefined events. Lastly, as part of the *postprocessing*[^1], the algorithm decides which candidates to accept, relabel, or merge [@Hessels2017; @Komogortsev2010].  

[^1]: @Hessels2017 called step two the *search rule* and step three the *classification rule*. For non-parametric methods, this distinction might be accurate. However, for parametric methods, calling step two "classification" is more appropriate since the probabilistic classification is done here. Step three usually consists of some heuristic relabeling and correcting for classification errors.

(ref:plot-algorithm-workflow) Example workflow for eye movement event classification algorithms: (a) the raw gaze signal in x (dark red) and y (dark blue) coordinates; (b) the raw gaze signal is filtered and transformed into a velocity signal; (c) samples are classified as events (indicated by colors), and (d) relabeled. Sequences of samples belonging to the same event are merged (indicated by black segments). Data from @Andersson2017.

```{r plot-algorithm-workflow, fig.cap="(ref:plot-algorithm-workflow)", fig.height = 8}

toPlot <- A2017[[3]][[3]] %>%
  mutate(x.va = px2va(x, dim[1], res[1], dist),
         y.va = px2va(y, dim[2], res[2], dist),
         x.vel = sgolayfilt(x.va, m = 1),
         y.vel = sgolayfilt(y.va, m = 1),
         vel = sqrt(x.vel^2 + y.vel ^2)*fr) %>%
  dplyr::filter(t > 4.3 & t < 4.5) %>%
  mutate(error = vel > 80,
         event.pre = as.factor(ifelse(error, 2, coderMN)),
         event.post = as.factor(coderMN))

# Create plot

p1 <- ggplot(toPlot, aes(x = t, x.va)) + 
  geom_line(aes(y = x.va)) + 
  geom_line(aes(y = y.va)) + 
  geom_point(aes(y = x.va), size = 2) + 
  geom_point(aes(y = y.va), size = 2) +
  scale_y_continuous(name = "Gaze position (deg)", limits = c(-5, 10), breaks = pretty(c(-5, 10)))

p2 <- ggplot(toPlot, aes(x = t, y = vel)) + 
  geom_line() + 
  geom_point(size = 2) +
  scale_y_continuous(name = "Velocity (deg/s)", limits = c(0, 250), breaks = pretty(c(0, 250)))

p3 <- ggplot(toPlot, aes(x = t, y = vel)) + 
  geom_line() + 
  geom_point(aes(color = event.pre), size = 2) +
  scale_y_continuous(name = "Velocity (deg/s)", limits = c(0, 250), breaks = pretty(c(0, 250)))

p4 <- ggplot(toPlot, aes(x = t, y = vel)) + 
  geom_line() + 
  geom_point(aes(color = event.post), size = 2) +
  scale_y_continuous(name = "Velocity (deg/s)", limits = c(0, 250), breaks = pretty(c(0, 250)))

plotList <- list(p1, p2, p3, p4)

for(p in 1:length(plotList)) {
  
  plotList[[p]] <- plotList[[p]] + 
    xlab("") + geom_line(color = "black") + ggtitle(letters[p]) + 
    scale_color_viridis_d() +
    theme(legend.position = "none")
  
}

plotList[[4]] <- plotList[[4]] + xlab("Time (s)") +
  geom_segment(aes(x = first(t[event.post == 4]), y = 250, xend = first(t[event.post == 2])-(1/fr), yend = 250), size = 1.2) +
  geom_segment(aes(x = first(t[event.post == 2]), y = 250, xend = first(t[event.post == 3])-(1/fr), yend = 250), size = 1.2) +
  geom_segment(aes(x = first(t[event.post == 3]), y = 250, xend = last(t[event.post == 3]), yend = 250), size = 1.2) +
  geom_segment(aes(x = last(t[event.post == 3])+(1/fr), y = 250, xend = last(t[event.post == 4]), yend = 250), size = 1.2)

grid.arrange(plotList[[1]], plotList[[2]], plotList[[3]], plotList[[4]], ncol = 1)

```

## Preprocessing
Algorithms require variables that describe gaze data (hereafter called *eye movement metrics*) to classify them into events. Many eye movement metrics have been proposed and used in previous algorithms [for examples, see @Zemblys2018; @Andersson2017], but most of them rely on thresholds or window ranges that have to be set by the user [e.g., the distance between the mean position in a 100 ms window before and after each sample, see @Olsson2007]. This can be problematic because such parameters are often set without theoretical justification and differ substantially between metrics or heavily depend on the eye-tracker's characteristics [e.g., sampling frequency; @Andersson2017]. In gazeHMM, we used velocity, acceleration, and sample-to-sample angle [synonymous to relative or change in angle; @Larsson2013] because they belong to the most basic metrics which do not require additional parameter settings.  
Theoretically, these three metrics should separate eye movement events. Fixations typically inherit samples with low velocity and acceleration [@Larsson2013]. Due to tremor, we assume that the angle between samples should not follow any direction but a uniformly random walk [@Larsson2015]. In contrast, saccade samples usually have a high velocity and acceleration and roughly follow the same direction. PSO samples tend to have moderate velocity and high acceleration since they occur between saccades and low-velocity events [@Larsson2013]. They can be specifically distinguished by their change in direction clustered around 180 degrees [@Pekkanen2017]. Importantly, the oscillations depend on the resolution of the gaze recording: Eye-trackers with higher sampling frequency yield more changes in direction and more samples in between those changes. Those samples in between typically follow the same direction. Thus, with high sampling frequencies, PSO samples might also cluster around a sample-to-sample angle of zero with outliers around 180 degrees. Lastly, smooth pursuit samples have a moderate velocity but low acceleration (due to the smoothness) and like saccades, they follow a similar direction [@Larsson2013; @Leigh2015]. Other algorithms focus exclusively on classifying microsaccades [e.g., @Mihali2017], but as stated earlier, these events were not in the scope of gazeHMM.  
The velocity and acceleration signals are computed from the raw gaze position by using a Savitzky-Golay filter [@Savitzky1964; similar to @Nystrom2010]. The sample-to-sample angle is calculated as: $$\alpha(t) = \arctan\left(\frac{y_{t+1} - y_{t}}{x_{t+1} - x_{t}}\right) -\arctan\left(\frac{y_{t} - y_{t-1}}{x_{t} - x_{t-1}}\right),$$ with $\alpha(t) := \alpha(t) + 2\pi$ for $\alpha(t) < 0$, and is therefore bound between 0 and $2\pi$.  
Most of the missing data in eye movement classification are due to blinks. Therefore, the user can indicate which samples should be labeled as blinks (other missing samples are treated as noise). Often, eye-trackers record a few samples with unreasonably high velocity and acceleration before losing the pupil signal when a blink occurs. Since these samples could distort the classification of saccades in the HMM, gazeHMM removes them heuristically. Before classifying the samples, it sets all samples within 50 ms before and after blink samples as missing. The window of 50 ms was rather motivated empirically than theoretically and can bet set to any value the user considers appropriate.

## The Generative Model
We denote the three eye movement metrics by *X*, *Y*, and *Z*. Each metric was generated by a hidden state variable *S*. Given *S*, the HMM treats *X*, *Y*, and *Z* as conditionally independent. Conditional independence might not accurately resemble the relationship between velocity and acceleration (which are naturally correlated). This step was merely taken to keep the HMM simple and identifiable. In gazeHMM, *S* can take one of two, three, or four hidden states. The first state always represents fixations, the second saccades, the third PSOs, and the fourth smooth pursuits. Thus, users can choose whether they would like to classify only fixations and saccades, or additionally PSOs and/or smooth pursuits.  
HMMs can be described by three submodels: An initial state model, a transition model, and a response model. The initial state model contains probabilities for the first state of the hidden sequence $\rho_i = P(S_1=i)$, with *i* denoting the hidden state. In gazeHMM, the initial states are modeled by a multinomial distribution. The evolution of the sequence is in turn described by the transition model, which comprises the probabilities for transitioning between different states in the HMM. Typically, probabilities to transition from state *i* to *j*, $a_{ij} = P(S_{t+1}=j|S_{t}=i)$, are expressed in matrix form [@Visser2011]:
$$\mathbf{A} = \left[\begin{array}
{rrr}
a_{11} & ... & a_{1j} \\
\vdots & \ddots & \vdots \\
a_{i1} & ... & a_{ij}
\end{array}\right].
$$
Again, the transition probabilities for each state are modeled by multinomial distributions.  
The response model encompasses distributions describing the response variables for every state in the model. Previous algorithms have used Gaussian distributions to describe velocity and acceleration signals (sometimes after log-transforming them). However, several reasons speak against choosing the Gaussian: First, both signals are usually positive (depending on the computation). Second, the distributions of both signals appear to be positively skewed conditionally on the states, and third, to have variances increasing with their mean. Thus, instead of using the Gaussian, it could be more appropriate to describe velocity and acceleration with a distribution that has these three properties. In gazeHMM, we use gamma distributions with a shape and scale parametrization for this purpose: 
$$\begin{aligned}
(X \mid S = i) &\sim \text{Gamma}(\alpha_{xi}, \beta_{xi}) \\
(Y \mid S = i) &\sim \text{Gamma}(\alpha_{yi}, \beta_{yi}),
\end{aligned}$$
with *i* denoting the hidden state. When we developed gazeHMM, the gamma distribution appeared to fit eye movement data well, but we also note that it might not necessarily be the best fitting distribution for every type of eye movement data. We assume that the best fitting distribution will depend on the task, eye-tracker, and individual (see discussion). We emphasize that gazeHMM does not critically depend on the choice of distribution and other distributions than the gamma can be readily included in the model.    
To model the sample-to-sample angle, we pursued a novel approach in gazeHMM: A mixture of von Mises distributions (with a mean and concentration parameter) and a uniform distribution: 
$$\begin{aligned}
(Z \mid S = 1) &\sim U(0, 2\pi) \\
(Z \mid S = 2) &\sim \text{von Mises}(\mu_{\text{1}}, \kappa_{\text{1}}) \\
(Z \mid S = 3) &\sim \text{von Mises}(\mu_{\text{2}}, \kappa_{\text{2}}) \\
(Z \mid S = 4) &\sim \text{von Mises}(\mu_{\text{3}}, \kappa_{\text{3}}).
\end{aligned}$$
Both the distributions and the metric operate on the full unit circle (i.e., between 0 and $2\pi$), which should lead to rather symmetric distributions. Because we assume fixations to change their direction similar to a uniformly random walk [@Larsson2013; @Larsson2015], their sample-to-sample angle can be modeled by a uniform distribution. Thus, the uniform distribution should distinguish fixations from the other events.  
Taking all three submodels together, the joint likelihood of the observed data and hidden states can be expressed as:
$$L(X, Y, Z, S|\mathbf{\lambda})=\rho_{S_{1}}f_{S_{1}}(X_1)f_{S_{1}}(Y_1)f_{S_{1}}(Z_1)\prod_{t=1}^{T-1}a_{S_{t}S_{t+1}}f_{S_{t+1}}(X_{t+1})f_{S_{t+1}}(Y_{t+1})f_{S_{t+1}}(Z_{t+1}),$$
with $\mathbf{\lambda}$ denoting the vector containing the initial state and transition probabilities as well as the response parameters. By summing over all possible state sequences, the likelihood of the data given the HMM parameters becomes [@Visser2011]:
$$L(X, Y, Z|\mathbf{\lambda})=\sum_{\text{all S}}\rho_{S_{1}}f_{S_{1}}(X_1)f_{S_{1}}(Y_1)f_{S_{1}}(Z_1)\prod_{t=1}^{T-1}a_{S_{t}S_{t+1}}f_{S_{t+1}}(X_{t+1})f_{S_{t+1}}(Y_{t+1})f_{S_{t+1}}(Z_{t+1}).$$
The parameters of the HMM are estimated through maximum likelihood using an expectation-maximization (EM) algorithm [@Dempster1977; @McLachlan1997]. The EM algorithm is generally suitable to estimate likelihoods with missing variables. For HMMs, it imputes missing with expected values and iteratively maximizes the joint likelihood of parameters conditional on the observed data and the expected hidden states [i.e., eye movement events; @Visser2011]. When evaluating the likelihood of missing data, gazeHMM integrates over all possible values, which results in a probability density of one. The sequence of hidden states is estimated through the Viterbi algorithm [@Viterbi1967; @ForneyJr1973] by maximizing the posterior state probability. Parameters of the response distributions (except for the uniform distribution) are optimized on the log-scale (except for the mean parameter of the von Mises distribution) using a spectral projected gradient method [@Birgin2000] and Barzilai-Borwein step lengths [@Barzilai1988]. The implementation in depmixS4 allows to include time-varying covariates for each parameter in the HMM. In gazeHMM, no such covariates were included and thus, only intercepts were estimated for each parameter.

## Postprocessing
After classifying gaze samples into states, gazeHMM applies a postprocessing routine to the estimated state sequence. We implemented this routine because constraining the probabilities for nonsaccade events to turn into PSOs to zero often caused PSOs not to appear in the state sequence at all. Moreover, gazeHMM does not explicitly control the duration of events in the HMM which occasionally led to unreasonably short events. Thus, the postprocessing routine heuristically compensates for such violations. This routine relabels one-sample fixations and smooth pursuits, saccades with a duration below a minimum threshold (here: 10 ms), and PSOs that follow nonsaccade events. Samples are relabeled as the state of the previous event. Finally, samples initially indicated as missing are labeled as noise (including blinks) and event descriptives are computed (e.g., fixation duration).  
The algorithm is implemented in R [version: 3.6.3; @RCoreTeam2020] and uses the packages \texttt{signal} [@Ligges2015] to compute velocity and acceleration signals, \texttt{depmixS4} [@Visser2010] for the HMM, \texttt{CircStats} [@Lund2018] for the von Mises distribution, and \texttt{BB} [@Varadhan2009] for Barzilai-Borwein spectral projected gradient optimization. The algorithm is available on GitHub (<https://github.com/maltelueken/gazeHMM>).

# Simulation Study
To assess how well the HMM recovers parameters and state sequences, we conducted a simulation study. The design and analysis of the study were preregistered on the Open Science Framework (<https://doi.org/10.17605/OSF.IO/VDJGP>). This section is majorly copied from the preregistration (with adapted tenses). The study was divided in four parts. Here, we only report the first two parts, which investigate the influence of parameter variation and adding noise to generated data on recovery. The other two parts, which address starting values and missing data, can be found in the supplementary material (<https://github.com/maltelueken/gazeHMM_validation>).  
The HMM repeatedly generated data with a set of parameters (true parameter values). The same model was applied to estimate the parameters from the generated data (estimated parameter values). We compared the true with the estimated parameter values to assess whether a parameter was recovered by the model. Additionally, we contrasted the true states of the HMM with the estimated states to judge how accurately the model recovered the states that generated the data.

## Starting Values
The HMM always started with a uniform distribution for the initial state and state transition probabilities. Random starting values for the estimation of shape, scale, and concentration parameters were generated by gamma distributions with a shape parameter of $\alpha_{start}=3$ and $\beta_{start;i}=\theta_i/2$, with $\theta_i$ being the true value of the parameter to be estimated in simulation $i\in(1,\dots,I)$. This setup ensured that the starting values were positive, their distributions were moderately skewed, and the modes of their distributions equaled the true parameter values. The mean parameters of the von Mises distribution always started at their true values.

## Design
In the first part, we varied the parameters of the HMM. For models with $k \in \{2, 3, 4\}$ states, $q \in \{10, 15, 20\}$ parameters were manipulated, respectively. For each parameter, the HMM generated 100 data sets with $N = 2500$ samples, and the parameter varied in a specified interval in equidistant steps. This resulted in $100 \times (10+15+20) = 4500$ recoveries. Only one parameter alternated at once, the other parameters were set to their default values. All parameters of the HMM were estimated freely (i.e., there were no fixed parameters in the model). We did not manipulate the initial state probabilities because these are usually irrelevant in the context of eye movement classification. For the transition probabilities, we only simultaneously changed the probabilities for staying in the same state (diagonals of the transition matrix) to reduce the complexity of the simulation. The leftover probability mass was split evenly between the probabilities for switching to a different state (per row of the transition matrix). Moreover, we did not modify the mean parameters of the von Mises distributions: As location parameters, they do not alter the shape of the distribution and they are necessary features for the HMM to distinguish between different states.  
We defined approximate ranges for each response variable (see supplementary material) and chose true parameter intervals and default values so that they produced samples that roughly corresponded to these ranges. Tables \@ref(tab:sim-parameters-trans) and \@ref(tab:sim-parameters-resp) show the intervals and default values for each parameter in the simulation. Parameters were scaled down by factor 10 (compared to the reported ranges) to improve fitting of the gamma distributions. We set the intervals for shape parameters of the gamma distributions for all events to [1,5] to examine how skewness influenced the recovery (shape values above five approach a symmetric distribution). The scale parameters were set so that the respective distribution approximately matched the assumed ranges. Since the concentration parameters of the von Mises distribution are the inverse of standard deviations, they were varied on the inverse scale.  
In the second part, we manipulated the sample size of the generated data and the amount of noise added to it. The model parameters were set to their default values. For models with $k \in \{2, 3, 4\}$ states and sample sizes of $N \in \{500, 2500, 10000\}$, we generated 100 data sets ($100 \times 3 \times 3 = 900$ recoveries). These sample sizes roughly match small, medium, and large eye-tracking data sets for a single participant and trial (e.g., with a frequency of 500 Hz, the sample sizes would correspond to recorded data with lengths of 1 s, 5 s, and 20 s, respectively). To simulate noise, we replaced velocity and acceleration values $y$ with draws from a gamma distribution with $\alpha_{noise} = 3$ and $\beta_{noise}=(y/2)\tau_{noise}$ with $\tau_{noise} \in [1,5]$ varying between data sets. This procedure ensured that velocity and acceleration values remained positive and were taken from moderately skewed distributions with modes equal to the original values. To angle, we added white noise from a von Mises distribution with $\mu_{noise} = 0$ and $\kappa_{noise} \in 1/[0.1,10]$ varying between data sets. $\tau_{noise}$ and $\kappa_{noise}$ were increased simultaneously in equidistant steps in their intervals.

```{r sim-parameters-trans}

sim.pars.trans <- as.data.frame(matrix(c("Interval", "-", "[.01,.99]", "1-a(i=j)/(k-1)",
                                           "Default", "1/k", "0.9", "0.1/(k-1)"), nrow = 2, byrow = T))

apa_table(sim.pars.trans,
          col.names = c("", "Initial\nstate\nprob.", "Trans. prob.\nsame state", "Trans. prob.\nother state"),
          caption = "(ref:sim-parameters-trans)",
          note = "(ref:sim-parameters-trans-note)")

```

(ref:sim-parameters-resp) Intervals and Default Parameter Values for the Response Model in the Simulation
(ref:sim-parameters-resp-note) Shape parameters are denoted by $\alpha$, scale parameters by $\beta$, mean parameters by $\mu$, and concentration parameters by $\kappa$. The default values for the uniform distribution in state one were min = 0 and max = $2\pi$.

```{r sim-parameters-resp}

sim.pars.resp <- as.data.frame(t(matrix(c("[1,5]", "[0.1,0.6]", "[1,5]", "[0.05,0.25]", "-", "-", 
                                  "3", "0.35", "3", "0.25", "-", "-",
                                  "[1,5]", "[5,15]", "[1,5]", "[1,5]", "-", "1/[0.1,10]",
                                  "3", "10", "3", "3", "0", "1",
                                  "[1,5]", "[0.5,1.5]", "[1,5]", "[1,5]", "-", "1/[0.1,10]",
                                  "3", "1", "3", "3", paste(expression(pi), sep = ""), "1",
                                  "[1,5]", "[0.5,1.5]", "[1,5]", "[0.05,0.25]", "-", "1/[0.1,10]",
                                  "3", "1", "3", "0.15", "0", "1"), nrow = 6, byrow = F)))

apa_table(cbind(rep(c("Interval", "Default"), 4), sim.pars.resp),
          col.names = c("", "Shape", "Scale", "Shape", "Scale", "Mean", "Concentration"),
          col_spanners = list(Velocity = c(2, 3), Acceleration = c(4, 5), "Rel. angle" = c(6, 7)),
          stub_indents = list("State 1" = c(1, 2), "State 2" = c(3, 4), "State 3" = c(5, 6), "State 4" = c(7, 8)),
          caption = "(ref:sim-parameters-resp)",
          note = "(ref:sim-parameters-resp-note)")

```

\FloatBarrier

## Data Analysis
For each parameter, we calculated the root median square proportion deviation [RMdSPD; analogous to root median square percentage errors, see @Hyndman2006] between the true and estimated parameter values:
$$
\begin{aligned}
\text{RMdSPD} & = \sqrt{\text{Median}(\epsilon_1^2,\dots,\epsilon_I^2)} \\ 
\epsilon_i^2 & = \left(\frac{\hat{\theta}_i - \theta_i}{\theta_i}\right)^2, 
\end{aligned}
$$
where $\theta_i$ is the true parameter value and $\hat{\theta}_i$ is the estimated parameter value for simulation $i\in(1,\dots,I)$, respectively. Even though it was not explicitly mentioned in the preregistration, this measure is only appropriate when $\theta_i \ne 0$. This was not the case for some mean parameters of the von Mises distributions. In those cases, we used $\theta_i = 2\pi$ instead. We treated $\text{RMdSPD} < 0.1$ as good, $0.1 \le \text{RMdSPD} < 0.5$ as moderate, and $\text{RMdSPD} \ge 0.5$ as bad recovery of a parameter. By taking the median, we reduced the influence of potential outliers in the estimation and using proportions enabled us to compare RMdSPD values across parameters and data sets.  
Additionally, we applied a bivariate linear regression with the estimated parameter values as the dependent and the true parameter values as the independent variable to each parameter that has been varied on an interval in part one. Regression slopes closer to one indicated that the model better captured parameter change. Regression intercepts different from zero reflected a bias in parameter estimation.  
To assess state recovery, we computed Cohen's kappa (for all events taken together, not for each event separately) as a measure of agreement between true and estimated states for each generated data set. Cohen's kappa estimates the agreement between two classifiers accounting for the agreement due to chance. Higher kappa values were interpreted as better model accuracy. We adopted the ranges proposed by @Landis1977 to interpret kappa values.
Models that could not be fitted were excluded from the recovery.

(ref:sim-parameters-trans) Intervals and Default Parameter Values for the Transition Model in the Simulation
(ref:sim-parameters-trans-note) The transition probability for staying in the same state is denoted by $a_{i=j}$ and the probability for switching to a different state by $a_{i\neq j}$. The number of states in the model is denoted by *k*.

```{r load sim results and helper functions, include=FALSE}

# Load simulation results

for (part in 1:4) {
  
  load(file = paste(here("simulation/part"), part, ".Rdata", sep = ""))
  
}


# Create functions to apply and invert mlogit link function (from depmixS4, Visser & Speekenbrink, 2019)

linkfun <- function(p, base) {
  lfun <- function(p, base) {
    p <- p/sum(p)
    beta <- numeric(length(p))
    if (any(p == 1)) 
      beta[which(p == 1)] = Inf
    else beta[-base] <- log(p[-base]/p[base])
    return(beta)
  }
  if (is.matrix(p)) {
    beta <- t(apply(p, 1, lfun, base = base))
  }
  else {
    beta <- lfun(p, base)
  }
  return(beta)
}

linkinv <- function(eta,base) {
  linv <- function(eta,base) {
    pp <- numeric(length(eta))
    if(any(is.infinite(eta)) || any(eta > log(.Machine$double.xmax)) || any(eta < log(.Machine$double.xmin))) {
      pp[which(is.infinite(eta))] <- 1
      pp[which(eta > log(.Machine$double.xmax))] <- 1 # change this to something better!
    } else {
      expb <- exp(eta)
      sumb <- sum(expb)
      pp[base] <- 1/sumb
      pp[-base] <- expb[-base]/sumb
    }
    return(pp)
  }
  if(is.matrix(eta)) {
    if(ncol(eta)==1) {
      pp <- as.matrix(apply(eta,1,linv,base=base)) # fixes problem with column matrix eta
    } else pp <- t(apply(eta,1,linv,base=base)) 	
  } else {
    pp <- linv(eta,base)
  }
  return(pp)
}


# Create function to transform parameters to normal scale

backtrans <- function(x) {
  
  out <- x
  
  nms <- names(x)
  
  out[str_detect(nms, "(Intercept)")] <- as.vector(apply(matrix(x[str_detect(nms, "(Intercept)")],
                                                        ncol = sqrt(length(x[str_detect(nms, "(Intercept)")])),
                                                        byrow = T), 1, linkinv, base = 1))
  
  out[nms %in% c("shape", "scale", "kappa")] <- exp(x[nms %in% c("shape", "scale", "kappa")])
  
  return(out)
}

```

```{r calculate RMdSPD, include=FALSE}

rmsd <- list()

for (part in 1:4) {
  
  rmsd[[part]] <- lapply(get(paste("estimates.", part, sep = "")), function(x) {
    lapply(x, function(y) {
      sqerr <- lapply(y, function(z) {
        
        err <- try(((backtrans(z$pars.est) - backtrans(z$pars.true))/
                      ifelse(backtrans(z$pars.true) == 0, 2*pi, backtrans(z$pars.true)))^2)
        
        if(is.numeric(err)) {
          
          out <- err
          
        } else {
          
          out <- rep(NA, length(z$pars.true))
          
        }
        
        names(out) <- names(z$pars.true)
        
        return(out)
      })
      
      rows <- length(sqerr)

      nms <- names(sqerr[[1]])

      pars <- matrix(unlist(sqerr), nrow = rows, byrow = T)
      
      msqerr <- apply(pars, 2, median, na.rm = T)
      
      names(msqerr) <- nms
      
      rmsqerr <- sqrt(msqerr)
      
      return(rmsqerr)
    })
  })
}


# Display RMdSPD in data frame

rmsd.data <- lapply(rmsd, function(x) lapply(x, as.data.frame))
rmsd.data <- lapply(rmsd.data, function(x) lapply(x, function(y) {as.data.frame(t(as.matrix(y)))}))
rmsd.data <- lapply(rmsd.data, function(x) lapply(1:length(x), function(y, data) {
  names(data[[y]]) <- names(rmsd[[1]][[y]][[1]])
  return(data[[y]])}, data = x))


# Create plots for part 1

plots.rmsd.1 <- lapply(rmsd.data[[1]], function(x) {
  
  names.pars.varied <- list(bquote(a["i=j"]), bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                            bquote(beta["acc"]), bquote(kappa))
  
  if(ncol(x) == 18) {
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]))
    
  } else if(ncol(x) == 30) {
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]))
    
  } else {
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]), bquote(a["i4"]))
    
  }
  
  names.pars.est <- append(trnames, list(bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                         bquote(beta["acc"]), bquote(mu), bquote(kappa)))
  
  x <- as_tibble(x, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(par.varied = c(0, 1, 2, 3, 4, rep(c(1, 2, 3, 4, 5), (nrow(x) %/% 5)-1)),
           state.varied = c(1, rep(1, 4), rep(2:((nrow(x) %/% 5)), each = 5))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "RMdSPD", names_repair = "unique") %>%
    mutate(state.est = rep(c(rep(1:max(state.varied), max(state.varied)+1),
                         rep(1:max(state.varied), each = 6)), nrow(x)),
           par.est = rep(c(rep(1, max(state.varied)), 
                           rep(2:(max(state.varied)+1), each = max(state.varied)), 
                           rep((max(state.varied)+2):(max(state.varied)+7), max(state.varied))), nrow(x))) %>%
    mutate_at(vars(par.varied, state.varied, par.est, state.est), as.factor) %>%
    mutate(state.varied = fct_recode(state.varied, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"),
           state.est = fct_recode(state.est, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"))

  p <- ggplot(data = data.long, aes(x = par.varied, y = par.est, fill = RMdSPD)) +
    geom_tile(color = "black") + facet_grid(cols = vars(state.varied), rows = vars(state.est)) +
    scale_x_discrete(name = "Varied parameter", labels = names.pars.varied) +
    scale_y_discrete(name = "Estimated parameter", labels = names.pars.est) +
    scale_fill_viridis_c(breaks = c(0, 0.1, 0.5, 1))
  
  return(p)
})


# Create plots for parts 2,3, and 4

plots.rmsd.234 <- lapply(2:4, function(y, data) lapply(data[[y]], function(x) {
  
  if(ncol(x) == 18) {
    
    k <- 2
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]))
    
  } else if(ncol(x) == 30) {
    
    k <- 3
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]))
    
  } else {
    
    k <-4
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]), bquote(a["i4"]))
    
  }
  
  names.pars.est <- append(trnames, list(bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                                         bquote(beta["acc"]), bquote(mu), bquote(kappa)))
  
  x <- as_tibble(x, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(cond = 1:3) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "RMdSPD", names_repair = "unique") %>%
    mutate(state.est = rep(c(rep(1:k, k+1),
                             rep(1:k, each = 6)), nrow(x)),
           par.est = rep(c(rep(1, k), 
                           rep(2:(k+1), each = k), 
                           rep((k+2):(k+7), k)), nrow(x))) %>%
    mutate_at(vars(cond, par.est, state.est), as.factor) %>%
    mutate(state.est = fct_recode(state.est, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"))
  
  p <- ggplot(data = data.long, aes(x = par.est, y = RMdSPD, color = cond)) +
    geom_point(position = position_dodge(0.25)) + facet_grid(rows = vars(state.est)) +
    scale_x_discrete(name = "Estimated parameter", labels = names.pars.est) +
    scale_y_continuous(breaks = c(0.1, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5)) +
    scale_color_viridis_d() +
    geom_hline(yintercept = 0.1, linetype = "dashed") +
    geom_hline(yintercept = 0.5, linetype = "dashed")
  
  if(y == 2) {
    names.cond <- c("500", "2500", "10000")
    label.cond <- "N"
  } else if (y == 3) {
    names.cond <- c("1", "2", "3")
    label.cond <- bquote(tau["start"])
  } else {
    names.cond <- c("1", "3", "5")
    label.cond <- "Number of\nintervals"
  }
  
  p <- p + scale_color_viridis_d(name = label.cond, labels = names.cond)
  
  return(p)
}), data = rmsd.data)

```

```{r calculate linear regressions, include=FALSE}

# Calculate regression weights for transition probabilities

regw.tr <- list()

regw.tr <- lapply(get("estimates.1"), function(x) {
  
  varpar <- lapply(x[1], function(y) {
    
    lapply(y, function(z) {
      
      nms <- names(z$pars.true)
      
      pars.tr <- logical(length(z$pars.true))
      
      pars.tr[str_detect(nms, "(Intercept)")] <- T
      
      intpar <- try(cbind(backtrans(z$pars.true[pars.tr]), backtrans(z$pars.est[pars.tr])))
      
      if(is.numeric(intpar)) {
        out <- intpar
      } else {
        out <- matrix(NA, nrow = length(z$pars.true[pars.tr]), ncol = 2)
      }
      
      out <- apply(out, 1, list)
      
      return(out)
    })
  })
  
  df <- list()
  
  for (i in 1:length(varpar[[1]][[1]])) {
    
    df[[i]] <- lapply(varpar, function(y, index) {
      
      lapply(y, function(z) {z[[index]]})
      
    }, index = i)
  }
  
  df <- lapply(df, as.data.frame)

  df <- lapply(df, function(z) {
    
    out <- as.data.frame(t(as.matrix(z)))
    
    names(out) <- c("true", "est")
    
    return(out)
  })
})


# Calculate linear regression weights for response parameters

regw.resp <- list()
  
regw.resp <- lapply(get("estimates.1"), function(x) {
  
  index <- 1:length(x[-1])
  
  varpar <- lapply(index, function(i, y) {
    
    lapply(y[[i]], function(z) {
      
      nms <- names(z$pars.true)
      
      #pars.tr <- logical(length(z$pars.true))
      pars.resp <- logical(length(z$pars.true))
      
      #pars.tr[str_detect(nms, "(Intercept)")] <- T
      pars.resp[nms %in% c("shape", "scale", "kappa")] <- T
      
      intpar <- try(c(backtrans(z$pars.true[pars.resp][i]), backtrans(z$pars.est[pars.resp][i])))
      
      if(is.numeric(intpar)) {
        return(intpar)
      } else {
        return(rep(NA, 2))
      }
    })
  }, y = x[-1])
  
  df <- lapply(varpar, as.data.frame)
  df <- lapply(df, function(z) {

    out <- as.data.frame(t(as.matrix(z)))

    names(out) <- c("true", "est")

    return(out)
  })
})


# Plot linear regressions for transition probabilities

D <- 100

regw.tr.data <- lapply(regw.tr, function(x) lapply(x, function(y) rbind(y)))
regw.tr.data <- lapply(regw.tr.data, function(x) reduce(x, rbind))

plots.lm.tr <- lapply(1:length(regw.tr.data), function(x, y) {
  
  data <- y[[x]] %>% 
    mutate(par = rep(1:(x+1)^2, each = D),
           From = rep(rep(1:(x+1), each = D), x+1),
           To = rep(1:(x+1), each = D*(x+1)))
  
  p <- ggplot(data, aes(x = true, y = est)) + 
    facet_grid(rows = vars(From), cols = vars(To), labeller = label_both) +
    geom_point() + geom_smooth(method = "lm", color = "#FDE725FF") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    scale_x_continuous(name = "True transition probability") +
    scale_y_continuous(name = "Estimated transition probability")
  
  return(p)
}, y = regw.tr.data)


# Plot linear regressions for response parameters

regw.resp.data <- lapply(regw.resp, function(x) lapply(x, function(y) rbind(y)))
regw.resp.data <- lapply(regw.resp.data, function(x) reduce(x, rbind))

parnames <- list(bquote(alpha["vel;1"]), bquote(beta["vel;1"]), bquote(alpha["acc;1"]), bquote(beta["acc;1"]),
                 bquote(alpha["vel;2"]), bquote(beta["vel;2"]), bquote(alpha["acc;2"]), bquote(beta["acc;2"]), bquote(kappa["2"]),
                 bquote(alpha["vel;3"]), bquote(beta["vel;3"]), bquote(alpha["acc;3"]), bquote(beta["acc;3"]), bquote(kappa["3"]),
                 bquote(alpha["vel;4"]), bquote(beta["vel;4"]), bquote(alpha["acc;4"]), bquote(beta["acc;4"]), bquote(kappa["4"]))

plots.lm.resp <- lapply(regw.resp.data, function(x) {
  
  npar <- nrow(x) %/% D
  
  data <- x %>% 
    mutate(par = rep(1:npar, each = D),
           type = rep(1, 2, 1, 2))
  
  p <- ggplot(data, aes(x = true, y = est)) + 
    facet_wrap(vars(par), scales = "free", labeller = label_bquote(.(parnames[[par]]))) +
    geom_point() + geom_smooth(method = "lm", color = "#FDE725FF") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    scale_x_continuous(name = "True response parameter") +
    scale_y_continuous(name = "Estimated response parameter")
  
  return(p)
})

```

```{r calculate accuracy, include=FALSE}

# Summarise accuracy

acc.data <- list()

for (part in 1:4) {
  
  acc.data[[part]] <- lapply(get(paste("estimates.", part, sep = "")), function(x) {
    out <- lapply(x, function(y) { 
      out <- lapply(y, function(z) {
        
        if(is.numeric(z$accuracy)) {
          acc <- z$accuracy
        } else {
          acc <- NA
        }
        
        return(acc)
      })
      
      return(as.vector(reduce(out, cbind)))
    })
    
    return(as.data.frame(t(reduce(out, cbind))))
  })
}

# Plot accuracy part 1

plots.acc.1 <- lapply(acc.data[[1]], function(x) {
  
  names.pars.varied <- list(bquote(a["i=j"]), bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                            bquote(beta["acc"]), bquote(kappa))
  
  x <- as_tibble(x, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(par.varied = c(0, 1, 2, 3, 4, rep(c(1, 2, 3, 4, 5), (nrow(x) %/% 5)-1)),
           state.varied = c(1, rep(1, 4), rep(2:((nrow(x) %/% 5)), each = 5))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy") %>%
    mutate_at(vars(par.varied, state.varied), as.factor) %>%
    mutate(state.varied = fct_recode(state.varied, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"))
  
  p <- ggplot(data = data.long, aes(x = par.varied, y = accuracy)) +
    geom_boxplot(outlier.shape = 4) + facet_grid(cols = vars(state.varied)) +
    scale_x_discrete(name = "Varied parameter", labels = names.pars.varied) + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  return(p)
})


# Plot accuracy parts 2, 3, and 4

acc.data.234 <- lapply(acc.data[2:4], function(x) reduce(x, rbind))

plots.acc.234 <- lapply(1:3, function(y, data) {
  
  x <- as_tibble(data[[y]], .name_repair = "unique")
  
  data.long <- x %>%
    mutate(cond = as.factor(rep(1:3, 3)),
           k = as.factor(rep(2:4, each = 3))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy")
  
  p <- ggplot(data = data.long, aes(x = k, y = accuracy, color = cond)) +
    geom_boxplot(outlier.shape = 4) + 
    scale_x_discrete(name = "k (number of states)")  + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1)) +
    scale_color_viridis_d()
  
  if(y == 1) {
    names.cond <- c("500", "2500", "10000")
    label.cond <- "N"
  } else if (y == 2) {
    names.cond <- c("1", "2", "3")
    label.cond <- bquote(tau["start"])
  } else {
    names.cond <- c("1", "3", "5")
    label.cond <- "Number of\nintervals"
  }
  
  p <- p + scale_color_viridis_d(name = label.cond, labels = names.cond)
  
  return(p)
}, data = acc.data.234)


# Plot accuracy over interval parts 2 and 4

plots.acc.int.24 <- lapply(c(1, 3), function(y, data) {
  
  x <- as_tibble(data[[y]], .name_repair = "unique")
  
  if(y == 1) {
  
    int <- rep(seq(1, 5, length.out = D), 9) 
  
  } else {
    
    int <- rep(floor(seq(1, 200, length.out = D)), 9)
    
  }
  
  noise.scale <- paste(1:5, round(1/seq(0.1, 10, length.out = 5), 2), sep = "\n")
  
  data.long <- x %>%
    mutate(cond = as.factor(rep(1:3, 3)),
           k = as.factor(rep(2:4, each = 3))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy") %>%
    mutate(int = int)
  
  p <- ggplot(data = data.long, aes(x = int, y = accuracy, color = cond)) +
    facet_grid(cols = vars(k), labeller = partial(label_both, sep = " = ")) + 
    geom_point()
  
  if(y == 1) {
    names.cond <- c("500", "2500", "10000")
    label.cond <- "N"
    x.name <- "Noise"
  } else {
    names.cond <- c("1", "3", "5")
    label.cond <- "Number of\nintervals"
    x.name <- "Interval length (in samples)"
  }
  
  p <- p + scale_color_viridis_d(name = label.cond, labels = names.cond) +
    scale_x_continuous(name = x.name, labels = noise.scale) + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  return(p)
}, data = acc.data.234)


# Exploratory analysis for label switching

load(here("simulation/part3_expl.Rdata"))

labsw <- lapply(1:length(estimates.3), function(x) {
  out <- lapply(estimates.3[[x]], function(y) {
    out <- lapply(y, function(z) {
      
      kappa <- try(cohen.kappa(z$states)$kappa)
      
      states <- z$states
      
      if(x == 2 & is.numeric(kappa) & kappa < 0.95) {
          
        states$y <- ifelse(z$states$y == 3, 2, ifelse(z$states$y == 2, 3, z$states$y))
          
        kappa <- cohen.kappa(states)$kappa
        
        if(is.numeric(kappa) & kappa < 0.95) {
          
          states$y <- ifelse(z$states$y == 3, 1, ifelse(z$states$y == 1, 3, z$states$y))
          
          kappa <- cohen.kappa(states)$kappa
        }
        
        if(is.numeric(kappa) & kappa < 0.95) {
          
          states$y <- ifelse(z$states$y == 3, 2, ifelse(z$states$y == 1, 3, 1))
          
          kappa <- cohen.kappa(states)$kappa
        }
      }
      
      if(x == 3 & is.numeric(kappa) & kappa < 0.5) {
        
        states$y <- ifelse(z$states$y == 3, 2, ifelse(z$states$y == 2, 3, z$states$y))
        
        kappa <- cohen.kappa(states)$kappa
        
        if(is.numeric(kappa) & kappa < 0.5) {

          states$y <- ifelse(z$states$y == 3, 4, ifelse(z$states$y == 4, 3, z$states$y))

          kappa <- cohen.kappa(states)$kappa
        }

        if(is.numeric(kappa) & kappa < 0.5) {

          states$y <- ifelse(z$states$y == 3, 1, ifelse(z$states$y == 1, 3, z$states$y))

          kappa <- cohen.kappa(states)$kappa
        }
        
        if(is.numeric(kappa) & kappa < 0.5) {
          
          kappa <- cohen.kappa(z$states)$kappa
        }
      }
      
      if(is.numeric(kappa)) {return(kappa)
        } else {return(NA)}
    })
    
    return(as.vector(reduce(out, cbind)))
  })
  
  return(t(reduce(out, cbind)))
})

labsw.df <- as.data.frame(reduce(labsw, rbind))


# Plot exploratory analysis results

plots.acc.expl <- lapply(1, function(y, data) {
  
  x <- as_tibble(data, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(cond = as.factor(rep(1:3, 3)),
           k = as.factor(rep(2:4, each = 3))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy")
  
  p <- ggplot(data = data.long, aes(x = k, y = accuracy, color = cond)) +
    geom_boxplot(outlier.shape = 4) + 
    scale_x_discrete(name = "k (number of states)")  + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  names.cond <- c("1", "2", "3")
  label.cond <- bquote(tau["start"])
  
  p <- p + scale_color_viridis_d(name = label.cond, labels = names.cond)
  
  return(p)
}, data = labsw.df)


```

## Results
### Parameter Variation
In the first part of the simulation, we examined how varying the parameters[^2] in the HMM affected the deviation of estimated parameters and the accuracy of estimated state sequences. For the two-state HMM, the recovery of parameters and states was nearly perfect (all RMdSPDs < 0.1, intercepts and slopes of regression lines almost zero and one, respectively, and Cohen's kappa close to 1). Therefore, we chose to include the respective figures in the supplementary material.  
For the HMM with three states, the RMdSPD is shown in Figure \@ref(fig:plot-rmdspd-1-3). When response parameters (other than $a_{i=j}$) were manipulated, the RMdSPDs for $a_{12}$ and $a_{31}$ were consistently between 0.1 and 0.5. Varying $\kappa$ in states two and three led to RMdSPDs between 0.1 and 0.5 in the respective states, which we interpreted as moderate recovery. Otherwise, RMdSPDs were consistently lower than 0.1, indicating good recovery. Inspecting the regression lines between true and estimated parameters (see Figures \@ref(fig:plot-regtr-1-3) and \@ref(fig:plot-regresp-1-3)) revealed strong and unbiased linear relationships (intercepts close to zero and slopes close to one). In contrast to the two-state HMM, larger deviations and more outliers were observed. Cohen's kappa values are presented in Figure \@ref(fig:plot-acc-1-3). For most estimated models, the kappa values between true and estimated state sequences were above 0.95, meaning almost perfect agreement. However, for some models, we observed kappas clustered around zero or -0.33, which is far from the majority of model accuracies. An exploratory examination of these clusters suggests that state labels were switched (see supplementary material).  
The RMdSPDs for the four-state HMM is shown in Figure \@ref(fig:plot-rmdspd-1-4). For estimated transition probabilities and $\alpha_{vel}$ and $\beta_{vel}$ parameters in states one and four, RMdSPDs were between 0.1 and 0.5, suggesting moderate recovery. Also, estimated kappa parameters in state four were often moderately recovered when parameters in states two, three, and four were varied. Otherwise, RMdSPDs were below 0.1, indicating good recovery. Looking at Figures \@ref(fig:plot-regtr-1-4) and \@ref(fig:plot-regresp-1-4), the regression lines between true and estimated parameters exhibit strong and unbiased relationships. However, there were larger deviations and more outliers than in the previous models, especially for states one and four. Cohen's kappa ranged mostly between 0.6 and 0.9, meaning moderate to almost perfect agreement between true and estimated state sequences (see Figure \@ref(fig:plot-acc-1-4)). Here, some outlying kappa values clustered around 0.25 and zero.

[^2]: Note that the initial state probability $\rho_i$ has $\textrm{RMdSPD} = 1$. Since the HMM only simulated one state sequence, this parameter is always either zero or one (leading to $\textrm{RMdSPD} = 1$). Therefore, we decided to exclude it from the analysis.

(ref:plot-rmdspd-1-3) RMdSPD between true and estimated parameters of the three-state HMM in part one of the simulation. Labels on the x-axis indicate which true parameters have been manipulated and labels on the y-axis show for which estimated parameter the RMdSPD is displayed. Top facet labels specify in which state the parameters have been varied and right facet labels denote to which state estimated parameters belong. $\rho_i$ is the initial probability for state *i* (indicated by the right facet label), $a_{ij}$ is the probability to transition from state *i* to state *j*, $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution.

```{r plot-rmdspd-1-3, fig.cap="(ref:plot-rmdspd-1-3)"}

print(plots.rmsd.1[[2]]+ theme(axis.text = element_text(size = 8)))

```

(ref:plot-regtr-1-3) Regression lines between true and estimated transition probabilities for the three-state HMM in part one. Right facet labels show *from* and top facet labels show *to* which state the HMM is moving. Dashed lines refer to perfect recovery.

```{r plot-regtr-1-3, fig.cap="(ref:plot-regtr-1-3)"}

print(plots.lm.tr[[2]])

```

(ref:plot-regresp-1-3) Regression lines between true and estimated response parameters of the three-state HMM in part one. Top facet labels indicate response parameters. Dashed lines refer to perfect recovery. $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution. Parameter subscripts indicate states and eye movement metrics.

```{r plot-regresp-1-3, fig.cap="(ref:plot-regresp-1-3)", fig.height=7}

print(plots.lm.resp[[2]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-acc-1-3) Boxplots displaying Cohen's kappa depending on which parameter of the three-state HMM has been manipulated in part one. Top facet labels indicate for which state parameters have been manipulated. Black solid lines symbolize medians and hinges the first and third quartile. Whiskers range from hinges to lowest/highest value within 1.5 times the IQR. Crosses represent outliers. $a_{i=j}$ is the probability to stay in the same state, $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution.

```{r plot-acc-1-3, fig.cap="(ref:plot-acc-1-3)"}

print(plots.acc.1[[2]])

```

(ref:plot-rmdspd-1-4) RMdSPD between true and estimated parameters of the four-state HMM in part one of the simulation. Labels on the x-axis indicate which true parameters have been manipulated and labels on the y-axis show for which estimated parameter the RMdSPD is displayed. Top facet labels specify in which state the parameters have been varied and right facet labels denote to which state estimated parameters belong. $\rho_i$ is the initial probability for state *i* (indicated by the right facet label), $a_{ij}$ is the probability to transition from state *i* to state *j*, $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution.

```{r plot-rmdspd-1-4, fig.cap="(ref:plot-rmdspd-1-4)"}

print(plots.rmsd.1[[3]] + theme(axis.text = element_text(size = 6)))

```

(ref:plot-regtr-1-4) Regression lines between true and estimated transition probabilities for the four-state HMM in part one. Right facet labels show *from* and top facet labels show *to* which state the HMM is moving. Dashed lines refer to perfect recovery.

```{r plot-regtr-1-4, fig.cap="(ref:plot-regtr-1-4)", fig.height=8}

print(plots.lm.tr[[3]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-regresp-1-4) Regression lines between true and estimated response parameters of the four-state HMM in part one. Top facet labels indicate response parameters. Dashed lines refer to perfect recovery. $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution. Parameter subscripts indicate states and eye movement metrics.

```{r plot-regresp-1-4, fig.cap="(ref:plot-regresp-1-4)", fig.height=7}

print(plots.lm.resp[[3]] + theme(axis.text = element_text(size = 6)))

```

(ref:plot-acc-1-4) Boxplots displaying Cohen's kappa depending on which parameter of the four-state HMM has been manipulated in part one. Top facet labels indicate for which state parameters have been manipulated. Black solid lines symbolize medians and hinges the first and third quartile. Whiskers range from hinges to lowest/highest value within 1.5 times the IQR. Crosses represent outliers. $a_{i=j}$ is the probability to stay in the same state, $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution.

```{r plot-acc-1-4, fig.cap="(ref:plot-acc-1-4)"}

print(plots.acc.1[[3]] + theme(axis.text = element_text(size = 8)))

```

\FloatBarrier

### Sample Size and Noise Variation
In the second part, we varied the sample size of the HMM and added noise to the generated data. For the two-state HMM, the RMdSPDs were above 0.5 for $\beta_{vel}$ and $\beta_{acc}$ in both states (see Figure \@ref(fig:plot-rmdspd-2-2)), suggesting bad recovery. The other estimated parameters showed RMdSPDs close to or below 0.1, which means they were recovered well. Increasing the sample size seemed to improve RMdSPDs for most parameters slightly. For $\beta_{vel}$ and $\beta_{acc}$ in both states, models with 2500 samples had the lowest RMdSPDs. Accuracy measured by Cohen's kappa was almost perfect with kappa values very close to one (see Figure \@ref(fig:plot-acc-2), left plot).  
For three states, the RMdSPDs for the $\beta_{vel}$ and $\beta_{acc}$ were above 0.5 in all three states (see Figure \@ref(fig:plot-rmdspd-2-3)), indicating bad recovery. Again, the other estimated parameters were below or close to 0.1, only $a_{12}$ and $a_{31}$ with 500 samples were closer to 0.5. For most parameters across all three states, models with higher sample sizes had lower RMdSPDs. The state recovery of the estimated models was almost perfect with most kappa values above 0.95 (see Figure \@ref(fig:plot-acc-2), middle plot). Several outliers clustered around kappa values of zero and -0.33.  
RMdSPDs regarding the four-state HMM are displayed in Figure \@ref(fig:plot-rmdspd-2-4). For states one and four, values for most parameters (including all transition probabilities) were above 0.5, suggesting bad recovery. Similarly, $\beta_{vel}$ and $\beta_{acc}$ in states two and three showed bad recovery. For states two and three, higher sample sizes showed slightly lower RMdSPDs. As in the previous part, most Cohen's kappa values ranged between 0.6 and 0.9, meaning substantial to almost perfect agreement between true and estimated states (Figure \@ref(fig:plot-acc-2), right plot). Multiple outliers clustered around 0.25 or zero.

(ref:plot-rmdspd-2-2) RMdSPD between true and estimated parameters of the two-state HMM in part two of the simulation. Colours indicate different sizes of generated data. Labels on the y-axis indicate for which estimated parameter the RMdSPD is displayed. Right facet labels denote to which state estimated parameters belong. $\rho_i$ is the initial probability for state *i* (indicated by the right facet label), $a_{ij}$ is the probability to transition from state *i* to state *j*, $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution.

```{r plot-rmdspd-2-2, fig.cap="(ref:plot-rmdspd-2-2)"}

print(plots.rmsd.234[[1]][[1]] + theme(axis.text.x = element_text(size = 10)))

```

(ref:plot-acc-2) Cohen's kappa depending on the variation of noise added to the data generated by the HMM. The upper labels on the x-axis indicate $\tau_{noise}$ and the lower labels $\kappa_{noise}$. Colours indicate different sizes of generated data. Top facet labels indicate the number of states in the HMM. $\rho_i$ is the initial probability for state *i* (indicated by the right facet label), $a_{ij}$ is the probability to transition from state *i* to state *j*, $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution.

```{r plot-acc-2, fig.cap="(ref:plot-acc-2)"}

print(plots.acc.int.24[[1]])

```

(ref:plot-rmdspd-2-3) RMdSPD between true and estimated parameters of the three-state HMM in part two of the simulation. Labels on the y-axis indicate for which estimated parameter the RMdSPD is displayed. Right facet labels denote to which state estimated parameters belong. $\rho_i$ is the initial probability for state *i* (indicated by the right facet label), $a_{ij}$ is the probability to transition from state *i* to state *j*, $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution.

```{r plot-rmdspd-2-3, fig.cap="(ref:plot-rmdspd-2-3)"}

print(plots.rmsd.234[[1]][[2]] + theme(axis.text.x = element_text(size = 10)))

```

(ref:plot-rmdspd-2-4) RMdSPD between true and estimated parameters of the three-state HMM in part two of the simulation. Labels on the y-axis indicate for which estimated parameter the RMdSPD is displayed. Right facet labels denote to which state estimated parameters belong. $\rho_i$ is the initial probability for state *i* (indicated by the right facet label), $a_{ij}$ is the probability to transition from state *i* to state *j*, $\alpha$ and $\beta$ are the shape and scale parameters of the gamma distributions, and $\mu$ and $\kappa$ are the mean and concentration parameter of the von Mises distribution.

```{r plot-rmdspd-2-4, fig.cap="(ref:plot-rmdspd-2-4)"}

print(plots.rmsd.234[[1]][[3]] + theme(axis.text.y = element_text(size = 8)))

```

\FloatBarrier

## Discussion
In the simulation study, we assessed the recovery of parameters and hidden states in the generative model of gazeHMM. Simulations in part one demonstrated that the HMM recovered parameters very well when they were manipulated. Deviations from true parameters were mostly small. In the four-state model, estimated transition probabilities for state one and four deviated moderately. Moreover, the HMM estimated state sequences very accurately with decreasing accuracy for the four-state model. In the second part, noise was added to the generated data and the sample size was varied. Despite noise, the generative model was still able to recover most parameters well. However, in the four-state model, the parameter recovery for states one and four substantially decreased (even for low amounts of noise, see supplementary material). In the three- and four-state models, scale parameters of gamma distributions were badly recovered (also even for low noise levels, see supplementary material). Increasing the sample size in the HMM slightly improved the recovery of most parameters. The state recovery of the model was slightly lowered when more states were included, but it was neither affected by the noise level nor the sample size. In the third part (included in the supplementary material), we showed that the variation in starting values used to fit the HMM did not influence parameter and state recovery. Missing data (in part four, also in the supplementary material) did not affect the parameter recovery but linearly decreased the recovery of hidden states. In all four parts, we observed clusters of outlying accuracy values. In part three, we exploratorily examined these clusters and reasoned that they can be attributed to label switching (i.e., flipping one or two state labels resolved the outlying clusters).  
In general, the generative model recovers parameters and hidden states well and, thus, we conclude that it can be used in our classification algorithm. However, the recovery decreases when a fourth state (i.e., smooth pursuit) is added to the model and, especially with four states, many parameters in the HMM are vulnerable to noise. In the next sections, we will see how noise that is present in real eye movement data affects the performance of gazeHMM.

# Validation Study
To validate gazeHMM, we applied the algorithm on two benchmark data sets. As starting values, we used $\rho=1/k$ for the initial state model as well as $a_{i=j}=0.9$ and $a_{i\neq j}=0.1/k$ for the transition model. The values for the response model are displayed in Table \@ref(tab:tab-starting-values). For a fifth eye movement event, we chose starting values that would enable the HMM to split any other event into two subevents (e.g., fixations into drift and microsaccades). In contrast to the simulation study, generating random starting values often led to bad model fits and label switching between states. To improve the fitting of the gamma distributions, velocity and acceleration signals were scaled down by factor 100 [^7] (so were the starting values for their gamma distributions).

[^7]: Scaling down by factor 100 differs from the simulation study (scaling down by 10). The algorithm allows the user to manually specify this factor, and in this case, factor 100 led to better model fits than factor 10.

(ref:tab-starting-values) Starting Values for the Response Model for Fitting gazeHMM to Benchmark Data
(ref:tab-starting-values-note) Starting values for velocity and acceleration signals are shown before scaling down by factor 100.

```{r tab-starting-values}

tab.start <- as.data.frame(matrix(c(10, 10, 10, 10, NA, NA,
                      50, 50, 50, 50, 0, 10,
                      50, 50, 50, 50, pi, 10,
                      20, 20, 20, 20, 0, 10,
                      20, 20, 50, 50, 0, 10), nrow = 5, byrow = T))

colnames(tab.start) <- c("Shape", "Scale", "Shape", "Scale", "Mean", "Concentration")
rownames(tab.start) <- c("Fixation", "Saccade", "PSO", "Pursuit", "Event 5")

apa_table(as.data.frame(tab.start),
          col_spanners = list(Velocity = c(2, 3), Acceleration = c(4, 5), "Rel. angle" = c(6, 7)),
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-starting-values)",
          note = "(ref:tab-starting-values-note)")

```

## Data Sets
We chose two data sets for validation: One was published in a study by @Andersson2017 and has been widely used for validation purposes [e.g., @Pekkanen2017]. It contains eye-tracking data from three conditions: A static condition, where subjects had to look freely at images, and two dynamic conditions, where they had to follow a constantly moving dot or objects in a video. The data were sampled with 500 Hz and two human coders (MN and RA) labeled them as belonging to six different eye movement events: Fixation, saccade, PSO, smooth pursuit, blink, or other. @Andersson2017 used the data to compare 10 different classification algorithms. We adopted their results to compare these 10 algorithms and the two human coders with gazeHMM. We used the original data from the study but removed two recordings from the moving dots condition because they majorly contained samples labeled as "other" or blinks. Moreover, the recordings could not be matched to the results obtained by @Andersson2017 [^4].  
The second data set was published in @Ehinger2019 and has to our knowledge not yet been used for validation. Here, we only took tasks four and five out of 10 tasks because these are qualitatively different from the first data set. In task four, subjects were instructed to fixate a central target for 20 s. Task 5 was set up similarly, but subjects had to blink when they heard one out of seven beeps (with a beep duration of 100 ms and 1.5 s intervals in between). Eye movements were recorded with 250-500 Hz. For simplicity, we only used data obtained by the EyeLink (SR Research Ltd., Ontario, Canada) eye-tracker.

[^4]: Two recordings from the moving dots conditions were substantially longer than the other recordings in the condition and contained more samples than were classified by the algorithms in the study by @Andersson2017. Since no sample indices were available in the data set, we could not match samples from the two recordings to the labels assigned by the algorithms and therefore decided to remove them from the analysis. We do not expect the conclusions of our analyses to depend on these two data sets.

## Data Analysis
Successful validation of gazeHMM was determined by two approaches: First, we applied gazeHMM with generative models containing 1-5 states to both data sets. The fits of the generative models were compared using Schwarz weights [@Wagenmakers2004], a conversion of the BIC [@Schwarz1978] into model weights. They can be interpreted as the probability of a model having generated the data compared to the competing models. For the static condition in the @Andersson2017 data set, we expected the generative model with three states (fixation, saccade, and PSO), and for the dynamic conditions the model with four states (incl. smooth pursuit) to display the highest Schwarz weight. Regarding the @Ehinger2019 data set, we assumed that the one-state model (only fixation) would show the highest weights for both tasks.    
The algorithm was applied separately to every subject, condition/task. For the @Andersson2017 data set, all generative models were successfully fitted, whereas, for the @Ehinger2019 data set, it was only 780 out of 900 models (87%, 60 models per task).  
Second, we compared gazeHMM to other algorithms and human coders. We applied our algorithm with a three-state generative model to the static condition in the @Andersson2017 data set, and with a four-state model to the dynamic conditions. For comparison criteria, we followed @Andersson2017: We calculated the RMSD of event durations and counts between all algorithms and the average of the two human coders. Our results differ slightly from the original study because we excluded two recordings (leading to less events) and calculated the event durations as $Dur(e) = max(\mathbf{t}_{e})-max(\mathbf{t}_{e-1})$, where $\mathbf{t_{e}}$ is the vector of sample time stamps for the event *e*. Cohen's kappa was calculated for each event as the binary agreement between the algorithms and the average of the human coders. Lastly, the overall disagreement indicated which samples were classified differently by the algorithms compared to the average of the human coders across all events. The human coders were compared directly to each other.

## Results
### Model Comparison

```{r load validation results, include=FALSE}

load(here("validation/Andersson2017_fitted.Rdata"))
load(here("validation/Andersson2017_raw.Rdata"))

```

```{r calculate Schwarz weights, include=FALSE}

# Compute Schwarz weights (from Wagenmakers & Farrell, 2004)

schwarz.weights <- function(bic, na.rm = T) {
  
  d.bic <- bic - min(bic, na.rm = na.rm) # eq 2
  
  exp(-0.5 * d.bic)/sum(exp(-0.5 * d.bic), na.rm = na.rm) # eq 4
  
}

A2017.bic <- lapply(A2017.fit, function(stim) {
  out <- lapply(stim, function(subj) {
    out <- lapply(1:length(subj), function(mod) {
      
      if(mod == 1) {
        
        bic <- try(-2*subj[[mod]][["LL"]] + 6*log(subj[[mod]][["N"]]))
        
      } else {
        
        bic <- try(BIC(subj[[mod]]$model))
        
      }
      
      return(ifelse(is.numeric(bic), bic, NA))
    })
    
    return(schwarz.weights(unlist(out)))
  })
  
  df <- as.data.frame(reduce(out, rbind))
  
  names(df) <- paste("model_", 1:length(stim[[1]]), sep = "")
  
  df$subject <- 1:nrow(df)
  
  return(df)
}) %>% reduce(rbind) %>% mutate(condition = rep(c("Moving dots", "Image", "Video"), c(9, 14, 9)))


# Create Schwarz weight plots

bic.plot <- A2017.bic %>%
  pivot_longer(names(A2017.bic)[1:5], names_to = "model", values_to = "weight") %>%
  mutate_at(c("subject", "model", "condition"), as.factor) %>%
  ggplot(aes(x = model, y = subject)) + geom_tile(aes(fill = weight), color = "black") +
  facet_grid(cols = vars(condition)) + 
  scale_x_discrete(name = "Number of states", labels = as.character(1:5)) +
  scale_y_discrete(name = "Subject") +
  scale_fill_viridis_c(name = "Schwarz\nweight", breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1))

```

Examining the Schwarz weights displayed in Figure \@ref(fig:plot-schwarz-weights), we observed that the five-state generative model showed the highest weights in all three conditions. Only in the moving dots condition, two subjects displayed the highest weights for the four-, and one subject for the three-state model. In sum, we concluded that the five-state generative model has most likely generated the @Andersson2017 data, opposing our expectations. Because the @Ehinger2019 data set showed a similar pattern, we included the results for this data in the supplementary material.  
A recent model recovery study showed that the BIC tended to prefer overly complex HMMs when they were misspecified [e.g., the conditional independence assumption was violated; @Pohle2017]. Instead, the integrated completed likelihood (ICL) criterion [@Biernacki2000] performed better at choosing the correct data-generating model. Therefore, we post hoc computed the weighted ICL criterion (analogous to Schwarz weights) for the models fitted to the @Andersson2017 data set. Using the ICL as the model selection criterion yielded very similar results to the BIC (see supplementary material). The preference for the five-state generative model was even more consistent across conditions and subjects.  

(ref:plot-schwarz-weights) Schwarz weights displayed for each subject and generative models with different numbers of states. Top facet labels indicate the condition in the @Andersson2017 data set. Higher weights indicate a better model fit.

```{r plot-schwarz-weights, fig.cap="(ref:plot-schwarz-weights)"}

print(bic.plot)

```

\FloatBarrier

### Comparison to Other Algorithms

```{r calculate duration RMSD, include=FALSE}

# Calculate event descriptives for algorithms and human coders

fr <- 500

alg.names <- names(A2017[[1]][[1]])[-c(1:5)]

alg.names <- c(alg.names[1:2], "gazeHMM-3", "gazeHMM-4", alg.names[3:length(alg.names)])[-7]

A2017.events <- lapply(1:4, function(e) {
  lapply(1:length(A2017), function(stim) {
    lapply(alg.names, function(alg) {
      if(alg == "gazeHMM-3") {
        
        dur <- sapply(A2017.fit[[stim]], function(df){
          
          return(df[[3]]$events[[e]]$dur)
        }) %>% reduce(append)
      } else if(alg == "gazeHMM-4") {
        
        dur <- sapply(A2017.fit[[stim]], function(df){
          
          return(df[[4]]$events[[e]]$dur)
        }) %>% reduce(append)
      } else {
        
        dur <- sapply(A2017[[stim]], function(df) {
          
          counter <- 1
          
          number <- numeric(nrow(df))
          
          number[1] <- 1
          
          for (i in 2:nrow(df)) {
            if(df[[alg]][i] != df[[alg]][i-1]) counter <- counter + 1
            
            number[i] <- counter
            
          }
          
          dur <- numeric(max(number))
          event <- numeric(max(number))
          
          for (n in unique(number)) {
            if(n > 1) {
              dur[n] <- max(df$t[number == n], na.rm = T) - max(df$t[number == (n-1)], na.rm = T)
            } else {
              dur[n] <- max(df$t[number == n], na.rm = T) - min(df$t[number == n], na.rm = T)
            }
            
            event[n] <- max(df[[alg]][number == n], na.rm = T)
          }
          
          return(dur[event == e])
        }) %>% reduce(append)
      }
      
      mdur <- ifelse(is.nan(mean(dur, na.rm = T)), NA, round(mean(dur, na.rm = T)*1e3))
      sddur <- ifelse(is.nan(sd(dur, na.rm = T)), NA, round(sd(dur, na.rm = T)*1e3))
      nrev <- ifelse(length(dur) == 0, NA, length(dur))
      
      return(data.frame(mdur, sddur, as.numeric(nrev)))
    }) %>% reduce(rbind)
  })
})


# Calculate RMSD between event distribution descriptives

A2017.rmsd <- lapply(A2017.events, function(event) {
  rmsd <- lapply(event, function(stim) {
    
    M.norm <- apply(stim, 2, function(x) {(x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T))})
    
    H <- M.norm[1:2,]
    A <- M.norm[-c(1:2),]
    
    rmsd.alg <- apply(A, 1, function(x) {sum(sqrt((x - rowSums(H/2))^2))})
    rmsd.cod <- sum(sqrt((H[1,] - H[2,])^2))
    
    return(as.data.frame(cbind(stim, rmsd = c(rmsd.cod, rmsd.cod, rmsd.alg))))
  }) %>% reduce(cbind)
  
  out <- cbind(alg.names, rmsd, stringsAsFactors = FALSE)
  
  names(out) <- c("Algorithm", rep(c("moving_dots", "image", "video"), each = 4))
  
  out[3, -c(1,6:9)] <- NA
  out[4, 6:9] <- NA
  
  out <- out[!apply(out[,-1], 1, function(x) {all(is.na(x))}),]
  
  return(out[,order(names(out))])
})

```

```{r calculate Cohen s kappa and confusion matrices, include= FALSE}

# Compute Cohen's kappa

A2017.kappa <- cbind.data.frame(alg.names, lapply(1:4, function(event) {
  out <- lapply(1:length(A2017), function(stim) {
    
    gazeHMM.3 <- lapply(A2017.fit[[stim]], function(subj) {
      
      subj[[3]]$samples$label
      
    }) %>% reduce(append)
    
    gazeHMM.4 <- lapply(A2017.fit[[stim]], function(subj) {
      
      subj[[4]]$samples$label
      
    }) %>% reduce(append)
    
    df <- cbind(reduce(A2017[[stim]], rbind)[,c(6,7)], gazeHMM.3, gazeHMM.4, reduce(A2017[[stim]], rbind)[,c(8:18)])[-7]
    
    kappa.alg <- sapply(names(df)[-c(1,2)], function(alg) {

      alg <- rep(df[[alg]], 2) == event
      cod <- c(df[,1], df[,2]) == event
      
      mat <- matrix(c(sum(alg & cod, na.rm = T), sum(alg & !cod, na.rm = T), 
                      sum(!alg & cod, na.rm = T), sum(!alg & !cod, na.rm = T)), 2, 2)
      
      return(cohen.kappa(mat)$kappa)
    })
    
    kappa.cod <- sapply(1:2, function(cod) {
      
      cod1 <- df[[cod]] == event
      cod2 <- df[,c(1,2)][-cod] == event
      
      mat <- matrix(c(sum(cod1 & cod2, na.rm = T), sum(cod1 & !cod2, na.rm = T), 
                      sum(!cod1 & cod2, na.rm = T), sum(!cod1 & !cod2, na.rm = T)), 2, 2)
      
      return(cohen.kappa(mat)$kappa)
    })
    
    return(c(kappa.cod, kappa.alg))
  }) %>% reduce(cbind)
  
  return(cbind(out[,2], out[,1], out[,3]))
}) %>% reduce(cbind), stringsAsFactors = F)


# Compute disagreement

A2017.disag <- cbind.data.frame(alg.names, lapply(1:length(A2017), function(stim) {
  
  gazeHMM.3 <- lapply(A2017.fit[[stim]], function(subj) {
    
    subj[[3]]$samples$label
    
  }) %>% reduce(append)
  
  gazeHMM.4 <- lapply(A2017.fit[[stim]], function(subj) {
    
    subj[[4]]$samples$label
    
  }) %>% reduce(append)
  
  df <- cbind(reduce(A2017[[stim]], rbind)[,c(6,7)], gazeHMM.3, gazeHMM.4, reduce(A2017[[stim]], rbind)[,c(8:18)])[-7]
  
  disag.alg <- sapply(names(df)[-c(1,2)], function(alg) {
    
    if(alg == "gazeHMM.3" || alg == "gazeHMM.4") {df[[alg]][df[[alg]] == 0] <- 5}
    
    alg <- rep(df[[alg]], 2)
    cod <- c(df[,1], df[,2])
    
    return(mean(alg != cod)*100)
  })
  
  disag.cod <- sapply(1:2, function(cod) {
    
    cod1 <- df[[cod]]
    cod2 <- df[,c(1,2)][-cod]
    
    return(mean(cod1 != cod2)*100)
  })
  
  return(c(disag.cod, disag.alg))
}) %>% reduce(cbind), stringsAsFactors = F)


# Create figure for disagreement ratio

names(A2017.disag) <- c("Algorithm", "Dots", "Image", "Video")

disag.df <- A2017.disag %>% pivot_longer(names(A2017.disag)[2:4], names_to = "Condition", values_to = "Ratio")

disag.df$Ratio[disag.df$Algorithm == "gazeHMM-4" & disag.df$Condition == "Image"] <- disag.df$Ratio[disag.df$Algorithm == "gazeHMM-3" & disag.df$Condition == "Image"]
disag.df$Algorithm[disag.df$Algorithm == "gazeHMM-4"] <- "gazeHMM-3/4"

disag.df <- disag.df %>%
  dplyr::filter(Algorithm != "gazeHMM-3") %>%
  group_by(Algorithm) %>%
  mutate(MRatio = mean(Ratio)) %>%
  ungroup() %>%
  arrange(MRatio)

disag.df$Algorithm <- factor(disag.df$Algorithm, levels = unique(disag.df$Algorithm))

disag.plot <- ggplot(disag.df, aes(x = Algorithm, y = Ratio, fill = Condition)) + 
  geom_col(position = "dodge", color = "black") + 
  scale_y_continuous(name = "Disagreement (in %)", limits = c(0, 100), breaks = c(0, 0.25, 0.5, 0.75, 1)*100) +
  scale_x_discrete(name = "") + 
  scale_fill_viridis_d() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Compute confusion matrix

A2017.conf <- as.data.frame(lapply(1:length(A2017), function(stim) {
  
  gazeHMM.3 <- lapply(A2017.fit[[stim]], function(subj) {
    
    subj[[3]]$samples$label
    
  }) %>% reduce(append)
  
  gazeHMM.4 <- lapply(A2017.fit[[stim]], function(subj) {
    
    subj[[4]]$samples$label
    
  }) %>% reduce(append)
  
  df <- reduce(A2017[[stim]], rbind)[,c(6,7)]
  
  cod <- c(df[,1], df[,2])
  
  conf.tab <- matrix(0, 6, 5)
  
  if(stim == 2) {
    
    alg <- rep(gazeHMM.3, 2)
    
    alg[alg == 0] <- 5
    
    conf.tab[,c(1:3, 5)] <- table(cod, alg)/rowSums(table(cod, alg))
    
  } else if(stim == 1) {
    
    alg <- rep(gazeHMM.4, 2)
    
    alg[alg == 0] <- 5
    
    conf.tab[c(1:4,6),] <- table(cod, alg)/rowSums(table(cod, alg))
    
  } else {
    
    alg <- rep(gazeHMM.4, 2)
    
    alg[alg == 0] <- 5
    
    conf.tab <- table(cod, alg)/rowSums(table(cod, alg))
    
  }
  
  return(t(conf.tab))
}) %>% reduce(rbind))  %>% cbind(Event = rep(c("Fixations", "Saccades", "PSOs", "Pursuits", "Blinks"), 3),
                                 Condition = rep(c("Moving dots", "Image", "Video"), each = 5), .) %>%
  arrange(Condition) %>%
  dplyr::select(-Condition)

names(A2017.conf) <- c("Event", "Fixation", "Saccade", "PSO", "Pursuit", "Blink", "Other")

```

As displayed in Table \@ref(tab:tab-rmsd-4-fix), gazeHMM showed a relatively low RMSD for fixations in the static condition compared to the other algorithms that were applied to the @Andersson2017 data set. The lower RMSD for fixations indicated more similar classification to the human coders in terms of their mean and SD duration as well as the number of classified fixations. Oppositely, for fixations in the dynamic conditions, the RMSD of gazeHMM was one of the highest among the compared algorithms, suggesting substantial differences to the human coders. It can be seen that gazeHMM classified a much larger number of fixations with very short durations. For saccades, gazeHMM had a relatively high RMSD for the static condition but the lowest RMSD for the moving dots condition, and a moderate value for the video condition (see Table \@ref(tab:tab-rmsd-4-sac)). The deviation was mostly because gazeHMM classified a higher number of saccades than the human coders. Only two other algorithms classified PSOs [NH and LNS; @Nystrom2010; @Larsson2013]. Here, gazeHMM showed a consistently higher RMSD than LNS and lower RMSD than NH (see Table \@ref(tab:tab-rmsd-4-pso)). Our algorithm classified shorter and more PSOs than the human coders. No other algorithm parsed smooth pursuits, but the RMSD for gazeHMM was higher than among human coders (see Table \@ref(tab:tab-rmsd-4-sp)). Again, it classified a much larger number of smooth pursuits with short durations.  
Table \@ref(tab:tab-kappa-4) contains the sample-to-sample agreement between the algorithms and human coders measured by Cohen's kappa. For fixations, gazeHMM showed one of the highest agreements for static and *the* highest agreements for dynamic data. The absolute agreement was substantial for the static and slight to fair for the dynamic conditions [@Landis1977]. For saccades, gazeHMM had the lowest agreement for the static condition and moderate agreement for the dynamic conditions. In absolute terms, the agreement was fair to moderate. Concerning PSOs, gazeHMM showed higher agreement than NH in the image and video conditions but consistently lower agreement compared to LNS. The absolute agreement was slight (image) to moderate (video). Lastly, the agreement for smooth pursuit was lower compared to the human coders and fair in absolute values.  
Concerning overall disagreement, Figure \@ref(fig:plot-disag) shows that gazeHMM had less disagreement to the human coders across all events for the dynamic conditions. For the static condition, we interpreted the difference to most other algorithms as slight ($\text{Med} (\Delta)  = `r abs(median(A2017.disag[["Image"]][3] - A2017.disag[["Image"]][-c(1:4)]))`$%), but for the dynamic conditions, as substantial (video: $\text{Med} (\Delta)  = `r abs(median(A2017.disag[["Video"]][4] - A2017.disag[["Video"]][-c(1:4)]))`$%) and large (dots: $\text{Med} (\Delta)  = `r abs(median(A2017.disag[["Dots"]][4] - A2017.disag[["Dots"]][-c(1:4)]))`$%).  
To explore which events gazeHMM classified differently than the average human coder, we looked at the confusion matrix between the two (see Table \@ref(tab:tab-conf-4)). It can be seen that gazeHMM classified many fixations as smooth pursuits and vice versa. Moreover, it confused many PSOs with saccades. The heuristic to detect blinks seemed to work successfully since gazeHMM classified most blink samples in agreement with human coding and only a minor part was mistaken for saccades. Inspecting an example of gaze data classified by gazeHMM compared to human coding leads to a similar notion: Figure \@ref(fig:plot-example-classification) illustrates that gazeHMM is rapidly switching between classifying fixations and smooth pursuits, whereas the human coder treats the same data as one large smooth pursuit event. In the example, the human coder also delayed the start of the PSO compared to gazeHMM.

(ref:tab-rmsd-4-fix) Fixation Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-fix-note) Durations are displayed in milliseconds. gazeHMM-3 classified three and gazeHMM-4 classified four events. RMSD = root mean square deviation. Table design adapted from @Andersson2017.

```{r tab-rmsd-4-fix}

apa_table(A2017.rmsd[[1]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = TRUE),
          caption = "(ref:tab-rmsd-4-fix)",
          note = "(ref:tab-rmsd-4-fix-note)")

```

(ref:tab-rmsd-4-sac) Saccade Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-sac-note) Durations are displayed in milliseconds. gazeHMM-3 classified three and gazeHMM-4 classified four events. RMSD = root mean square deviation. Table design adapted from @Andersson2017.

```{r tab-rmsd-4-sac}

apa_table(A2017.rmsd[[2]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = TRUE),
          caption = "(ref:tab-rmsd-4-sac)",
          note = "(ref:tab-rmsd-4-sac-note)")

```

(ref:tab-rmsd-4-pso) PSO Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-pso-note) Durations are displayed in milliseconds. gazeHMM-3 classified three and gazeHMM-4 classified four events. RMSD = root mean square deviation. Table design adapted from @Andersson2017.

```{r tab-rmsd-4-pso}

apa_table(A2017.rmsd[[3]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = TRUE),
          caption = "(ref:tab-rmsd-4-pso)",
          note = "(ref:tab-rmsd-4-pso-note)")

```

(ref:tab-rmsd-4-sp) Smooth Pursuit Duration Descriptives and RMSD Between gazeHMM and Human Coders
(ref:tab-rmsd-4-sp-note) Durations are displayed in milliseconds. gazeHMM-3 classified three and gazeHMM-4 classified four events. RMSD = root mean square deviation. Table design adapted from @Andersson2017.

```{r tab-rmsd-4-sp}

apa_table(A2017.rmsd[[4]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = TRUE),
          caption = "(ref:tab-rmsd-4-sp)",
          note = "(ref:tab-rmsd-4-sp-note)")

```

(ref:tab-kappa-4) Cohen's Kappa Between Human Coders and Algorithms for Different Conditions and Events
(ref:tab-kappa-4-note) gazeHMM-3 classified three and gazeHMM-4 classified four events. Table design adapted from @Andersson2017.

```{r tab-kappa-4}

A2017.kappa[3, -c(1, seq(2, 13, 3))] <- NA
A2017.kappa[4, seq(2, 13, 3)] <- NA

names(A2017.kappa) <- c("Algorithm", rep(c("Image", "Dots", "Video"), 4))

apa_table(A2017.kappa,
          col_spanners = list("Fixations" = c(2, 4), "Saccades" = c(5, 7), "PSOs" = c(8, 10), "Smooth pursuits" = c(11, 13)),
          row.names = FALSE,
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-kappa-4)",
          note = "(ref:tab-kappa-4-note)")

```

(ref:plot-disag) Disagreement between algorithms and human coders for different conditions (in %). gazeHMM-3/4 classified three events for image data and four events for moving dots/video data. Algorithms are displayed in order according to mean disagreement over conditions (least/left to highest/right).

```{r plot-disag, fig.cap="(ref:plot-disag)"}

print(disag.plot)

```

(ref:tab-conf-4) Confusion Matrix Between gazeHMM (Rows) and Human Coders (Columns) for Different Conditions
(ref:tab-conf-4-note) gazeHMM classified four events and blinks. Values indicate proportions of samples where gazeHMM and human coders agree divided by the total number of samples classified by the human coders for each event (i.e., columns sum to one). 

```{r tab-conf-4}

apa_table(A2017.conf,
          stub_indents = list("Image" = c(1:5), "Moving dots" = c(6:10), "Video" = c(11:15)),
          row.names = FALSE,
          midrules = c(6, 12),
          caption = "(ref:tab-conf-4)",
          note = "(ref:tab-conf-4-note)")

```

(ref:plot-example-classification) Example data from @Andersson2017 displayed as x-, and y-coordinates, velocity (in deg/s), acceleration (in deg/s$^2$), and sample-to-sample (relative) angle (in rad). Colors indicate classification by gazeHMM and symbols by human coder MN. Note that the human coder classified all fixations by gazeHMM as smooth pursuits and delayed the separation between the saccade and PSO.

```{r plot-example-classification, fig.cap="(ref:plot-example-classification)", fig.height=7.5}

cbind(A2017.fit[[3]][[3]][[4]]$samples, coderMN = as.factor(A2017[[3]][[3]]$coderMN)) %>% 
  gather(c("x", "y", "vel", "acc", "angle"), key = metric, value = value, factor_key = T) %>% 
  mutate_at(vars(label, state), as.factor) %>%
  dplyr::filter(t > 4.2, t <= 4.5) %>% 
  rename(gazeHMM = label, Time = t) %>% 
  mutate(metric = fct_recode(metric, "x-coordinate" = "x", "y-coordinate" = "y", "Velocity" = "vel",
                             "Acceleration" = "acc", "Rel. angle" = "angle"),
         coderMN = fct_recode(coderMN, "Fix" = "1", "Sacc" = "2", "PSO" = "3", "SP" = "4", "Blink" = "5", "Other" = "6"),
         gazeHMM = fct_recode(gazeHMM, "Noise" = "0", "Fix" = "1", "Sacc" = "2", "PSO" = "3", "SP" = "4")) %>%
  ggplot(aes(x = Time, y = value)) + 
  facet_wrap(vars(metric), ncol = 1, scales = "free", strip.position = "left") +
  geom_path() + xlab("Time in s") + ylab("") +
  geom_point(aes(color = gazeHMM, shape = coderMN), size = 2) + 
  scale_color_viridis_d() +
  theme(strip.placement = "outside",
        axis.text = element_text(size = 8))

```

\FloatBarrier

# General Discussion
In this report, we presented gazeHMM, a novel algorithm for classifying gaze data into eye movement events. The algorithm models velocity, acceleration, and sample-to-sample angle signals with gamma distributions and a mixture of von Mises and a uniform distribution. An HMM serves as the generative model of the algorithm and classifies the gaze samples into fixations, saccades, and optionally PSOs, and/or smooth pursuits. We showed in a simulation study that the generative model of gazeHMM recovered parameters and hidden state sequences well with a few exceptions: Adding a fourth event (i.e., smooth pursuit) to the model and introducing even small amounts of noise to the generated data were the most critical factors for decreasing recovery. Furthermore, we applied gazeHMM with different numbers of states to benchmark data by @Andersson2017 and compared the model fit. The model comparison revealed that a five-state HMM had consistently most likely generated the data. This result opposed our expectation that a three-state model would be preferred for static and a four-state model for dynamic data. When comparing gazeHMM against other algorithms, gazeHMM showed mostly good agreement to human coding. On one hand, it outperformed the other algorithms in the overall disagreement with human coding for dynamic data. On the other hand, gazeHMM confused a lot of fixations with smooth pursuits, which led to rapid switching between the two events. It also tended to mistake PSO samples as belonging to saccades.  
Considering the results of the simulation study, it seems reasonable that adding the smooth pursuit state to the HMM decreased parameter and state recovery: It is the event that is overlapping most closely with another event (fixations) in terms of velocity, acceleration, and sample-to-sample angle. The overlap can cause the HMM to confuse parameters and hidden states. The decrease in parameter recovery (especially for scale parameters) due to noise shows that the overlap is enhanced by more dispersion in the data. The scale parameters might be particularly vulnerable to extreme data points. Despite these drawbacks, the recovery of the generative model in gazeHMM seems very promising. The simulation study gives also an approximate reference for the maximum recovery of hidden states that can be achieved by the HMM (Cohen's kappa values of ~1 for two, ~0.95 for three, and ~0.8 for four events).  
The model comparison on the benchmark data suggested that the generative model in gazeHMM is not optimally specified for eye movement data, yet. There are several explanations for this result:  
*There were more eye movement events present in the data than we expected*. Eye movement events can be divided into subevents. For example, fixations consist of drift and tremor movements [@Duchowski2017] and PSOs encompass dynamic, static, and glissadic over- and undershoots [@Larsson2013]. A study on a recently developed HMM algorithm supports this explanation: @Houpt2018 applied the unsupervised BP-AR-HMM algorithm to the @Andersson2017 data set and classified more distinct states than the human coders. Some of the states classified by BP-AR-HMM matched the same event coded by humans. Since the subevents are usually not interesting for users of classification algorithms, the ability of HMMs to classify might limit their ability to generate eye movements.  
*Model selection criteria are generally not appropriate for comparing HMMs with different numbers of states*. This argument has been discussed in the field of ecology [see @Li2017], where studies found that selection criteria preferred models with more states than expected [similar to the result of this study; e.g., @Langrock2015]. @Li2017 explain this bias with the simplicity of the submodels in HMMs: Initial state, transition, and response models for each state are usually relatively simple. When they do not describe the processes in the respective states accurately, the selection criteria compensate for that by preferring a model with more states. Thus, there are not more latent states present in the data, but the submodels of the HMM are misspecified or too simple. Correcting for model misspecifications led to a better model recovery in studies on animal movements [@Langrock2015; @Li2017]. However, @Pohle2017 showed in simulations that the ICL identified the correct model despite several misspecifications. It has to be noted that the study by @Pohle2017 only used data generating models with two states.  
*The submodels of gazeHMM were misspecified*. @Pohle2017 identified two scenarios in which model recovery using the ICL did not give optimal results: Outliers in the data and inadequate distributions in the response models. Both situations could apply to gazeHMM and eye movement data: Outliers occur frequently in eye-tracking data due to measurement error. Choosing adequate response distributions in HMMs is usually difficult and can depend on the individual and task from which the data are obtained [@Langrock2015]. Moreover, gazeHMM only estimated intercepts for all parameters and thus, no time-varying covariates were included [cf. @Li2017]. This aspect could indeed oversimplify the complex nature of eye movement data.  
Comparing gazeHMM to other algorithms on benchmark data showed that gazeHMM showed good agreement with human coders. However, the evaluation criteria (RMSD of event durations, sample-to-sample agreement, and overall disagreement) yielded different results. The fact that gazeHMM outperformed all other algorithms regarding the overall disagreement can be because it is the only algorithm classifying five events [incl. blinks; see also the *accuracy paradox*, @Zemblys2019]. Nevertheless, Cohen's kappa values of 0.67 (fixations - image) or 0.62 (saccades - moving dots) indicate substantial agreement to human coders, especially in light of the maximum references from the simulation study. At this point, it is important to mention that human coding should not be considered a gold standard in event classification: @Hooge2018 observed substantial differences between coders and within coders over time. Despite these differences, they recommend comparisons to human coding to demonstrate the performance of new algorithms and to find errors in their design.

## Advantages of gazeHMM
Given the four proposed goals that gazeHMM should fulfill, we can draw the following conclusions: Even though gazeHMM does require some parameter settings (in the pre- and postprocessing), they only have a minor influence on the classification and are merely included to compensate for some drawbacks of the generative model. Their default values should be appropriate for most applications. A major advantage of gazeHMM is that it does not require human-labeled data as input. Instead, it estimates all parameters and hidden states from the data. Since human coding is quite laborious, difficult to reproduce, and by times inconsistent [as noted earlier, @Hooge2018], this property makes gazeHMM a good alternative to other recently developed algorithms that require human coded input [@Pekkanen2017; @Zemblys2018; @Bellet2019]. This could also explain why the agreement to human coding is lower for gazeHMM than for algorithms that learn from human-labeled data. Another advantage of gazeHMM is its ability to classify four eye movement events, namely fixations, saccades, PSOs, and smooth pursuit. Whereas most algorithms only parse fixations and saccades [@Andersson2017], few classify PSOs [e.g., @Zemblys2018], and even less categorize smooth pursuits [e.g., @Pekkanen2017]. However, including smooth pursuits in gazeHMM led to some undesirable classifications on benchmark data, resulting in rapid switching between fixation and smooth pursuit events. Therefore, we recommend using gazeHMM with four events only for exploratory purposes. Without smooth pursuits, we consider gazeHMM's classification as appropriate for application. Lastly, its implementation in R using depmixS4 [@Visser2010] should make gazeHMM a tool that is easy to use and customize for individual needs.

## Future Directions
Despite its advantages, there are several aspects in which gazeHMM can be improved: First, a multivariate distribution could be used to account for the correlation between velocity and acceleration signals [for examples, see @Balakrishnan2009]. Potential problems of this approach might be choosing the right distribution and convergence issues (due to a large number of parameters). Another option to model the correlation could be to include one of the response variables as a covariate of the other.  
Second, instead of the gamma being the generic (and potentially inappropriate) response distribution, a non-parametric approach could be used: @Langrock2015 use a linear combination of standardized B-splines to approximate response densities, which led to HMMs with fewer states being preferred. This approach could potentially combat the problem of unexpectedly high-state HMMs being preferred for eye movement data but would also undermine the advantages of using a parametric model.  
Third, one solution to diverging results when comparing gazeHMM with different events could be model averaging: Instead of using the maximum posterior state probability of each sample from the preferred model, the probabilities could be weighted according to a model selection criterion (e.g., Schwarz weight) and averaged. Then, the maximum averaged probability could be used to classify the samples into events. This approach could lead to a more robust classification because it reduces the overconfidence of each competing model and easily adapts to new data [analogous to Bayesian model averaging; @Hinne2020]. However, the model comparison for gazeHMM often showed extreme weights for a five-state model, which would lead to a very limited influence of the other models in the averaged probabilities.  
Fourth, including covariates of the transition probabilities and response parameters could improve the fit of gazeHMM on eye movement data. As pointed out earlier, just estimating intercepts of parameters could be too simple to model the complexity of eye movements. A candidate for such a covariate might be a periodic function of time [@Li2017] which could, for instance, capture the specific pattern of saccades. Whether covariates are improving the fit of submodels to eye movement data could in turn be assessed by inspecting pseudo-residuals and autocorrelation functions [see @Zucchini2016].  
Fifth, to avoid rapid switching between fixations and smooth pursuits as well as unreasonably short saccades, gazeHMM could explicitly model the duration of events. This can be achieved by setting the diagonal transition probabilities to zero and assign a distribution of state durations to each state [@Bishop2006]. Consequently, the duration distributions of fixations and smooth pursuits could differ from saccades and PSOs. This extension of the HMM is also called the hidden semi-Markov model and has been successfully used by @Mihali2017 to classify microsaccades. Drawbacks of this extension are higher computational costs and difficulties with including covariates [@Zucchini2016].  
Lastly, allowing constrained parameters in the HMM could replace some of the postprocessing steps in gazeHMM. This could potentially be achieved by using different response distributions or parameter optimization methods. Moreover, switching from the maximum likelihood to the Markov chain Monte Carlo (Bayesian) framework could help to avoid convergence problems with constrained parameters, but would also open new research questions about suitable priors for HMM parameters in the eye movement domain, efficient sampling plans, accounting for label switching, and computational efficiency, naming only a few.

## Conclusion
In the previous sections, we developed and tested a generative, HMM-based algorithm called gazeHMM. Both a simulation and validation study showed that gazeHMM is a suitable algorithm for classifying eye movement events. For smooth pursuits, the classification is not optimal and thus not yet recommended. On one side, the algorithm has some advantages over concurrent event classification algorithms, not relying on human-labeled training data being the most important one. On the other side, it is not able to perform model comparisons as expected and still poses difficulties regarding good model specifications for different eye movement events. These difficulties highlight challenges for HMMs in general but also specifically for eye movements.

# Declarations
## Funding
Šimon Kucharský was supported by the NWO (Nederlandse Organisatie voor Wetenschappelijk Onderzoek) grant no. 406.10.559.

## Conflicts of Interest
The authors declare that they have no financial or non-financial interests regarding the content of this article.

## Ethics Approval
Not applicable.

## Consent to Participate
Not applicable.

## Consent to Publication
Not applicable.

## Availability of Data and Materials
See Open Practices and Data Availability Statement.

## Code Availability
See Open Practices and Data Availability Statement. 

# Open Practices and Data Availability Statement
The data and materials for the validation study [@Andersson2017; @Ehinger2019; @Larsson2013; @Larsson2015] are available on GitHub at <https://github.com/richardandersson/EyeMovementDetectorEvaluation> and on Figshare at <https://doi.org/10.6084/m9.figshare.c.4379810.v1>. The code to obtain the results for the simulation and validation study is available at <https://github.com/maltelueken/gazeHMM_validation> and the algorithm can be accessed at <https://github.com/maltelueken/gazeHMM>. The simulation study was preregistered (<https://doi.org/10.17605/OSF.IO/VDJGP>), whereas the validation study was not.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
