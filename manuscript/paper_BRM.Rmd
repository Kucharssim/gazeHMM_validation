---
title             : "Keeping an Eye on Hidden Markov Models in Gaze Data Classification"
shorttitle        : "HMMs in Gaze Classification"

author: 
  - name          : "Malte Lueken"
    affiliation   : "1"
    corresponding : yes
    address       : "Postal address"
    email         : "malte_lueken@arcor.de"
  - name          : "Simon Kucharsky"
    affiliation   : "1"
  - name          : "Ingmar Visser"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, University of Amsterdam. Amsterdam, The Netherlands"

authornote: |
  This is an author note.

abstract: |
  Eye-tracking allows researchers to infer cognitive processes from eye movements that are classified into distinct events. Parsing the events is typically done by algorithms that transform, filter, classify, and merge raw data samples. Previous algorithms have successfully used hidden Markov models (HMMs) for classification but still inhere weaknesses. Therefore, I developed gazeHMM, an HMM algorithm that has no critical parameters to be set by users, does not require human coded data as input, and classifies fixations, saccades, PSOs, and smooth pursuits. The development was guided by the question of whether HMMs are useful at describing eye movements and whether they improve the event classification. I evaluated gazeHMM in a simulation study and on benchmark data. The simulation study showed that gazeHMM successfully recovered HMM parameters and hidden state sequences. Critical exceptions for good recovery were adding a smooth pursuit like state to gazeHMM and noisy data. For benchmark data, model comparisons with gazeHMM yielded preferred models with more states than expected. I assessed the classification performance of gazeHMM compared to other algorithms by the agreement to human event coding. Here, gazeHMM improved the event classification but did not outperform all other algorithms in most cases. Both the simulation study and benchmark application showed poor classification performance for smooth pursuits. Thus, I advice to classify smooth pursuits only for exploration and recommend gazeHMM with fixations, saccades, and PSOs for application. Future HMM algorithms could use covariates to better capture eye movement processes and explicitly model event durations to improve the classification of smooth pursuits.
  
keywords          : "eye movements; eye-tracking; parameter recovery; dependent mixture models"
wordcount         : "12962"

bibliography      : "report.bib"

floatsintext      : yes
figurelist        : yes
tablelist         : yes
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

# Introduction

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, results = "asis")

library(papaja)
library(tidyverse)
library(signal)
library(flexcircmix)
library(CircStats)
library(depmixS4)
library(psych)
library(here)
library(grid)
library(gridExtra)
library(DiagrammeR)
library(DiagrammeRsvg)
library(magrittr)
library(rsvg)
source(here("algorithm/preprocessing_helper_functions.R"))
source(here("algorithm/model_helper_functions.R"))

```

```{r set graphics theme, include=FALSE}

theme_set(theme_apa())

```

```{r load data, include=FALSE}

load(here("validation/Andersson2017_raw.Rdata"))

res <- c(1024, 768)
dim <- c(380, 300)
dist <- 670
fr <- 500

```

# Simulation Study
To assess how well the HMM recovers parameters and state sequences, we conducted a simulation study. The design and analysis of the study were preregistered on the Open Science Framework (https://doi.org/10.17605/OSF.IO/VDJGP). This section is majorly copied from the preregistration (with adapted tenses). The study was divided in four parts. Here, we only report the first two parts, the other two parts, which address starting values and missing data, can be found in the supplementary material (URL).  
The HMM repeatedly generated data with a set of parameters (true parameter values). The same model was applied to estimate the parameters from the generated data (estimated parameter values). We compared the true with the estimated parameter values to assess whether a parameter was recovered by the model. Additionally, we contrasted the true states of the HMM with the estimated states to judge how accurately the model recovered the states that generated the data.

## Starting Values
The HMM always started with a uniform distribution to estimate the initial state and state transition probabilities. Random starting values for the estimation of shape, scale, and concentration parameters were generated by gamma distributions with a shape parameter of $\alpha_{start}=3$ and $\beta_{start}=\psi_{true}/2$, with $\psi_{true}$ being the true value of the parameter to be estimated. This setup ensured that the starting values were positive, their distributions were moderately skewed, and the modes of their distributions equaled the true parameter values. Mean parameters of the von Mises distribution always started at their true values.

## Design
In the first part, we varied the parameters of the HMM. For models with $k \in \{2, 3, 4\}$ states, $q \in \{10, 15, 20\}$ parameters were manipulated, respectively. For each parameter, the HMM generated 100 data sets with $N = 2500$ samples and the parameter varied in a specified interval in equidistant steps. This resulted in $100 \times (10+15+20) = 4500$ recoveries. Only one parameter alternated at once, the other parameters were set to their default values. All parameters of the HMM were estimated freely (i.e., there were no fixed parameters in the model). We did not manipulate the initial state probabilities because these are usually irrelevant in the context of eye movement classification. For the transition probabilities, we only simultaneously changed the probabilities for staying in the same state (diagonals of the transition matrix) to reduce the complexity of the simulation. The left over probability mass was split evenly between the probabilities for switching to a different state (per row of the transition matrix). Moreover, we did not modify the mean parameters of the von Mises distributions: As location parameters, they do not alter the shape of the distribution and they are necessary features for the HMM to distinguish between different states.  
We defined approximate ranges for each response variable (see supplementary material) and chose true parameter intervals and default values so that they produced samples that roughly corresponded to these ranges. Tables \@ref(tab:sim-parameters-trans) and \@ref(tab:sim-parameters-resp) show the intervals and default values for each parameter in the simulation. Parameters were scaled down by factor 10 (compared to the reported ranges) to improve fitting of the gamma distributions. We set the intervals for shape parameters of the gamma distributions for all events to [1,5] to examine how skewness influenced the recovery (shape values above five approach a symmetric distribution). The scale parameters were set so that the respective distribution approximately matched the assumed ranges. Since the concentration parameters of the von Mises distribution are the inverse of standard deviations, they were varied on the inverse scale. An example of simulated data from the HMM with default parameters is visualized in Figure \@ref(fig:plot-sim-ex).  
In the second part, we manipulated the sample size of the generated data and the amount of noise added to it. The model parameters were set to their default values. For models with $k \in \{2, 3, 4\}$ states and sample sizes of $N \in \{500, 2500, 10000\}$, we generated 100 data sets ($100 \times 3 \times 3 = 900$ recoveries). These samples sizes roughly corresponded to small, medium, and large eye-tracking data sets for a single participant and trial. To simulate noise, we replaced velocity and acceleration values $y$ with draws from a gamma distribution with $\alpha_{noise} = 3$ and $\beta_{noise}=(y/2)\tau_{noise}$ with $\tau_{noise} \in [1,5]$ varying between data sets. This procedure ensured that velocity and acceleration values remained positive and were taken from moderately skewed distributions with modes equal to the original values. To angle, we added white noise from a von Mises distribution with $\mu_{noise} = 0$ and $\kappa_{noise} \in 1/[0.1,10]$ varying between data sets. $\tau_{noise}$ and $\kappa_{noise}$ were increased simultaneously in equidistant steps in their intervals.

## Data Analysis
For each parameter, we calculated the root median square proportion deviation [RMdSPD; analogous to root median square percentage errors, see @Hyndman2006] between the true and estimated parameter values: $$\text{RMdSPD} = \sqrt{\text{Med}\left((\frac{\psi_{true}-\psi_{est}}{\psi_{true}})^2\right)}.$$
Even though it was not explicitly mentioned in the preregistration, this measure is only appropriate when $\psi_{true} \ne 0$. This was not the case for some mean parameters of the von Mises distributions. In those cases, we used $\psi_{true} = 2\pi$ instead. We treated $\text{RMdSPD} < 0.1$ as good, $0.1 \le \text{RMdSPD} < 0.5$ as moderate, and $\text{RMdSPD} \ge 0.5$ as bad recovery of a parameter. By taking the median, we reduced the influence of potential outliers in the estimation and using proportions enabled us to compare RMdSPD values across parameters and data sets.  
Additionally, we applied a bivariate linear regression with the estimated parameter values as the dependent and the true parameter values as the independent variable to each parameter that has been varied on an interval in part one. Regression slopes closer to one indicated that the model better captured parameter change. Regression intercepts different from zero reflected a bias in parameter estimation.  
To assess state recovery, we computed Cohen's kappa (for all events taken together, not for each event separately) as a measure of agreement between true and estimated states for each generated data set. Cohen's kappa estimates the agreement between two classifiers accounting for the agreement due to chance. Higher kappa values were interpreted as better model accuracy. We adopted the ranges proposed by @Landis1977 to interpret kappa values.
Models that could not be fitted were excluded from the recovery.

(ref:sim-parameters-trans) Intervals and Default Parameter Values for the Transition Model in the Simulation
(ref:sim-parameters-trans-note) The transition probability for staying in the same state is denoted by $a_{i=j}$ and the probability for switching to a different state by $a_{i\neq j}$. The number of states in the model is denoted by *k*.

```{r sim-parameters-trans}

sim.pars.trans <- as.data.frame(matrix(c("Interval", "-", "[.01,.99]", "1-a(i=j)/(k-1)",
                                           "Default", "1/k", "0.9", "0.1/(k-1)"), nrow = 2, byrow = T))

apa_table(sim.pars.trans,
          col.names = c("", "Initial\nstate\nprob.", "Trans. prob.\nsame state", "Trans. prob.\nother state"),
          caption = "(ref:sim-parameters-trans)",
          note = "(ref:sim-parameters-trans-note)")

```

(ref:sim-parameters-resp) Intervals and Default Parameter Values for the Response Model in the Simulation
(ref:sim-parameters-resp-note) Shape parameters are denoted by $\alpha$, scale parameters by $\beta$, mean parameters by $\mu$, and concentration parameters by $\kappa$. The default values for the uniform distribution in state one were min = 0 and max = $2\pi$.

```{r sim-parameters-resp}

sim.pars.resp <- as.data.frame(t(matrix(c("[1,5]", "[0.1,0.6]", "[1,5]", "[0.05,0.25]", "-", "-", 
                                  "3", "0.35", "3", "0.25", "-", "-",
                                  "[1,5]", "[5,15]", "[1,5]", "[1,5]", "-", "1/[0.1,10]",
                                  "3", "10", "3", "3", "0", "1",
                                  "[1,5]", "[0.5,1.5]", "[1,5]", "[1,5]", "-", "1/[0.1,10]",
                                  "3", "1", "3", "3", paste(expression(pi), sep = ""), "1",
                                  "[1,5]", "[0.5,1.5]", "[1,5]", "[0.05,0.25]", "-", "1/[0.1,10]",
                                  "3", "1", "3", "0.15", "0", "1"), nrow = 6, byrow = F)))

apa_table(cbind(rep(c("Interval", "Default"), 4), sim.pars.resp),
          col.names = c("", "Shape", "Scale", "Shape", "Scale", "Mean", "Concentration"),
          col_spanners = list(Velocity = c(2, 3), Acceleration = c(4, 5), "Rel. angle" = c(6, 7)),
          stub_indents = list("State 1" = c(1, 2), "State 2" = c(3, 4), "State 3" = c(5, 6), "State 4" = c(7, 8)),
          caption = "(ref:sim-parameters-resp)",
          note = "(ref:sim-parameters-resp-note)")

```

```{r load sim results and helper functions, include=FALSE}

# Load simulation results

for (part in 1:4) {
  
  load(file = paste(here("simulation/part"), part, ".Rdata", sep = ""))
  
}


# Create functions to apply and invert mlogit link function (from depmixS4, Visser & Speekenbrink, 2019)

linkfun <- function(p, base) {
  lfun <- function(p, base) {
    p <- p/sum(p)
    beta <- numeric(length(p))
    if (any(p == 1)) 
      beta[which(p == 1)] = Inf
    else beta[-base] <- log(p[-base]/p[base])
    return(beta)
  }
  if (is.matrix(p)) {
    beta <- t(apply(p, 1, lfun, base = base))
  }
  else {
    beta <- lfun(p, base)
  }
  return(beta)
}

linkinv <- function(eta,base) {
  linv <- function(eta,base) {
    pp <- numeric(length(eta))
    if(any(is.infinite(eta)) || any(eta > log(.Machine$double.xmax)) || any(eta < log(.Machine$double.xmin))) {
      pp[which(is.infinite(eta))] <- 1
      pp[which(eta > log(.Machine$double.xmax))] <- 1 # change this to something better!
    } else {
      expb <- exp(eta)
      sumb <- sum(expb)
      pp[base] <- 1/sumb
      pp[-base] <- expb[-base]/sumb
    }
    return(pp)
  }
  if(is.matrix(eta)) {
    if(ncol(eta)==1) {
      pp <- as.matrix(apply(eta,1,linv,base=base)) # fixes problem with column matrix eta
    } else pp <- t(apply(eta,1,linv,base=base)) 	
  } else {
    pp <- linv(eta,base)
  }
  return(pp)
}


# Create function to transform parameters to normal scale

backtrans <- function(x) {
  
  out <- x
  
  nms <- names(x)
  
  out[str_detect(nms, "(Intercept)")] <- as.vector(apply(matrix(x[str_detect(nms, "(Intercept)")],
                                                        ncol = sqrt(length(x[str_detect(nms, "(Intercept)")])),
                                                        byrow = T), 1, linkinv, base = 1))
  
  out[nms %in% c("shape", "scale", "kappa")] <- exp(x[nms %in% c("shape", "scale", "kappa")])
  
  return(out)
}

```

```{r calculate RMdSPD, include=FALSE}

rmsd <- list()

for (part in 1:4) {
  
  rmsd[[part]] <- lapply(get(paste("estimates.", part, sep = "")), function(x) {
    lapply(x, function(y) {
      sqerr <- lapply(y, function(z) {
        
        err <- try(((backtrans(z$pars.est) - backtrans(z$pars.true))/
                      ifelse(backtrans(z$pars.true) == 0, 2*pi, backtrans(z$pars.true)))^2)
        
        if(is.numeric(err)) {
          
          out <- err
          
        } else {
          
          out <- rep(NA, length(z$pars.true))
          
        }
        
        names(out) <- names(z$pars.true)
        
        return(out)
      })
      
      rows <- length(sqerr)

      nms <- names(sqerr[[1]])

      pars <- matrix(unlist(sqerr), nrow = rows, byrow = T)
      
      msqerr <- apply(pars, 2, median, na.rm = T)
      
      names(msqerr) <- nms
      
      rmsqerr <- sqrt(msqerr)
      
      return(rmsqerr)
    })
  })
}


# Display RMdSPD in data frame

rmsd.data <- lapply(rmsd, function(x) lapply(x, as.data.frame))
rmsd.data <- lapply(rmsd.data, function(x) lapply(x, function(y) {as.data.frame(t(as.matrix(y)))}))
rmsd.data <- lapply(rmsd.data, function(x) lapply(1:length(x), function(y, data) {
  names(data[[y]]) <- names(rmsd[[1]][[y]][[1]])
  return(data[[y]])}, data = x))


# Create plots for part 1

plots.rmsd.1 <- lapply(rmsd.data[[1]], function(x) {
  
  names.pars.varied <- list(bquote(a["i=j"]), bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                            bquote(beta["acc"]), bquote(kappa))
  
  if(ncol(x) == 18) {
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]))
    
  } else if(ncol(x) == 30) {
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]))
    
  } else {
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]), bquote(a["i4"]))
    
  }
  
  names.pars.est <- append(trnames, list(bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                         bquote(beta["acc"]), bquote(mu), bquote(kappa)))
  
  x <- as_tibble(x, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(par.varied = c(0, 1, 2, 3, 4, rep(c(1, 2, 3, 4, 5), (nrow(x) %/% 5)-1)),
           state.varied = c(1, rep(1, 4), rep(2:((nrow(x) %/% 5)), each = 5))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "RMdSPD", names_repair = "unique") %>%
    mutate(state.est = rep(c(rep(1:max(state.varied), max(state.varied)+1),
                         rep(1:max(state.varied), each = 6)), nrow(x)),
           par.est = rep(c(rep(1, max(state.varied)), 
                           rep(2:(max(state.varied)+1), each = max(state.varied)), 
                           rep((max(state.varied)+2):(max(state.varied)+7), max(state.varied))), nrow(x))) %>%
    mutate_at(vars(par.varied, state.varied, par.est, state.est), as.factor) %>%
    mutate(state.varied = fct_recode(state.varied, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"),
           state.est = fct_recode(state.est, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"))

  p <- ggplot(data = data.long, aes(x = par.varied, y = par.est, fill = RMdSPD)) +
    geom_tile() + facet_grid(cols = vars(state.varied), rows = vars(state.est)) +
    scale_x_discrete(name = "Varied parameter", labels = names.pars.varied) +
    scale_y_discrete(name = "Estimated parameter", labels = names.pars.est) +
    scale_fill_distiller(breaks = c(0, 0.1, 0.5, 1), palette = "Spectral")
  
  return(p)
})


# Create plots for parts 2,3, and 4

plots.rmsd.234 <- lapply(2:4, function(y, data) lapply(data[[y]], function(x) {
  
  if(ncol(x) == 18) {
    
    k <- 2
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]))
    
  } else if(ncol(x) == 30) {
    
    k <- 3
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]))
    
  } else {
    
    k <-4
    
    trnames <- list(bquote(rho["i"]), bquote(a["i1"]), bquote(a["i2"]), bquote(a["i3"]), bquote(a["i4"]))
    
  }
  
  names.pars.est <- append(trnames, list(bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                                         bquote(beta["acc"]), bquote(mu), bquote(kappa)))
  
  x <- as_tibble(x, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(cond = 1:3) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "RMdSPD", names_repair = "unique") %>%
    mutate(state.est = rep(c(rep(1:k, k+1),
                             rep(1:k, each = 6)), nrow(x)),
           par.est = rep(c(rep(1, k), 
                           rep(2:(k+1), each = k), 
                           rep((k+2):(k+7), k)), nrow(x))) %>%
    mutate_at(vars(cond, par.est, state.est), as.factor) %>%
    mutate(state.est = fct_recode(state.est, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"))
  
  p <- ggplot(data = data.long, aes(x = par.est, y = RMdSPD, color = cond)) +
    geom_point(position = position_dodge(0.25)) + facet_wrap(vars(state.est)) +
    scale_x_discrete(name = "Estimated parameter", labels = names.pars.est) +
    scale_y_continuous(breaks = c(0.1, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5)) +
    geom_hline(yintercept = 0.1, linetype = "dashed") +
    geom_hline(yintercept = 0.5, linetype = "dashed")
  
  if(y == 2) {
    names.cond <- c("500", "2500", "10000")
    label.cond <- "N"
  } else if (y == 3) {
    names.cond <- c("1", "2", "3")
    label.cond <- bquote(tau["start"])
  } else {
    names.cond <- c("1", "3", "5")
    label.cond <- "Number of\nintervals"
  }
  
  p <- p + scale_color_discrete(name = label.cond, labels = names.cond)
  
  return(p)
}), data = rmsd.data)

```

```{r calculate linear regressions, include=FALSE}

# Calculate regression weights for transition probabilities

regw.tr <- list()

regw.tr <- lapply(get("estimates.1"), function(x) {
  
  varpar <- lapply(x[1], function(y) {
    
    lapply(y, function(z) {
      
      nms <- names(z$pars.true)
      
      pars.tr <- logical(length(z$pars.true))
      
      pars.tr[str_detect(nms, "(Intercept)")] <- T
      
      intpar <- try(cbind(backtrans(z$pars.true[pars.tr]), backtrans(z$pars.est[pars.tr])))
      
      if(is.numeric(intpar)) {
        out <- intpar
      } else {
        out <- matrix(NA, nrow = length(z$pars.true[pars.tr]), ncol = 2)
      }
      
      out <- apply(out, 1, list)
      
      return(out)
    })
  })
  
  df <- list()
  
  for (i in 1:length(varpar[[1]][[1]])) {
    
    df[[i]] <- lapply(varpar, function(y, index) {
      
      lapply(y, function(z) {z[[index]]})
      
    }, index = i)
  }
  
  df <- lapply(df, as.data.frame)

  df <- lapply(df, function(z) {
    
    out <- as.data.frame(t(as.matrix(z)))
    
    names(out) <- c("true", "est")
    
    return(out)
  })
})


# Calculate linear regression weights for response parameters

regw.resp <- list()
  
regw.resp <- lapply(get("estimates.1"), function(x) {
  
  index <- 1:length(x[-1])
  
  varpar <- lapply(index, function(i, y) {
    
    lapply(y[[i]], function(z) {
      
      nms <- names(z$pars.true)
      
      #pars.tr <- logical(length(z$pars.true))
      pars.resp <- logical(length(z$pars.true))
      
      #pars.tr[str_detect(nms, "(Intercept)")] <- T
      pars.resp[nms %in% c("shape", "scale", "kappa")] <- T
      
      intpar <- try(c(backtrans(z$pars.true[pars.resp][i]), backtrans(z$pars.est[pars.resp][i])))
      
      if(is.numeric(intpar)) {
        return(intpar)
      } else {
        return(rep(NA, 2))
      }
    })
  }, y = x[-1])
  
  df <- lapply(varpar, as.data.frame)
  df <- lapply(df, function(z) {

    out <- as.data.frame(t(as.matrix(z)))

    names(out) <- c("true", "est")

    return(out)
  })
})


# Plot linear regressions for transition probabilities

D <- 100

regw.tr.data <- lapply(regw.tr, function(x) lapply(x, function(y) rbind(y)))
regw.tr.data <- lapply(regw.tr.data, function(x) reduce(x, rbind))

plots.lm.tr <- lapply(1:length(regw.tr.data), function(x, y) {
  
  data <- y[[x]] %>% 
    mutate(par = rep(1:(x+1)^2, each = D),
           From = rep(rep(1:(x+1), each = D), x+1),
           To = rep(1:(x+1), each = D*(x+1)))
  
  p <- ggplot(data, aes(x = true, y = est)) + 
    facet_grid(rows = vars(From), cols = vars(To), labeller = label_both) +
    geom_point() + geom_smooth(method = "lm") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    scale_x_continuous(name = "True transition probability") +
    scale_y_continuous(name = "Estimated transition probability")
  
  return(p)
}, y = regw.tr.data)


# Plot linear regressions for response parameters

regw.resp.data <- lapply(regw.resp, function(x) lapply(x, function(y) rbind(y)))
regw.resp.data <- lapply(regw.resp.data, function(x) reduce(x, rbind))

parnames <- list(bquote(alpha["vel;1"]), bquote(beta["vel;1"]), bquote(alpha["acc;1"]), bquote(beta["acc;1"]),
                 bquote(alpha["vel;2"]), bquote(beta["vel;2"]), bquote(alpha["acc;2"]), bquote(beta["acc;2"]), bquote(kappa["2"]),
                 bquote(alpha["vel;3"]), bquote(beta["vel;3"]), bquote(alpha["acc;3"]), bquote(beta["acc;3"]), bquote(kappa["3"]),
                 bquote(alpha["vel;4"]), bquote(beta["vel;4"]), bquote(alpha["acc;4"]), bquote(beta["acc;4"]), bquote(kappa["4"]))

plots.lm.resp <- lapply(regw.resp.data, function(x) {
  
  npar <- nrow(x) %/% D
  
  data <- x %>% 
    mutate(par = rep(1:npar, each = D),
           type = rep(1, 2, 1, 2))
  
  p <- ggplot(data, aes(x = true, y = est)) + 
    facet_wrap(vars(par), scales = "free", labeller = label_bquote(.(parnames[[par]]))) +
    geom_point() + geom_smooth(method = "lm") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    scale_x_continuous(name = "True response parameter") +
    scale_y_continuous(name = "Estimated response parameter")
  
  return(p)
})

```

```{r calculate accuracy, include=FALSE}

# Summarise accuracy

acc.data <- list()

for (part in 1:4) {
  
  acc.data[[part]] <- lapply(get(paste("estimates.", part, sep = "")), function(x) {
    out <- lapply(x, function(y) { 
      out <- lapply(y, function(z) {
        
        if(is.numeric(z$accuracy)) {
          acc <- z$accuracy
        } else {
          acc <- NA
        }
        
        return(acc)
      })
      
      return(as.vector(reduce(out, cbind)))
    })
    
    return(as.data.frame(t(reduce(out, cbind))))
  })
}

# Plot accuracy part 1

plots.acc.1 <- lapply(acc.data[[1]], function(x) {
  
  names.pars.varied <- list(bquote(a["i=j"]), bquote(alpha["vel"]), bquote(beta["vel"]), bquote(alpha["acc"]),
                            bquote(beta["acc"]), bquote(kappa))
  
  x <- as_tibble(x, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(par.varied = c(0, 1, 2, 3, 4, rep(c(1, 2, 3, 4, 5), (nrow(x) %/% 5)-1)),
           state.varied = c(1, rep(1, 4), rep(2:((nrow(x) %/% 5)), each = 5))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy") %>%
    mutate_at(vars(par.varied, state.varied), as.factor) %>%
    mutate(state.varied = fct_recode(state.varied, "State 1" = "1", "State 2" = "2", "State 3" = "3", "State 4" = "4"))
  
  p <- ggplot(data = data.long, aes(x = par.varied, y = accuracy)) +
    geom_boxplot(outlier.shape = 4) + facet_grid(cols = vars(state.varied)) +
    scale_x_discrete(name = "Varied parameter", labels = names.pars.varied) + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  return(p)
})


# Plot accuracy parts 2, 3, and 4

acc.data.234 <- lapply(acc.data[2:4], function(x) reduce(x, rbind))

plots.acc.234 <- lapply(1:3, function(y, data) {
  
  x <- as_tibble(data[[y]], .name_repair = "unique")
  
  data.long <- x %>%
    mutate(cond = as.factor(rep(1:3, 3)),
           k = as.factor(rep(2:4, each = 3))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy")
  
  p <- ggplot(data = data.long, aes(x = k, y = accuracy, color = cond)) +
    geom_boxplot(outlier.shape = 4) + 
    scale_x_discrete(name = "k (number of states)")  + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  if(y == 1) {
    names.cond <- c("500", "2500", "10000")
    label.cond <- "N"
  } else if (y == 2) {
    names.cond <- c("1", "2", "3")
    label.cond <- bquote(tau["start"])
  } else {
    names.cond <- c("1", "3", "5")
    label.cond <- "Number of\nintervals"
  }
  
  p <- p + scale_color_discrete(name = label.cond, labels = names.cond)
  
  return(p)
}, data = acc.data.234)


# Plot accuracy over interval parts 2 and 4

plots.acc.int.24 <- lapply(c(1, 3), function(y, data) {
  
  x <- as_tibble(data[[y]], .name_repair = "unique")
  
  if(y == 1) {
  
    int <- rep(seq(1, 5, length.out = D), 9) 
  
  } else {
    
    int <- rep(floor(seq(1, 200, length.out = D)), 9)
    
  }
  
  data.long <- x %>%
    mutate(cond = as.factor(rep(1:3, 3)),
           k = as.factor(rep(2:4, each = 3))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy") %>%
    mutate(int = int)
  
  p <- ggplot(data = data.long, aes(x = int, y = accuracy, color = cond)) +
    facet_grid(cols = vars(k), labeller = partial(label_both, sep = " = ")) + 
    geom_point()
  
  if(y == 1) {
    names.cond <- c("500", "2500", "10000")
    label.cond <- "N"
    x.name <- bquote(tau["noise"])
  } else {
    names.cond <- c("1", "3", "5")
    label.cond <- "Number of\nintervals"
    x.name <- "Interval length (in samples)"
  }
  
  p <- p + scale_color_discrete(name = label.cond, labels = names.cond) +
    scale_x_continuous(name = x.name) + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  return(p)
}, data = acc.data.234)


# Exploratory analysis for label switching

load(here("simulation/part3_expl.Rdata"))

labsw <- lapply(1:length(estimates.3), function(x) {
  out <- lapply(estimates.3[[x]], function(y) {
    out <- lapply(y, function(z) {
      
      kappa <- try(cohen.kappa(z$states)$kappa)
      
      states <- z$states
      
      if(x == 2 & is.numeric(kappa) & kappa < 0.95) {
          
        states$y <- ifelse(z$states$y == 3, 2, ifelse(z$states$y == 2, 3, z$states$y))
          
        kappa <- cohen.kappa(states)$kappa
        
        if(is.numeric(kappa) & kappa < 0.95) {
          
          states$y <- ifelse(z$states$y == 3, 1, ifelse(z$states$y == 1, 3, z$states$y))
          
          kappa <- cohen.kappa(states)$kappa
        }
        
        if(is.numeric(kappa) & kappa < 0.95) {
          
          states$y <- ifelse(z$states$y == 3, 2, ifelse(z$states$y == 1, 3, 1))
          
          kappa <- cohen.kappa(states)$kappa
        }
      }
      
      if(x == 3 & is.numeric(kappa) & kappa < 0.5) {
        
        states$y <- ifelse(z$states$y == 3, 2, ifelse(z$states$y == 2, 3, z$states$y))
        
        kappa <- cohen.kappa(states)$kappa
        
        if(is.numeric(kappa) & kappa < 0.5) {

          states$y <- ifelse(z$states$y == 3, 4, ifelse(z$states$y == 4, 3, z$states$y))

          kappa <- cohen.kappa(states)$kappa
        }

        if(is.numeric(kappa) & kappa < 0.5) {

          states$y <- ifelse(z$states$y == 3, 1, ifelse(z$states$y == 1, 3, z$states$y))

          kappa <- cohen.kappa(states)$kappa
        }
        
        if(is.numeric(kappa) & kappa < 0.5) {
          
          kappa <- cohen.kappa(z$states)$kappa
        }
      }
      
      if(is.numeric(kappa)) {return(kappa)
        } else {return(NA)}
    })
    
    return(as.vector(reduce(out, cbind)))
  })
  
  return(t(reduce(out, cbind)))
})

labsw.df <- as.data.frame(reduce(labsw, rbind))


# Plot exploratory analysis results

plots.acc.expl <- lapply(1, function(y, data) {
  
  x <- as_tibble(data, .name_repair = "unique")
  
  data.long <- x %>%
    mutate(cond = as.factor(rep(1:3, 3)),
           k = as.factor(rep(2:4, each = 3))) %>%
    pivot_longer(names(x), names_to = "par.est", values_to = "accuracy")
  
  p <- ggplot(data = data.long, aes(x = k, y = accuracy, color = cond)) +
    geom_boxplot(outlier.shape = 4) + 
    scale_x_discrete(name = "k (number of states)")  + 
    scale_y_continuous(name = "Cohen's kappa", breaks = c(-1, -0.33, 0, 0.25, 0.5, 0.75, 1))
  
  names.cond <- c("1", "2", "3")
  label.cond <- bquote(tau["start"])
  
  p <- p + scale_color_discrete(name = label.cond, labels = names.cond)
  
  return(p)
}, data = labsw.df)


```

## Results
### Parameter Variation
In the first part of the simulation, I examined how varying the parameters in the HMM affects the deviation of estimated parameters and accuracy of estimated state sequences. Figure \@ref(fig:plot-rmdspd-1-2) displays the RMdSPD between true and estimated parameters depending on which parameter has been manipulated in the HMM. The RMdSPD was below 0.1 for all estimated and manipulated parameters, indicating good recovery [^2]. The regressions between manipulated true and estimated parameters are shown in Figures \@ref(fig:plot-regtr-1-2) and \@ref(fig:plot-regresp-1-2). With one outlier at parameter $\alpha_{acc;1}$, the estimated parameters matched the true parameters very closely. Thus, parameter change was captured well and the estimation almost unbiased. Considering accuracy, Figure \@ref(fig:plot-acc-1-2) displays Cohen's kappa between true and estimated hidden state sequences. With two exceptions, kappa values were almost one, suggesting nearly perfect agreement.

[^2]: Note that the initial state probability $\rho_i$ has $\textrm{RMdSPD} = 1$. Since the HMM only simulated one state sequence, this parameter is always either zero or one (leading to $\textrm{RMdSPD} = 1$). Therefore, I decided not to include it in the analysis.

(ref:plot-rmdspd-1-2) RMdSPD between true and estimated parameters of the two-state HMM in part one of the simulation. Labels on the x-axis indicate which true parameters have been manipulated and labels on the y-axis show for which estimated parameter the RMdSPD is displayed. Top facet labels specify in which state the parameters have been varied and right facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-1-2, fig.cap="(ref:plot-rmdspd-1-2)"}

print(plots.rmsd.1[[1]])

```

(ref:plot-regtr-1-2) Regression lines between true and estimated transition probabilities for the two-state HMM in part one. Top facet labels show to and right facet labels show from which state the HMM is moving. Dashed lines refer to perfect recovery.

```{r plot-regtr-1-2, fig.cap="(ref:plot-regtr-1-2)"}

print(plots.lm.tr[[1]])

```

(ref:plot-regresp-1-2) Regression lines between true and estimated response parameters of the two-state HMM in part one. Top facet labels indicate response parameters. Dashed lines refer to perfect recovery.

```{r plot-regresp-1-2, fig.cap="(ref:plot-regresp-1-2)"}

print(plots.lm.resp[[1]] + theme(axis.text = element_text(size = 10)))

```

(ref:plot-acc-1-2) Boxplots displaying Cohen's kappa depending on which parameter of the two-state HMM has been manipulated in part one. Top facet labels indicate for which state parameters have been manipulated. Black solid lines symbolize medians. Crosses represent outliers (distance to first/third quartile higher than 1.5 times the inter-quartile range [IQR]).

```{r plot-acc-1-2, fig.cap="(ref:plot-acc-1-2)"}

print(plots.acc.1[[1]])

```

For the HMM with three states, the RMdSPD is shown in Figure \@ref(fig:plot-rmdspd-1-3). When response parameters (other than $a_{i=j}$) were manipulated, the RMdSPDs for $a_{12}$ and $a_{31}$ were consistently between 0.1 and 0.5. Varying $\kappa$ in states two and three led to RMdSPDs between 0.1 and 0.5 in the respective states, which we interpreted as moderate recovery. Otherwise, RMdSPDs were consistently lower than 0.1, indicating good recovery. Inspecting the regressions between manipulated true and estimated parameters (see Figures \@ref(fig:plot-regtr-1-3) and \@ref(fig:plot-regresp-1-3)) revealed strong and unbiased linear relationships (intercepts close to zero and slopes close to one). In contrast to the two-state HMM, larger deviations and more outliers were observed. Cohen's kappa values are presented in Figure \@ref(fig:plot-acc-1-3). For most estimated models, the kappa values between true and estimated state sequences were above 0.95, meaning almost perfect agreement. However, for some models, we observed kappas clustered around zero or -0.33, which suggests that state labels were switched.  

(ref:plot-rmdspd-1-3) RMdSPD between true and estimated parameters of the three-state HMM in part one of the simulation. Labels on the x-axis indicate which true parameters have been manipulated and labels on the y-axis show for which estimated parameter the RMdSPD is displayed. Top facet labels specify in which state the parameters have been varied and right facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-1-3, fig.cap="(ref:plot-rmdspd-1-3)"}

print(plots.rmsd.1[[2]]+ theme(axis.text = element_text(size = 8)))

```

(ref:plot-regtr-1-3) Regression lines between true and estimated transition probabilities for the three-state HMM in part one. Top facet labels show to and right facet labels show from which state the HMM is moving. Dashed lines refer to perfect recovery.

```{r plot-regtr-1-3, fig.cap="(ref:plot-regtr-1-3)"}

print(plots.lm.tr[[2]])

```

(ref:plot-regresp-1-3) Regression lines between true and estimated response parameters of the three-state HMM in part one. Top facet labels indicate response parameters. Dashed lines refer to perfect recovery.

```{r plot-regresp-1-3, fig.cap="(ref:plot-regresp-1-3)", fig.height=7}

print(plots.lm.resp[[2]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-acc-1-3) Boxplots displaying Cohen's kappa depending on which parameter of the three-state HMM has been manipulated in part one. Top facet labels indicate for which state parameters have been manipulated. Black solid lines symbolize medians and hinges the first and third quartile. Whiskers range from hinges to lowest/highest value within 1.5 times the IQR. Crosses represent outliers.

```{r plot-acc-1-3, fig.cap="(ref:plot-acc-1-3)"}

print(plots.acc.1[[2]])

```

The RMdSPDs for the four-state HMM is shown in Figure \@ref(fig:plot-rmdspd-1-4). For estimated transition probabilities and $\alpha_{vel}$ and $\beta_{vel}$ parameters in states one and four, RMdSPDs were between 0.1 and 0.5, suggesting moderate recovery. Estimated kappa parameters in state four were also often moderately recovered when parameters in states two, three, and four were varied. Otherwise, RMdSPDs were below 0.1, indicating good recovery. Looking at the regressions between true and estimated parameters, Figures \@ref(fig:plot-regtr-1-4) and \@ref(fig:plot-regresp-1-4) illustrate strong and unbiased relationships. However, there were larger deviations and more outliers than in the previous models, especially for states one and four. Cohen's kappa ranged majorly between 0.6 and 0.9, meaning moderate to almost perfect agreement between true and estimated state sequences (see Figure \@ref(fig:plot-acc-1-4)). Here, some kappa values clustered around 0.25 and zero, which, again, can be interpreted as the result of label switching.    
Overall, simulations in part one demonstrated that the HMM recovered parameters very well when they were manipulated. Deviations from true parameters were mostly small. In the four-state model, estimated transition probabilities for state one and four deviated moderately. Moreover, the HMM estimated state sequences very accurately with decreasing accuracy for the four-state model.

(ref:plot-rmdspd-1-4) RMdSPD between true and estimated parameters of the four-state HMM in part one of the simulation. Labels on the x-axis indicate which true parameters have been manipulated and labels on the y-axis show for which estimated parameter the RMdSPD is displayed. Top facet labels specify in which state the parameters have been varied and right facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-1-4, fig.cap="(ref:plot-rmdspd-1-4)"}

print(plots.rmsd.1[[3]] + theme(axis.text = element_text(size = 6)))

```

(ref:plot-regtr-1-4) Regression lines between true and estimated transition probabilities for the four-state HMM in part one. Top facet labels show to and right facet labels show from which state the HMM is moving. Dashed lines refer to perfect recovery.

```{r plot-regtr-1-4, fig.cap="(ref:plot-regtr-1-4)", fig.height=8}

print(plots.lm.tr[[3]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-regresp-1-4) Regression lines between true and estimated response parameters of the four-state HMM in part one. Top facet labels indicate response parameters. Dashed lines refer to perfect recovery.

```{r plot-regresp-1-4, fig.cap="(ref:plot-regresp-1-4)", fig.height=8}

print(plots.lm.resp[[3]] + theme(axis.text = element_text(size = 6)))

```

(ref:plot-acc-1-4) Boxplots displaying Cohen's kappa depending on which parameter of the four-state HMM has been manipulated in part one. Top facet labels indicate for which state parameters have been manipulated. Black solid lines symbolize medians and hinges the first and third quartile. Whiskers range from hinges to lowest/highest value within 1.5 times the IQR. Crosses represent outliers.

```{r plot-acc-1-4, fig.cap="(ref:plot-acc-1-4)"}

print(plots.acc.1[[3]] + theme(axis.text = element_text(size = 8)))

```

### Sample Size and Noise Variation
In the second part, we varied the sample size of the HMM and added noise to the generated data. For the two-state HMM, the RMdSPDs were above 0.5 for $\beta_{vel}$ and $\beta_{acc}$ in both states (see Figure \@ref(fig:plot-rmdspd-2-2)), suggesting bad recovery. The other estimated parameters showed RMdSPDs close to or below 0.1, which means they were recovered well. Increasing the sample size seemed to improve RMdSPDs for most parameters slightly. For $\beta_{vel}$ and $\beta_{acc}$ in both states, models with 2500 samples had the lowest RMdSPDs. Accuracy measured by Cohen's kappa was almost perfect with kappa values very close to one (see Figure \@ref(fig:plot-acc-2), left plot).

(ref:plot-rmdspd-2-2) RMdSPD between true and estimated parameters of the two-state HMM in part two of the simulation. Colours indicate different sizes of generated data. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-2-2, fig.cap="(ref:plot-rmdspd-2-2)"}

print(plots.rmsd.234[[1]][[1]] + theme(axis.text = element_text(size = 8)))

```

(ref:plot-acc-2) Cohen's kappa depending on the variation of noise added to the data generated by the HMM. Colours indicate different sizes of generated data. Top facet labels indicate the number of states in the HMM.

```{r plot-acc-2, fig.cap="(ref:plot-acc-2)"}

print(plots.acc.int.24[[1]])

```

Regarding three states, the RMdSPDs for the $\beta_{vel}$ and $\beta_{acc}$ were above 0.5 in all three states (see Figure \@ref(fig:plot-rmdspd-2-3)), indicating bad recovery. Again, the other estimated parameters were below or close to 0.1, only $a_{12}$ and $a_{31}$ with 500 samples were closer to 0.5. For most parameters across all three states, models with higher sample sizes had lower RMdSPDs. The state recovery of the estimated models was almost perfect with most kappa values above 0.95 (see Figure \@ref(fig:plot-acc-2), middle plot). Several outliers clustered around kappas of zero and -0.33, signaling label switching.

(ref:plot-rmdspd-2-3) RMdSPD between true and estimated parameters of the three-state HMM in part two of the simulation. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-2-3, fig.cap="(ref:plot-rmdspd-2-3)"}

print(plots.rmsd.234[[1]][[2]] + theme(axis.text = element_text(size = 6)))

```

RMdSPDs regarding the four-state HMM are displayed in Figure \@ref(fig:plot-rmdspd-2-4). For states one and four, values for most parameters (including all transition probabilities) were above 0.5, suggesting bad recovery. Similarly, $\beta_{vel}$ and $\beta_{acc}$ in states two and three showed bad recovery. For states two and three, higher samples sizes showed slightly lower RMdSPDs. As in the previous part, most Cohen's kappa values ranged between 0.6 and 0.9, meaning substantial to almost perfect agreement between true and estimated states (Figure \@ref(fig:plot-acc-2), right plot). Multiple kappa values clustered around 0.25 or zero, which can be explained by label switching.   
In general, the HMM recovered parameters well despite noise being added to the data. However, in the four-state model, the parameter recovery for states one and four substantially decreased. In the three- and four-state models, scale parameters of gamma distributions were badly recovered. Increasing the sample size in the HMM slightly improved the recovery of most parameters. The state recovery of the model was slightly lowered when more states were included, but it was neither affected by the noise variability $\tau_{noise}$ nor the sample size.

(ref:plot-rmdspd-2-4) RMdSPD between true and estimated parameters of the three-state HMM in part two of the simulation. Labels on the x-axis indicate for which estimated parameter the RMdSPD is displayed. Top facet labels denote to which state estimated parameters belong.

```{r plot-rmdspd-2-4, fig.cap="(ref:plot-rmdspd-2-4)"}

print(plots.rmsd.234[[1]][[3]] + theme(axis.text = element_text(size = 8)))

```

# Validation Study
To validate gazeHMM, we applied the algortihm on two benchmark data sets. As starting values, we used $\rho=1/k$ for the initial state model as well as $a_{i=j}=0.9$ and $a_{i\neq j}=0.1/k$ for the transition model. The values for the response model are displayed in Table \@ref(tab:tab-starting-values). In contrast to the simulation study, generating random starting values often led to bad model fits and label switching between states. To improve the fitting of the gamma distributions, velocity and acceleration signals were scaled down by factor 100 [^7] (so were the starting values for their gamma distributions).

[^7]: Scaling down by factor 100 differs from the simulation study (scaling down by 10). The algorithm allows the user to manually specify this factor, and in this case, factor 100 led to better model fits than factor 10.

(ref:tab-starting-values) Starting Values for the Response Model for Fitting gazeHMM to Benchmark Data
(ref:tab-starting-values-note) Starting values for velocity and acceleration signals are shown before scaling down by factor 100. Values for event 5 were chosen so that they approximately match plausible distributions for microsaccades.

```{r tab-starting-values}

tab.start <- as.data.frame(matrix(c(10, 10, 10, 10, NA, NA,
                      50, 50, 50, 50, 0, 10,
                      50, 50, 50, 50, pi, 10,
                      20, 20, 20, 20, 0, 10,
                      20, 20, 50, 50, 0, 10), nrow = 5, byrow = T))

colnames(tab.start) <- c("Shape", "Scale", "Shape", "Scale", "Mean", "Concentration")
rownames(tab.start) <- c("Fixation", "Saccade", "PSO", "Pursuit", "Event 5")

apa_table(as.data.frame(tab.start),
          col_spanners = list(Velocity = c(2, 3), Acceleration = c(4, 5), "Rel. angle" = c(6, 7)),
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-starting-values)",
          note = "(ref:tab-starting-values-note)")

```

## Data Sets
We chose two data sets for validation: One was published in a study by @Andersson2017 and has been widely used for validation purposes [e.g., @Pekkanen2017]. It contains eye-tracking data from three conditions: A static condition, where subjects had to look freely at images, and two dynamic conditions, where they had to follow a constantly moving dot or objects in a video. The data was sampled with 500 Hz and two human coders (MN and RA) labeled them as belonging to six different eye movement events: Fixation, saccade, PSO, smooth pursuit, blink, or other. @Andersson2017 used the data to compare 10 different classification algorithms. We used their results to compare these 10 algorithms and the two human coders with gazeHMM.  
The second data set was published in @Ehinger2019 and has to our knowledge not yet been used for validation. Here, we only use tasks four and five out of 10 tasks because these are qualitatively different to the first data set. In task four, subjects were instructed to fixate a central target for 20 s. Task 5 was set up similarly, but subjects had to blink when they heared one out of seven beeps (with a beep duration of 100 ms and 1.5 s intervals in between). Eye movements were recorded with 250-500 Hz. For simplicity, we only used data obtained by the EyeLink (SR Research Ltd., Ontario, Canada) eye-tracker.

## Data Analysis
A successful validation of gazeHMM was determined by using two approaches: First, we applied gazeHMM with generative models containing 1-5 states to both data sets. The fits of the generative models were compared using Schwarz weights [@Wagenmakers2004], a weighted version of the BIC. They can be interpreted as the probability of a model having generated the data compared to the competing models. For the static condition in the @Andersson2017 data set, we expected the generative model with three states (fixation, saccade, and PSO), and for the dynamic conditions the model with four states (incl. smooth pursuit) to display the highest Schwarz weight. Regarding the @Ehinger2019 data set, we assumed that the one-state model (only fixation) would show the highest weights for both tasks.    
The algorithm was applied separately to every subject, condition, or task. For the @Andersson2017 data set, all generative models were successfully fitted, whereas for the @Ehinger2019 data set, it was only 780 out of 900 models (87%, 60 models per task).  
Second, we compared gazeHMM to other algorithms and human coders. We applied our algorithm with a three-state generative model to the static condition in the @Andersson2017 data set, and with a four-state model to the dynamic conditions. For comparison criteria, we followed @Andersson2017: We calculated the RMSD of event durations and counts between all algorithms and the two human coders. Cohen's kappa was calculated for each event as the binary agreement between gazeHMM and the human coders. Lastly, the overall disagreement indicated which samples were classified differently by gazeHMM across all events.

## Results
### Model Comparison

```{r load validation results, include=FALSE}

load(here("validation/Andersson2017_fitted.Rdata"))
load(here("validation/Andersson2017_raw.Rdata"))

```

```{r calculate Schwarz weights, include=FALSE}

# Compute Schwarz weights

schwarz.weights <- function(bic, na.rm = T) {
  
  d.bic <- bic - min(bic, na.rm = na.rm) # eq 2
  
  exp(-0.5 * d.bic)/sum(exp(-0.5 * d.bic), na.rm = na.rm) # eq 4
  
}

A2017.bic <- lapply(A2017.fit, function(stim) {
  out <- lapply(stim, function(subj) {
    out <- lapply(1:length(subj), function(mod) {
      
      if(mod == 1) {
        
        bic <- try(-2*subj[[mod]][["LL"]] + 6*log(subj[[mod]][["N"]]))
        
      } else {
        
        bic <- try(BIC(subj[[mod]]$model))
        
      }
      
      return(ifelse(is.numeric(bic), bic, NA))
    })
    
    return(schwarz.weights(unlist(out)))
  })
  
  df <- as.data.frame(reduce(out, rbind))
  
  names(df) <- paste("model_", 1:length(stim[[1]]), sep = "")
  
  df$subject <- 1:nrow(df)
  
  return(df)
}) %>% reduce(rbind) %>% mutate(condition = rep(c("Moving dots", "Image", "Video"), c(11, 14, 9)))


# Create Schwarz weight plots

bic.plot <- A2017.bic %>%
  pivot_longer(names(A2017.bic)[1:5], names_to = "model", values_to = "weight") %>%
  mutate_at(c("subject", "model", "condition"), as.factor) %>%
  ggplot(aes(x = model, y = subject)) + geom_tile(aes(fill = weight)) +
  facet_grid(cols = vars(condition)) + 
  scale_x_discrete(name = "Number of states", labels = as.character(1:5)) +
  scale_y_discrete(name = "Subject") +
  scale_fill_distiller(name = "Schwarz\nweight", breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), palette = "Spectral")

```

Examining the Schwarz weights displayed in \@ref(fig:plot-schwarz-weights), we observed that the five-state generative model showed the highest weights in all three conditions. Only in the moving dots condition, two subjects each displayed the highest weights for one- and three-state models. In sum, we concluded that the five-state generative model has most likely generated the @Andersson2017 data, opposing our expectations. Because the @Ehinger2019 data set showed a similar pattern, we included the results for this data in the supplementary material.  
A recent model recovery study showed that the BIC tended to prefer overly complex HMMs when they were misspecified [e.g., the conditional independence assumption was violated; @Pohle2017]. Instead, the integrated completed likelihood (ICL) criterion [@Biernacki2000] performed better at choosing the correct data generating model. Therefore, we post hoc computed the weighted ICL criterion (analogous to Schwarz weights) for the models fitted to the @Andersson2017 data set. Using the ICL as the model selection criterion yielded very similar results to the BIC (see supplementary material). The preference for the five-state generative model was even more consistent across conditions and subjects.

(ref:plot-schwarz-weights) Schwarz weights displayed for each subject and generative models with different numbers of states. Top facet labels indicate the condition in the @Andersson2017 data set. Higher weights indicate a better model fit.

```{r plot-schwarz-weights, fig.cap="(ref:plot-schwarz-weights)"}

print(bic.plot)

```

### Comparison to Other Algorithms

```{r calculate duration RMSD, include=FALSE}

# Calculate event descriptives for human coders

fr <- 500

A2017.events <- lapply(A2017, function(stim) {
  lapply(1:4, function(e) {
    out <- lapply(stim, function(df) {
      
      if(all(is.nan(df$t) || df$t == 0)) {
        
        df$t <- seq(0, length(df$t)-1)/fr
        
      } else {
        
        df$t <- (df$t - df$t[1])/1e6
        
        df <- df[df$t >= 0,]
        
      }
      
      counter_MN <- 1
      counter_RA <- 1
      
      number_MN <- numeric(nrow(df))
      number_RA <- numeric(nrow(df))
      
      number_MN[1] <- 1
      number_RA[1] <- 1
      
      for (i in 2:nrow(df)) {
        if(df$label_MN[i] != df$label_MN[i-1]) counter_MN <- counter_MN + 1
        if(df$label_RA[i] != df$label_RA[i-1]) counter_RA <- counter_RA + 1
        
        number_MN[i] <- counter_MN
        number_RA[i] <- counter_RA
    
      }
      
      dur_MN <- numeric(max(number_MN))
      event_MN <- numeric(max(number_MN))
      
      for (n in unique(number_MN)) {
        if(n > 1) {
          dur_MN[n] <- max(df$t[number_MN == n], na.rm = T) - max(df$t[number_MN == (n-1)], na.rm = T)
        } else {
          dur_MN[n] <- max(df$t[number_MN == n], na.rm = T) - min(df$t[number_MN == n], na.rm = T)
        }
        
        event_MN[n] <- max(df$label_MN[number_MN == n], na.rm = T)
      }
      
      dur_RA <- numeric(max(number_RA))
      event_RA <- numeric(max(number_RA))
      
      for (n in unique(number_RA)) {
        if(n > 1) {
          dur_RA[n] <- max(df$t[number_RA == n], na.rm = T) - max(df$t[number_RA == (n-1)], na.rm = T)
        } else {
          dur_RA[n] <- max(df$t[number_RA == n], na.rm = T) - min(df$t[number_RA == n], na.rm = T)
        }
        
        event_RA[n] <- max(df$label_RA[number_RA == n], na.rm = T)
      }
      
      return(list(dur_MN[event_MN == e], dur_RA[event_RA == e]))
    })
    
    df <- reduce(out, cbind)
    
    dur_MN <- unlist(df[1,])
    dur_RA <- unlist(df[2,])
    
    return(data.frame(MN = c(mean(dur_MN), sd(dur_MN), length(dur_MN)),
                      RA = c(mean(dur_RA), sd(dur_RA), length(dur_RA))))
  })
})


# Calculate RMSD between event distribution descriptives

ref <- list(dot = list(fix = list(CDT = c(60, 127, 165),
                                  EM = c(NA, NA, NA),
                                  IDT = c(323, 146, 8),
                                  IKF = c(217, 184, 72),
                                  IMST = c(268, 140, 12),
                                  IHMM = c(214, 286, 67),
                                  IVT = c(203, 282, 71),
                                  NH = c(380, 333, 30),
                                  BIT = c(189, 113, 67),
                                  LNS = c(NA, NA, NA)),
                       sac = list(CDT = c(NA, NA, NA),
                                  EM = c(17, 14, 93),
                                  IDT = c(32, 14, 10),
                                  IKF = c(60, 26, 29),
                                  IMST = c(13, 5, 18),
                                  IHMM = c(41, 17, 27),
                                  IVT = c(36, 14, 28),
                                  NH = c(43, 16, 42),
                                  BIT = c(NA, NA, NA),
                                  LNS = c(26, 11, 53)),
                       pso = list(NH = c(24, 12, 17),
                                  LNS = c(20, 9, 31))),
            img = list(fix = list(CDT = c(397, 559, 251),
                                  EM = c(NA, NA, NA),
                                  IDT = c(399, 328, 242),
                                  IKF = c(174, 239, 513),
                                  IMST = c(304, 293, 333),
                                  IHMM = c(133, 216, 701),
                                  IVT = c(114, 204, 827),
                                  NH = c(258, 299, 292),
                                  BIT = c(209, 136, 423),
                                  LNS = c(NA, NA, NA)),
                       sac = list(CDT = c(NA, NA, NA),
                                  EM = c(25, 22, 787),
                                  IDT = c(25, 15, 258),
                                  IKF = c(62, 37, 353),
                                  IMST = c(17, 10, 335),
                                  IHMM = c(48, 26, 368),
                                  IVT = c(41, 22, 373),
                                  NH = c(50, 20, 344),
                                  BIT = c(NA, NA, NA),
                                  LNS = c(29, 12, 390)),
                       pso = list(NH = c(28, 13, 237),
                                  LNS = c(25, 9, 319))),
            vid = list(fix = list(CDT = c(213, 297, 211),
                                  EM = c(NA, NA, NA),
                                  IDT = c(554, 454, 48),
                                  IKF = c(258, 296, 169),
                                  IMST = c(526, 825, 71),
                                  IHMM = c(234, 319, 194),
                                  IVT = c(202, 306, 227),
                                  NH = c(429, 336, 83),
                                  BIT = c(248, 215, 170),
                                  LNS = c(NA, NA, NA)),
                       sac = list(CDT = c(NA, NA, NA),
                                  EM = c(20, 16, 252),
                                  IDT = c(24, 53, 41),
                                  IKF = c(55, 20, 107),
                                  IMST = c(18, 10, 76),
                                  IHMM = c(42, 18, 109),
                                  IVT = c(36, 16, 112),
                                  NH = c(44, 18, 104),
                                  BIT = c(NA, NA, NA),
                                  LNS = c(28, 12, 122)),
                       pso = list(NH = c(28, 13, 78),
                                  LNS = c(24, 10, 87))))


A2017.rmsd <- lapply(2:4, function(k) {
  lapply(1:length(A2017.fit), function(x) { 
    out <- lapply(1:k, function(y) {
      out <- lapply(A2017.fit[[x]], function(z) {
        
        if(class(z[[k]]) == "gazeHMM") z[[k]]$events[[y]]
        
      })
      
      df <- try(reduce(out, rbind))
      
      alg <- try(c(mean(df$dur, na.rm = T), sd(df$dur, na.rm = T), nrow(df)))
      
      if(y == 4) {
        M <- try(cbind(A2017.events[[x]][[y]], alg))
      } else {
        M <- try(cbind(A2017.events[[x]][[y]], alg, as.data.frame(ref[[x]][[y]])))
        M[1:2,4:ncol(M)] <- M[1:2,4:ncol(M)]/1e3
      }
      
      M.norm <- try(apply(M, 1, function(x) {(x - min(x, na.rm = T))/(max(x, na.rm = T) - min(x, na.rm = T))}))
      
      if(y == 4) {
        rmsd <- try(sum(sqrt((M.norm[3,] - colSums(M.norm[1:2,]/2))^2)))
      } else {
        rmsd <- try(apply(M.norm[3:nrow(M.norm),], 1, function(r) {sum(sqrt((r - colSums(M.norm[1:2,]/2))^2))}))
      }
      
      coder <- try(sum(sqrt((M.norm[1,] - M.norm[2,])^2)))
      
      rmsd <- c(coder, coder, rmsd)
      
      return(rbind(M, rmsd))
    })
  })
})


# Create tables for RMSDs

rmsd.table <- lapply(1:length(A2017.rmsd), function(x) {
  out <- lapply(1:length(A2017.rmsd[[x]]), function(y) {
    out <- lapply(1:length(A2017.rmsd[[x]][[y]]), function(z) {
      
      tabmat <- t(apply(A2017.rmsd[[x]][[y]][[z]], 2, function(d) {as.character(round(d, 3))}))
      
      # tabmat[3,] <- paste("**", tabmat[3,], "**", sep = "")
      
      colnames(tabmat) <- c("Mean", "SD", "Events", "RMSD")
      rownames(tabmat)[1:3] <- c("coderMN", "coderRA", paste0("gazeHMM-", x+1))
      
      return(cbind(z, rownames(tabmat), tabmat))
    })
    
    cbind(y, reduce(out, rbind))
  })
  
  df <- as.data.frame(reduce(out, rbind), stringsAsFactors = F) %>%
    mutate(Condition = factor(y, labels = c("moving dots", "image", "video"))) 
  
  lapply(1:(x+1), function(x) {
    
    df <- df %>% dplyr::filter(z == x) %>%
      dplyr::select(-c("z", "y")) %>%
      rename(Algorithm = V3) %>%
      pivot_wider(names_from = "Condition", names_glue = "{Condition}_{.value}", names_sort = T, values_from = c("Mean", "SD", "Events", "RMSD")) 
    
    df %>% dplyr::select(c("Algorithm", "image_Mean", "image_SD", "image_Events", "image_RMSD",
                           "moving dots_Mean", "moving dots_SD", "moving dots_Events", "moving dots_RMSD",
                           "video_Mean", "video_SD", "video_Events", "video_RMSD"))
  })
})

rmsd.table[[3]][1:3] <- lapply(1:3, function(x) {
  
  df <- rbind(rmsd.table[[3]][[x]][1:2,], rmsd.table[[2]][[x]][3,], rmsd.table[[3]][[x]][3:nrow(rmsd.table[[3]][[x]]),])
  
  df[c(1:3, 5:nrow(df)),2:5] <- rmsd.table[[2]][[x]][,2:5]
  
  df[3,6:13] <- NA
  df[4,2:5] <- NA
  
  return(df)
})

rmsd.table[[3]][[4]][3,2:5] <- NA

```

```{r calculate Cohen s kappa and confusion matrices, include= FALSE}

# Compute Cohen's kappa and confusion matrices

ref.acc <- matrix(c(0.92, 0.81, 0.83, 0.95, 0.91, 0.94, 0.88, 0.82, 0.83, NA, NA, NA,
                    0.92, 0.84, 0.82, 0.95, 0.91, 0.94, 0.88, 0.80, 0.81, NA, NA, NA,
                    0.38, 0.06, 0.11, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.00, 0.00, 0.00, 0.64, 0.66, 0.67, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.36, 0.00, 0.03, 0.45, 0.26, 0.38, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.63, 0.03, 0.14, 0.58, 0.46, 0.59, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.38, 0.00, 0.03, 0.54, 0.30, 0.52, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.67, 0.03, 0.13, 0.69, 0.60, 0.71, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.67, 0.03, 0.13, 0.75, 0.63, 0.76, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.52, 0.00, 0.01, 0.67, 0.60, 0.68, 0.24, 0.20, 0.25, NA, NA, NA,
                    0.67, 0.03, 0.14, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, NA, NA, NA,
                    0.00, 0.00, 0.00, 0.81, 0.75, 0.81, 0.64, 0.59, 0.63, NA, NA, NA), ncol = 12, byrow = T)

A2017.acc <- lapply(2:4, function(k) {
  lapply(1:length(A2017.fit), function(x) { 
    out <- lapply(1:length(A2017.fit[[x]]), function(y) {
      
      if(class(A2017.fit[[x]][[y]][[as.character(k)]]) == "gazeHMM") {
        cod.MN <- as.data.frame(cbind(A2017.fit[[x]][[y]][[as.character(k)]]$samples$label, A2017[[x]][[y]]$label_MN))
        cod.RA <- as.data.frame(cbind(A2017.fit[[x]][[y]][[as.character(k)]]$samples$label, A2017[[x]][[y]]$label_RA))
        
        return(rbind(cod.MN, cod.RA))
      }
    })
    
    df <- reduce(out, rbind)
    
    df[df[,1] == 0,1] <- 5
    
    events <- lapply(1:k, function(e) {
      
      alg <- df[,1] == e
      cod <- df[,2] == e
      
      mat <- matrix(c(sum(alg & cod, na.rm = T), sum(alg & !cod, na.rm = T), 
                      sum(!alg & cod, na.rm = T), sum(!alg & !cod, na.rm = T)), 2, 2)
      
      over <- mat[2,1]/sum(mat[,1])
      under <- mat[1,2]/sum(mat[,2])
      
      return(list(kappa = cohen.kappa(mat)$kappa, 
                  over = over,
                  under = under))
    })
    
    ratio <- mean(df[,1] != df[,2], na.rm = T)
    
    conmat <- table(df[,1], df[,2])
    
    return(list(events = events, ratio =  ratio, conmat = conmat))
  })
})


# Compute Cohen's kappa between human coders

A2017.cod.acc <- lapply(A2017, function(x) {
  df <- lapply(x, function(y) {
    
    return(y[,6:7])
  }) %>% reduce(rbind)
  
  events <- lapply(1:4, function(e) {
    
    MN <- df[,1] == e
    RA <- df[,2] == e
    
    mat <- matrix(c(sum(MN & RA, na.rm = T), sum(MN & !RA, na.rm = T), 
                    sum(!MN & RA, na.rm = T), sum(!MN & !RA, na.rm = T)), 2, 2)
    
    return(cohen.kappa(mat)$kappa)
  })
})


# Create tables for Cohen's kappa and confusion matrices

kappa.table <- lapply(A2017.acc, function(x) {
  out <- lapply(x, function(y){
    out <- lapply(y$events, function(z){
      return(round(z$kappa, 3))
    })
    
    return(reduce(out, cbind))
  })
  
  tabmat <- reduce(out, rbind)
  
  tabmat <- rbind(tabmat[2,], tabmat[1,], tabmat[3,])
  
  return(as.vector(tabmat))
})

conf.table <- lapply(A2017.acc, function(x) {
  out <- lapply(x, function(y){
    
    tabmat <- apply(y$conmat, 2, as.character)
    
    # diag(tabmat)[1:(nrow(tabmat)-1)] <- paste("**", diag(tabmat)[1:(nrow(tabmat)-1)], "**", sep = "")
    
    # tabmat[nrow(tabmat), 5] <- paste("**", tabmat[nrow(tabmat), 5], "**", sep = "")

    colnames(tabmat) <- c("Fixations", "Saccades", "PSOs", "Pursuits", "Blinks", "Other")
    rownames(tabmat) <- c("Fixations", "Saccades", "PSOs", "Pursuits", "Blinks", "Other")[1:nrow(tabmat)]
    rownames(tabmat)[nrow(tabmat)] <- "Blinks"

    return(as.data.frame(tabmat))
  })
})


# Create figure for disagreement ratio

ref.disag <- list(c(7, 7, A2017.acc[[3]][[2]]$ratio*100, A2017.acc[[2]][[2]]$ratio*100, 23, 92, 20, 24, 20, 20, 19, 32, 31, 84),
                  c(11, 11, A2017.acc[[3]][[1]]$ratio*100, A2017.acc[[2]][[1]]$ratio*100, 89, 96, 86, 85, 86, 84, 84, 93, 89, 93),
                  c(19, 19, A2017.acc[[3]][[3]]$ratio*100, A2017.acc[[2]][[3]]$ratio*100, 64, 95, 61, 62, 61, 59, 59, 70, 67, 92))

disag.df <- data.frame(Image = ref.disag[[1]], Dots = ref.disag[[2]], Video = ref.disag[[3]],
                       Algorithm = c("coderMN", "coderRA", "gazeHMM-4", "gazeHMM-3", "CDT", "EM", "IDT", "IKF", "IMST", "IHMM", "IVT", "NH", "BIT", "LNS"),
                       stringsAsFactors = F)

disag.df$Algorithm <- factor(disag.df$Algorithm, levels = unique(disag.df$Algorithm))

disag.df <- disag.df %>% pivot_longer(c("Image", "Dots", "Video"), names_to = "Condition", values_to = "Ratio")

disag.plot <- ggplot(disag.df, aes(x = Algorithm, y = Ratio, fill = Condition)) + 
  geom_col(position = "dodge") + 
  scale_y_continuous(name = "Disagreement (in %)", limits = c(0, 100), breaks = c(0, 0.25, 0.5, 0.75, 1)*100) +
  scale_x_discrete(name = "") + coord_flip()

```

As displayed in \@ref(tab:tab-rmsd-4-fix), gazeHMM showed a relatively low RMSD for fixations in the static condition compared to the other algorithms that were applied to the @Andersson2017 data set. The lower RMSD for fixations indicated more similar classification to the human coders in terms of their mean and SD duration as well as the number of classified fixations. Oppositely, for fixations in the dynamic conditions, the RMSD of gazeHMM was the highest among the compared algorithms, suggesting substantial differences to the human coders. It is likely that these occured because gazeHMM classified a much larger number of fixations with very short durations. For saccades, gazeHMM had high RMSDs for the image and moving dots conditions, but for the video condition the RMSD was comparably low (see Table \@ref(tab:tab-rmsd-4-sac)). Only two other algorithms classified PSOs [NH and LNS; @Nystrom2010; @Larsson2013], and gazeHMM showed consistently higher RMSDs than LNS (see Table \@ref(tab:tab-rmsd-4-pso)). Compared to NH, the RMSD was higher in the image and video conditions but lower in the moving dots condition. No other algorithm parsed smooth pursuits, but the RMSD for gazeHMM was higher than among the human coders (see Table \@ref(tab:tab-rmsd-4-sp)). Again, it classified a much larger number of smooth pursuits with short durations.

(ref:tab-rmsd-4-fix) Fixation Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-fix-note) Durations are displayed in seconds. gazeHMM-3 classified three and gazeHMM-4 classified four events. RMSD = root mean square deviation. Table adapted from @Andersson2017.

```{r tab-rmsd-4-fix}

apa_table(rmsd.table[[3]][[1]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-rmsd-4-fix)",
          note = "(ref:tab-rmsd-4-fix-note)")

```

(ref:tab-rmsd-4-sac) Saccade Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-sac-note) Durations are displayed in seconds. gazeHMM-3 classified three and gazeHMM-4 classified four events. RMSD = root mean square deviation. Table adapted from @Andersson2017.

```{r tab-rmsd-4-sac}

apa_table(rmsd.table[[3]][[2]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-rmsd-4-sac)",
          note = "(ref:tab-rmsd-4-sac-note)")

```

(ref:tab-rmsd-4-pso) PSO Duration Descriptives and RMSD Between Algorithms and Human Coders
(ref:tab-rmsd-4-pso-note) Durations are displayed in seconds. gazeHMM-3 classified three and gazeHMM-4 classified four events. RMSD = root mean square deviation. Table adapted from @Andersson2017.

```{r tab-rmsd-4-pso}

apa_table(rmsd.table[[3]][[3]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-rmsd-4-pso)",
          note = "(ref:tab-rmsd-4-pso-note)")

```

(ref:tab-rmsd-4-sp) Smooth Pursuit Duration Descriptives and RMSD Between gazeHMM and Human Coders
(ref:tab-rmsd-4-sp-note) Durations are displayed in seconds. gazeHMM-4 classified four events. RMSD = root mean square deviation. Table adapted from @Andersson2017.

```{r tab-rmsd-4-sp}

apa_table(rmsd.table[[3]][[4]],
          col.names = c("Algorithm", rep(c("Mean", "SD", "Events", "RMSD"), 3)),
          col_spanners = list("Image" = c(2, 5), "Moving dots" = c(6, 9), "Video" = c(10, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-rmsd-4-sp)",
          note = "(ref:tab-rmsd-4-sp-note)")

```

Table \@ref(tab:tab-kappa-4) contains the sample-to-sample agreement between the algorithms and human coders measured by Cohen's kappa. For fixations, gazeHMM shows one of the highest agreements for static, and *the* highest agreements for dynamic data. The absolute agreement is substantial for static and slight to fair [@Landis1977]. For saccades, the relative agreement for gazeHMM is low for the image, moderate for the moving dots, and high for the video condition. In absolute terms, the agreement is moderate to substantial. Concerning PSOs, gazeHMM shows higher agreement than NH in the image and video conditions but consistently lower agreement compared to LNS. Absolutely, the agreement is slight to fair. Lastly, the 

Additionally, I compared gazeHMM to the other algorithms by using Cohen's kappa as a measure of sample-to-sample agreement between algorithms and human coders (see Table \@ref(tab:tab-kappa-4)). Absolute kappa values were interpreted according to @Landis1977.  
For fixations, absolute kappa values in all three conditions indicated a slight to fair agreement between gazeHMM and human coders. Compared to the other algorithms, the agreement of gazeHMM was the lowest in the image condition but the highest in the moving dots and video condition.  
For saccades, gazeHMM showed kappa values that correspond to a moderate to substantial agreement and were relatively high compared to the other algorithms. However, gazeHMM never reached the highest agreement.  
For PSOs, absolute kappa values showed slight to fair agreement agreement to human coders. It was relatively low in the moving dots condition and second-rate in the image and video conditions compared to NH and LNS.  
The agreement for smooth pursuits was moderate in the moving dots condition, fair in the video condition, and slight in the image condition. No other algorithm in the study was designed to detect smooth pursuits.  
In sum, gazeHMM revealed a relatively high sample-to-sample agreement to human coders for saccades. For PSOs and smooth pursuits, I found fair to moderate agreement. The performance for fixations was absolutely and relatively low.

(ref:tab-kappa-4) Cohen's Kappa Between Human Coders and Algorithms for Different Conditions and Events
(ref:tab-kappa-4-note) Negative values were set to zero. gazeHMM-3 classified three and gazeHMM-4 classified four events. Table adapted from @Andersson2017.

```{r tab-kappa-4}

tab.kappa.4 <- cbind(c("coderMN", "coderRA", "gazeHMM-3", "gazeHMM-4", "CDT", "EM", "IDT", "IKF", "IMST", "IHMM", "IVT", "NH", "BIT", "LNS"),
                     as.data.frame(rbind(ref.acc[1:2,], c(kappa.table[[2]], NA, NA, NA), kappa.table[[3]], ref.acc[3:nrow(ref.acc),]))) 

tab.kappa.4[4, c(2, 5, 8, 11)] <- NA
tab.kappa.4[3, -c(1, 2, 5, 8, 11)] <- NA

apa_table(tab.kappa.4,
          col.names = c("Algorithm", rep(c("Image", "Dots", "Video"), 4)),
          col_spanners = list("Fixations" = c(2, 4), "Saccades" = c(5, 7), "PSOs" = c(8, 10), "Smooth pursuit" = c(11, 13)),
          font_size = "tiny",
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-kappa-4)",
          note = "(ref:tab-kappa-4-note)")

```

#### Disagreement and Confusion
At last, I computed the overall disagreement [^4] as the percentage of all samples taken together that were classified differently by gazeHMM than the human coders. Figure \@ref(fig:plot-disag) shows that in the image condition, gazeHMM with four events had a higher overall disagreement by between 10 and 20% than all but two of the other algorithms. In the moving dots and video conditions, the disagreement was between 20 and 30% lower than for all other algorithms. Notably, gazeHMM's disagreement was similar across conditions (around 45%). In summary, gazeHMM had a lower overall disagreement than all other algorithms in the moving dots and video conditions but higher disagreement in the image condition.

[^4]: Even though I proposed to compute the *disagreement ratio* and @Andersson2017 also used that term, the comparison technically involves percentages and not ratios. Thus, I decided to use only the term *disagreement* instead.

(ref:plot-disag) Disagreement between algorithms and human coders for different conditions (in %). gazeHMM-3 classified three and gazeHMM-4 classified four events. Figure adapted from @Andersson2017.

```{r plot-disag, fig.cap="(ref:plot-disag)"}

print(disag.plot)

```

(ref:tab-conf-4) Confusion Matrix Between gazeHMM (Rows) and Human Coders (Columns) for Different Conditions
(ref:tab-conf-4-note) gazeHMM classified four events and blinks.

```{r tab-conf-4}

tab.conf.4 <- cbind(Event = rep(c("Fixations", "Saccades", "PSOs", "Pursuits", "Blinks"), 3), reduce(conf.table[[3]], rbind)) %>%
  mutate(Condition = rep(c("Moving dots", "Image", "Video"), each = 5)) %>%
  arrange(Condition) %>%
  dplyr::select(-Condition)

apa_table(tab.conf.4,
          stub_indents = list("Image" = c(1:5), "Moving dots" = c(6:10), "Video" = c(11:15)),
          midrules = c(6, 12),
          caption = "(ref:tab-conf-4)",
          note = "(ref:tab-conf-4-note)")

```

Contrary to my second hypothesis, applying gazeHMM to a benchmark data set revealed that it did not outperform all other algorithms in terms of RMSD and sample-to-sample agreement. The two criteria rather indicated a moderate performance of gazeHMM. However, the overall disagreement was lower for gazeHMM than for all other algorithms in the dynamic conditions but the highest in the static image condition.  
Looking at the confusion matrix in Table \@ref(tab:tab-conf-4), it can be seen that gazeHMM confused fixations and smooth pursuit to a large extend. In the static image condition, it overclassified smooth pursuit events, while they were underclassified in the dynamic conditions.

### gazeHMM With Three States
Even though it was not confirmed by the model comparison, applying gazeHMM with three states to static data seems theoretically more appropriate. Including only three states could prevent gazeHMM from overclassifying smooth pursuits in the image data and yield better validation results. Therefore, I decided to apply gazeHMM with three states to the image data and explore the comparison to other algorithms.  
Table \@ref(tab:tab-rmsd-3) shows the RMSD between gazeHMM and human coders only for the image condition. For fixations, gazeHMM shows a comparably low RMSD, but it was relatively high for saccades and PSOs. 
The sample-to-sample agreement between gazeHMM with three states and the human coders measured by Cohen's kappa is displayed in Table \@ref(tab:tab-kappa-3) (only for the image condition). For fixations, the absolute kappa indicated substantial agreement and was relatively high compared to the other algorithms. The absolute kappa for saccades corresponded to moderate agreement. Compared to the other algorithms, it was rather low. For PSOs, the absolute agreement was fair, but relatively low compared to the other algorithms.  
Figure \@ref(fig:plot-disag) illustrates the overall disagreement between gazeHMM with three events and the human coders compared to the other algorithms. In all three conditions, the disagreement was slightly lower for gazeHMM than for all the other algorithms (5-10% for images and moving dots, around 20% for videos). Compared to gazeHMM with four states, the RMSD for fixations decreased while the sample-to-sample agreement increased. For saccades and PSOs, the opposite pattern was found. The overall disagreement substantially decreased. Overall, applying gazeHMM with three states to static data seems more appropriate, since the increase in fixation classification outweighs the decrease in classification of saccades and PSOs. There was support for my second hypothesis since gazeHMM had lower overall disagreement to human coders, ranging between 5 and 20% across conditions, than all other algorithms.

(ref:tab-kappa-3) Cohen's Kappa Between Human Coders and Algorithms for Different Events in the Image Condition
(ref:tab-kappa-3-note) Negative values were set to zero. gazeHMM-3 classified three and gazeHMM-4 classified four events. Table adapted from @Andersson2017.

```{r tab-kappa-3}

tab.kappa.3 <- cbind(c("coderMN", "coderRA", "gazeHMM-4", "gazeHMM-3", "CDT", "EM", "IDT", "IKF", "IMST", "IHMM", "IVT", "NH", "BIT", "LNS"),
                     as.data.frame(rbind(ref.acc[1:2, 1:9], kappa.table[[3]][1:9], kappa.table[[2]], ref.acc[3:nrow(ref.acc), 1:9]))) %>%
  dplyr::select(c(1, seq(2, 10, 3)))

apa_table(tab.kappa.3,
          col.names = c("Algorithm", "Fixations", "Saccades", "PSOs"),
          format.args = list(na_string = "-", drop0trailing = T),
          caption = "(ref:tab-kappa-3)",
          note = "(ref:tab-kappa-3-note)")

```

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
